\documentclass{report}

\input{../preamble}
\input{../macros}
\input{../letterfonts}

\title{\Huge{15756 Randomized Algorithms}}
\author{\huge{Rohan Jain}}
\date{}

\begin{document}
\newcommand{\poly}{\text{poly}}
\maketitle
\newpage% or \cleardoublepage
% \pdfbookmark[<level>]{<title>}{<dest>}
\pdfbookmark[section]{\contentsname}{toc}
\tableofcontents

\pagebreak

\chapter{100 Prisoners Problem}

This is a problem based on a riddle:
\qs{}{There is a jail and a warden. However, the warden is very lazy and says if they win a game, they're all set free, otherwise they all die. There is 100 boxes in a room and each box has a hidden number from 1 to 100. They are assorted in a random permutation. Each prisoner is also given a number from 1 to 100. Each prisoner is allowed to enter the room and will be allowed to open 50 boxes and if they find their number, they win. They will only live if everyone finders their number. The prisoners can't communicate with each other. What strategy should they use to maximize their chances of winning?}

\noindent Naive strategy: Each prisoner opens 50 boxes at random. The probability of winning is $\dfrac{1}{2^{100}}$.

\thm{}{There exists a strategy such that the probability of winning is $\geq \frac{30}{100}$.}
\begin{proof}
\noindent The algorithm is as follows:
\begin{enumerate}
    \item Each prisoner opens the box with their number.
    \item If they find their number, they open the box with the number they found.
    \item They continue this process until they find their number or they open 50 boxes.
\end{enumerate}

\noindent Cycle notation: Consider the following arrangement, where the ordinal is the box number:
\begin{enumerate}
    \item 7
    \item 6
    \item 4
    \item 3
    \item 8
    \item 1
    \item 2
    \item 0
    \item 5
    \item 10
\end{enumerate}
\noindent This can be represented as 4 individual cycles: $(1, 7, 2, 6)$, $(3, 4)$, $(5, 8, 9)$, $(10)$.

A critical observation to make is that prisoner $i$ will win if and only if the cycle containing $i$ is of length $\leq 50$. That is, everyone wins iff there are no cycles of length $> 50$.

We warm up by considering opening 75 boxes instead of 50. The observation is that if anyone loses, at least 75 people will lose. Let $X$ be a random variable representing the number of people who lose. Then, $\E[X] = 25$ as each player has a $\frac{1}{4}$ chance of losing. Recall Markov's inequality:
\thm{}{Let $X$ be a non-negative random variable. Then, for any $t > 0$, $\Pr[X \geq t] \leq \frac{\E[X]}{t}$.}

\noindent Applying Markov's inequality, we have $\Pr[X \geq 75] \leq \frac{25}{75} = \frac{1}{3}$. Thus, the probability of winning is $\geq \frac{2}{3}$. Moving back to the original problem, we can apply the same logic.
\begin{align*}
    \Pr(\text{anyone loses}) = \Pr(\text{at least one cycle of length} > 50).
\end{align*}

Recall how we can count the number of cycles of length $\geq 50$. For a cycle to be exactly length $n > \frac{100}n = 50$, we need to choose $n$ boxes and then permute them. Thus, the number of cycles of length $n$ is $\frac{100!}{n \cdot (100 - n)!}$. Now we have to worry about the number of arrangements of the other $100-n$ boxes, which is $(100-n)!$. Multiplying these two quantities, we get:
\begin{align*}
    \frac{100!}{n \cdot (100-n)!} \cdot (100-n)! = \frac{100!}{n}.
\end{align*}
As there are $100!$ total permutations, we realize that exactly $\frac 1n$ of the permutations contain a cycle of length $n$. Thus, the probability of losing is:
\begin{align*}
    \Pr(\text{anyone loses}) = \sum_{n=51}^{100} \frac{1}{n} = \sum_{n=1}^{100} \frac 1n - \sum_{n=1}^{50} \frac 1n = H_{100} - H_{50}\approx \ln 100 - \ln 50 = \ln 2 \approx 0.693.
\end{align*}
Thus, the probability of winning is $\geq 1 - 0.693 \approx 0.307$ as desired.

\end{proof}

\noindent We move on to proving that this algorithm is actually optimal:
\thm{}{The algorithm described above is optimal.}
\begin{proof}
    We consider a second version of the game, where any box that a previous prisoner has opened stays open. So if prisoner $i$ walks in to see that $i$ has been revealed, he just leaves. Otherwise, he opens boxes until he finds $i$ or has opened 50 boxes. Boxes are never closed. 
    \mlenma{}{Cycle algorithm is no better at version 2 than version 1.}
    \begin{proof}
        Option 1: Someone from my cycle in the past lost. 

        Option 2: I am the first person in my cycle to enter.

        In both versions, the boxes in my cycle are closed. This means that in both versions, I win iff the cycle is of length $\leq 50$.

        Option 3: Someone from my cycle has already entered, and they won. This means the cycle is of length $\leq 50$, so I win in both versions. 
    \end{proof}
    \mlenma{}{All algorithms are equally good in version 2.}
    \begin{proof}
        By symmetry: If I'm opening a box, I have to pick out of the remaining boxes, and they all have the same probability of containing my number. 
    \end{proof}
    Together, these lemmas show that the cycle algorithm is optimal in version 1. 
\end{proof}

\chapter{Graph algorithms}
*Note: all graphs are going to be undirected and unweighted.

\section{Graph coloring}
Suppose we are given a graph $G = (V, E)$. Suppose $G$ is 3-colorable. Recall that a 3-coloring is a function $c: V \to \{1, 2, 3\}$ such that $c(u) \neq c(v)$ for all $(u, v) \in E$. Simply put, you can color the vertices such that no two adjacent vertices have the same color.
\qs{}{Can we efficiently find a 3-coloring of $G$?}
\noindent \textbf{Answer:} No, this is NP-hard.

\noindent \textbf{Observation:} It's possible to partition the vertices in the graph into two parts that are each $\triangle$-free. This is easy because we can create the partition of one set being the odd vertices (1 and 3) and the other set being the even vertices (2).

\thm{McDiarmid (1993)}{Given a 3-colorable graph, you can construct a triangle-free partition of the vertices in $\poly(n)$ expected time.}
\begin{proof}
    Consider the following algorithm: in each round, find a triangle in some side, pick a random vertex from the triangle, and move it to the other side. Repeat this process until there are no more triangles.

Now we analyze the algorithm. Fix a 3-coloring of $G$. Let $X$ be the number of vertices of color 1 on the left side. Let $Y$ be the number of vertices of color 2 on the right side. Now we analyze what happens to $X+Y$ during a round: 
\begin{itemize}
    \item If we move a vertex from the left to the right, $X$ can decrease by 1 with probability $\frac 13$, $Y$ can increase by 1 with probability $\frac 13$, or both can stay the same with probability $\frac 13$. 
    \item If we move a vertex from the right to the left, $Y$ can decraese by 1, $X$ can increase by 1, or both can stay the same all with probability $\frac 13$.
\end{itemize}
So overall, $X+Y$ increases by 1 with probability $\frac 13$, decreases by 1 with probability $\frac 13$, and stays the same with probability $\frac 13$. This is just a random walk.

The random walk starts at an unknown value, but if it ever leaves $[0, n]$, the algorithm terminates. It can look something like $+1, -1, 0, 0, -1, +1, +1, +1$, and at some point $t_1$ the algorithm finishes. For sake of discussion, assume this hypothetical random walk keeps going. At some point, the walk may leave $[0, n]$ at time $t_2$. Note that $t_2$ can exist only if $t_1$ exists, and $t_1 < t_2$. So $\E[t_2] \geq \E[t_1]$.

Goal: prove that $\E[t_2] \in O(n^2)$. This will imply that $\E[t_1] \in O(n^2)$, which will imply that the algorithm runs in $\poly(n)$ time.

We step away from the graph problem to analyze random walks further. Consider a random walk on $\Z$ that starts at 0. Let $T$ be the first time the walk arrives at $n$ or $-n$. We want to show that $\E[T] \in O(n^2)$.

Fact: after $t$ steps, the middle ~90\% of possible positions would be $O(\sqrt t)$ from the origin. Additionally, within this ~90\%, the walk is about uniformly distributed.

Using the fact above, we want $t=n^2$ to be the time it takes to reach $n$ or $-n$. 

Now we return to the original problem. 

\mlenma{}{For any power of 2, the expected time to get from the origin to $n$ or $-n$ is $n^2$.}
\begin{proof}
    If $n=1$, the expected time is 1. Define $T_n$ as the expected time to get from the origin to $n$ or $-n$. Pretend we split the number line into $-n, -n/2, 0, n/2, n$. Then we can write $T_n$ as:
    \begin{align*}
        T(n) = T(n/2) + T(n/2) + \frac 12 T(n).
    \end{align*}
    Solving, we get $T(n) = 4T(n/2) \implies T(n) = n^2$.
\end{proof}

This means that our algorithm is $O(n^2)$ rounds, meaning the algorithm runs in $\poly(n)$ time.
\end{proof}

\section{Shortest paths}
Given $G = (V,E)$, for each $x, y \in V$, we want $d(x, y) \pm O(1)$ in time $O(n^{2.5} \log n)$.

Naive algorithm: $O(|V| \cdot |E|) = O(n^3)$. 

Now consider a shortest path from $x$ to $y$.
\begin{itemize}
    \item Option 1: Every vertex on the path has degree $< \sqrt n$. 
    
    Then for each vertex, we can BFS, but only on low degree vertices. This takes $O(n^{2.5})$ time. The result is that we get exact distances for all vertices on the path.
    \item Option 2: There is a vertex on the path with degree $\geq \sqrt n$.
    
    Now we sample $100\sqrt n \log n$ iid uniformly random vertices and call this set $S$. We claim that with high probability, every high degree vertex $w$ will be incident to a vertex $s \in S$. 

    If we take this claim for granted, observe that for $x,y$ in option 2, $d(x, y) = d(x, s) + d(s, y) \pm 2$ for some $s \in S$. So for each $s \in S$, do a BFS which takes $O(n^{2.5} \log n)$ time. For each pair $(x, y)$, compute the minimum $d(x, s) + d(s, y)$ over all $s \in S$. This takes $O(n^{2.5} \log n)$ time.
\end{itemize}

Now we prove the claim made in option 2. 
\begin{proof}
    Let $w$ be a high degree vertex. We want to find $P($none of $w$'s neighbors are sampled). 
    \begin{align*}
        P(\text{none of $w$'s neighbors are sampled}) &\leq P(\text{given sample misses})^{|S|} \\ 
        &= \left(1 - \frac{1}{\sqrt n}\right)^{100 \sqrt n \log n}.
    \end{align*}
    Now we use the fact that $\left(1 - \frac 1n\right)^n \leq \frac 1e$. So now,
    \begin{align*}
        \left(1 - \frac{1}{\sqrt n}\right)^{100 \sqrt n \log n} &\leq \frac{1}{e^{100 \log n}} = \frac{1}{n^{100}}.
    \end{align*}
    This is the probability of $w$ screwing up. The probability of any high degree vertex screwing up is $\leq \frac{n}{n^{100}} = \frac{1}{n^{99}}$ by union bound.
\end{proof}
\newpage
\section{Global Min-Cut}
We start by stating the question we are trying to solve.
\qs{Global Min-Cut Problem}{
    We are given an unweighted and undirected graph $G = (V, E)$. A ``cut'' of the graph is a partition of the vertices into two sets such that each set is non-empty. The ``cost'' of the cut (also called the cut ``size'') is the number of edges that have one endpoint in each set. The goal is to find the cut with the smallest cost.
}

\noindent Let $n:= |V|$. We consider two randomized algorithms:
\begin{itemize}
    \item Karger's Algorithm (1995) - poly$(n)$-time.
    \item Karger-Stein Algorithm (1996) - $\tilde{O}(n^2)$ time.
\end{itemize}

\subsection{Edge Contraction}
The key idea in both algorithms is edge contraction. Given an edge $(u, v)$, we can contract it by removing the edge and merging the two vertices into a single vertex. The new vertex will have all the edges that $u$ and $v$ had. For example:

\begin{center}
    \begin{tikzpicture}
        \node[draw, circle] (A) at (0, 1) {A};
        \node[draw, circle] (B) at (2, 1) {B};
        \node[draw, circle] (C) at (1, -1) {C};
    
        \draw (A) -- (B) node[midway, above] {e};
        \draw (B) -- (C);
        \draw (A) -- (C);
    
        \draw[->, thick] (2.5, 0) -- (4.5, 0) node[midway, above] {Contract edge e};
    
        \node[draw, circle] (AB) at (6, 1) {AB};
        \node[draw, circle] (C2) at (6, -1) {C};

        \draw (AB) to [bend left=20] (C2);
        \draw (AB) to [bend right=20] (C2);
    \end{tikzpicture}
\end{center}
\noindent Each contraction reduces the number of vertices by 1, and we want to stop when we have two vertices left. This leaves us with two vertices and many edges between them.
\nt{We allow multi-edges.}
\noindent \textbf{Question:} Should we keep self-edges?

\noindent \textbf{Answer:} No. Delete them.

\noindent The observation we make is that we win iff edges at the end are a min-cut.

\subsection{Karger's Algorithm}
\noindent The algorithm is as follows:
\begin{itemize}
    \item Randomly pick an edge $(u, v)$.
    \item Contract $(u, v)$.
    \item Repeat until 2 vertices are left.
\end{itemize}
\mlenma{}{For any min-cut C, the probability that Karger's algorithm finds C is $\geq \frac{1}{\text{poly}(n)}$.}
\begin{proof}
    Let $e_1, e_2, \ldots$ be the edges we contract. Let's start by considering $P(e_1 \in C)$.

    We first have $P(e_1 \in C) = \frac{|C|}{|E|}$, but there's something better to notice. Let $d$ be the minimum degree of any vertex in $G$. Then $|C| \leq d$ and $|E| \geq \frac{dn}{2}$. So $P(e_1 \in C) \leq \frac{2}{n}$ and $P(e_1 \notin C) \geq 1 - \frac{2}{n}$.

    Now consider $P(e_2 \notin C \mid e_1 \notin C)$. By the same analysis, we have $P(e_2 \notin C \mid e_1 \notin C) \geq 1- \frac{2}{n-1}$. This is because after we contract $e_1$, we have $n-1$ vertices left. So, $C$ is still a min-cut with respect to the remaining vertices. Set $d$ as the new minimum degree, and we have $|C| \leq d$ and $|\text{remaining edges}| \geq \frac{d(n-1)}{2}$. Then:
    \begin{align*}
        P(e_2 \in C \mid e_1 \notin C) = \frac{|C|}{|\text{remaining edges}|} \leq \frac{d}{\frac{d(n-1)}{2}} = \frac{2}{n-1}.
    \end{align*}
    Now:
    \begin{align*}
        P(e_3 \notin C | e_1, e_2 \notin C) &\geq 1 - \frac{2}{n-2} \\
        &\vdots \\
        P(e_k \notin C | e_1, \ldots, e_{k-1} \notin C) &\geq 1 - \frac{2}{n-k+1}.
    \end{align*}
    So,
    \begin{align*}
        P(\text{we find } C) = P(e_1, \ldots, e_{n-2} \notin C) &\geq \left(\frac{n-2}{n}\right) \left(\frac{n-3}{n-1}\right) \ldots \left(\frac{1}{3}\right) \\
        &\approx \frac{2}{n(n-1)}.
    \end{align*}
\end{proof}
\textbf{Conclusion:} The algorithm succeeds with probability $\geq  \frac{2}{n(n-1)}$.

\noindent \textbf{New Goal:} Find min-cut with $\geq 1 - \frac{1}{n^2}$ probability. 

\noindent \textbf{Idea:} Repeat $t$ times and return the best cut we find. (We will determine the value for $t$ below.)

The probability that all $t$ trials fail is $\leq \left(1 - \frac{2}{n(n-1)}\right)^t$. We want this to be $\leq \frac{1}{n^2}$. We utilize the inequality below:
\begin{align*}
    \left(1 - \frac 1k\right)^k \leq \frac 1e \leq \left( 1 - \frac 1k\right)^{k-1}.
\end{align*}
So,
\begin{align*}
    \left(1 - \frac{2}{n(n-1)}\right)^t &\leq \frac{1}{e^{t \cdot \frac{2}{n(n-1)}}}\implies t = 2\binom n2 \log n \\
    &\leq \frac{1}{e^{2\log n}} = \frac 1{n^2},
\end{align*}
as desired. 
\subsection{Karger-Stein Algorithm}
We start with the following motivation. We have:
\begin{align*}
    P(\text{first }n/2 \text{ contractions succeed}) \geq \frac{n-2}{n} \frac{n-3}{n-1} \ldots \frac{n/2 - 1}{n/2 + 1} \approx \frac{1}{4}.
\end{align*}
This is because after we telescope, the bottom two terms are both near $n$ while the top two terms are near $n/2$. So now, if the first $n/2$ contractions succeed, what's the probability that the next $n/4$ succeed?
\begin{align*}
    P(\text{next }n/4 \text{ contractions succeed}) \geq \frac{n/2 - 2}{n/2} \frac{n/2 - 3}{n/2 - 1} \ldots \frac{n/4 - 1}{n/4 + 1} \approx \frac{1}{4}.
\end{align*}
And this pattern continues. 
\newpage \noindent We now go over the Karger-Stein algorithm:
\begin{itemize}
    \item Represent the problem as a tree.
    \item For the first $n/2$ contractions, run Karger's algorithm 4 times.
    \item For the next $n/4$ contractions, run Karger's algorithm 16 times.
    \item So on, until the base case of the algorithm which is when there are two vertices left. 
\end{itemize}


\begin{center}
\begin{tikzpicture}[
    level distance=1.5cm,
    level 1/.style={sibling distance=4cm},
    level 2/.style={sibling distance=1cm},
    ]
    
    \node {$n$} % Root node
      child {node {$n/2$}
        child {node {$n/4$}
          child [level distance=0.25cm] {edge from parent[draw=none] child {node {$\vdots$} edge from parent[draw=none]}}
        }
        child {node {$n/4$}
          child [level distance=0.25cm] {edge from parent[draw=none] child {node {$\vdots$} edge from parent[draw=none]}}
        }
        child {node {$n/4$}
          child [level distance=0.25cm] {edge from parent[draw=none] child {node {$\vdots$} edge from parent[draw=none]}}
        }
        child {node {$n/4$}
          child [level distance=0.25cm] {edge from parent[draw=none] child {node {$\vdots$} edge from parent[draw=none]}}
        }
      }
      child {node {$n/2$}
        child {node {$n/4$}
          child [level distance=0.25cm] {edge from parent[draw=none] child {node {$\vdots$} edge from parent[draw=none]}}
        }
        child {node {$n/4$}
          child [level distance=0.25cm] {edge from parent[draw=none] child {node {$\vdots$} edge from parent[draw=none]}}
        }
        child {node {$n/4$}
          child [level distance=0.25cm] {edge from parent[draw=none] child {node {$\vdots$} edge from parent[draw=none]}}
        }
        child {node {$n/4$}
          child [level distance=0.25cm] {edge from parent[draw=none] child {node {$\vdots$} edge from parent[draw=none]}}
        }
      }
      child {node {$n/2$}
        child {node {$n/4$}
          child [level distance=0.25cm] {edge from parent[draw=none] child {node {$\vdots$} edge from parent[draw=none]}}
        }
        child {node {$n/4$}
          child [level distance=0.25cm] {edge from parent[draw=none] child {node {$\vdots$} edge from parent[draw=none]}}
        }
        child {node {$n/4$}
          child [level distance=0.25cm] {edge from parent[draw=none] child {node {$\vdots$} edge from parent[draw=none]}}
        }
        child {node {$n/4$}
          child [level distance=0.25cm] {edge from parent[draw=none] child {node {$\vdots$} edge from parent[draw=none]}}
        }
      }
      child {node {$n/2$}
        child {node {$n/4$}
          child [level distance=0.25cm] {edge from parent[draw=none] child {node {$\vdots$} edge from parent[draw=none]}}
        }
        child {node {$n/4$}
          child [level distance=0.25cm] {edge from parent[draw=none] child {node {$\vdots$} edge from parent[draw=none]}}
        }
        child {node {$n/4$}
          child [level distance=0.25cm] {edge from parent[draw=none] child {node {$\vdots$} edge from parent[draw=none]}}
        }
        child {node {$n/4$}
          child [level distance=0.25cm] {edge from parent[draw=none] child {node {$\vdots$} edge from parent[draw=none]}}
        }
      };
\end{tikzpicture}
\end{center}

\noindent \textbf{Time Analysis}: The first level takes $\tilde{O}(n^2)$ time. The second level takes $\tilde{O}\left(\frac{n^2}{2}\right)$ time. The third level takes $\tilde{O}\left(\frac{n^2}{4}\right)$ time. So the total time is $\tilde{O}(n^2)$. 

\noindent So, at level $i$:
\begin{itemize}
    \item There are $4^i$ edges.
    \item The work per edge $\leq (|\text{remaining vertices}|)^2 = \tilde{O}\left(\frac{n}{2^i}\right)^2 = \tilde{O}\left(\frac{n^2}{4^i}\right)$. 
\end{itemize}
This implies that the cost of the $i$th level is less than $\tilde O(n^2)$. Summing, we get a total cost $\tilde O(n^2)$.

Now we consider the probability that our algorithm succeeds. Again, we have a $4$-ary tree with a depth of $t = O(\log n)$. Each edge flips a coin where $P(\text{heads}) = \frac 14$. So what we want is a $t$ length chain of all heads. We can show the following result:
\begin{align*}
    P(\text{good path}) \geq \frac 1t \geq \Omega \left(\frac{1} {\log n}\right).
\end{align*}

The goal is an algorithm that succeeds with probability greater than $1 - \frac{1}{n^2}$. 

\noindent \textbf{Exercise:} $O(\log^2 n)$ repetitions suffice.

\section{Galton Walton Tree}
Building on the previous problem, we introduce the concept of a Galton Walton tree. This is a $4$-ary tree with $t$ levels where each edge flips a biased coin. The probability of heads is $1/4$. A root-to-leaf path in this tree is ``successful'' if all edges are heads.

\noindent \textbf{Claim:} $P(\text{successful path}) \geq \frac{1}{t}$.
\begin{proof}
    Pretend the paths are independent. Then we have that
    \begin{align*}
        P(\text{No successful path}) = \left(1 - \frac{1}{4^t}\right)^{4^t} \leq \frac{1}{e}.
    \end{align*}

    This is not a very interesting result. It is worth noting that the paths are extremely dependent on each other. Consider a path of length $t-2$ and creating two separate paths by diverging at the last edge. This should illustrate the dependence of the paths.

    Now let $f_t$ the probability of tailing in a tree with $t$ levels. We can define this as a recurrence on $f_{t-1}$. We have:
    \begin{align*}
        f_t = \left(\frac{1}{4} f_{t-1} + \frac{3}{4}\right)^4.
    \end{align*}

    To proceed with induction on $t$, we first prove the following identity:
    \begin{align*}
        1 - 1/k < \frac{1}{e^{1/k}} < 1 - 1/(k+1).
    \end{align*}
    This is a useful identity to have in our back pocket. We have:
    \begin{align*}
        (1 - 1/k)^k &< 1/e < (1 - 1/k)^{k-1} \\
        \implies 1 - 1/k &< 1/e^{1/k} \\ 1/e^{1/(k-1)} &< 1 - 1/k \implies 1/e^{1/k} < 1 - 1/(k+1).
    \end{align*}
    Putting it all together, we get the desired result. We can now proceed with induction.

    Our hypothesis: $f_{t-1} \leq 1 - \frac{1}{t-1}$. What we want to show is that $f_t \leq 1 - \frac 1t$. 

    We have:
    \begin{align*}
        f_t &\leq \left( \frac 34 + \frac 14 \left( 1 - \frac{1}{t-1} \right)\right)^4 \\
        &= \left(1 - \frac{1}{4(t-1)}\right)^4 \\
        &\leq \left(\frac{1}{e^{\frac{1}{4(t-1)}}}\right)^4 \\
        &= \frac{1}{e^{1/(t-1)}} \tag{by the above identity}\\
        &< 1 - \frac{1}{t}. \tag{by the above identity}
    \end{align*}
    This completes the induction, as well as the entire proof.
\end{proof}


\chapter{Lovasz Local Lemma}{}
\mlenma{Lovasz Local Lemma}{Let $A_1, A_2, \ldots, A_n$ be ``bad events.'' Suppose:
\begin{itemize}
    \item $P(A_i) < p$ for some $p$.
    \item Each $A_i$  has a ``dependency set'' $D_i \subseteq \{A_1, A_2, \ldots, A_n\}$ such that $A_i$ is independent of $\{A_1, A_2, \ldots, A_n\} \setminus D_i$ and $|D_i| \leq d$. 
\end{itemize}
If $p < \frac{1}{ed}$, then there exists an outcome where none of the $A_i$ occur. That is, $P(\text{no bad events}) \geq \frac{1}{d^n}$.}
\noindent \textbf{Application:} KSAT. In KSAT, we have a bunch of boolean variables $x_1, x_2, \ldots$. A K-SAT formula is a conjunction of $n$ clauses, where each clause selects $k$ unique variables that cannot take a specific form. For example, if we have $k=3$, we might have a clause that says $(x_1, x_2, x_3) \neq (0, 1, 0)$. The question is whether we can find a satisfying assignment of these variables. 

\thm{}{Suppose each clause overlaps in variables $\leq d-1$ other clauses. Also suppose $\frac{1}{2^k} < \frac{1}{ed}$. Then, there exists a satisfying assignment.}
\begin{proof}
    $x_1, x_2, \ldots$ are random $0, 1$ variables. Let $A_i$ be the event that clause $i$ is not satisfied. Then $P(A_i) \leq \frac{1}{2^k}$. By the LLL, there exists a satisfying assignment.
\end{proof}

\section{Algorithmic Lovasz Local Lemma}
\subsection{Algorithmic Setting}
We have random variables $X_1, X_2, \ldots, X_n$ and events $A_i$ that are determined by some subset of the $X_i$s. We can think about this as a dependency graph where all the $X_i$ and $A_i$ are nodes, and edges are drawn between $X_i$ and $A_j$ if $X_i$ determines $A_j$.

\mlenma{Algorithmic Lovasz Local Lemma}{Suppose each $A_i$ overlaps with at most $d-1$ other $A_j$s on which $X_i$s they use. Suppose $P(A_i) < \frac{1}{4d}$. Then there exists a polynomial time algorithm$^*$ that finds an outcome of $X_i$s such that none of the $A_i$s occur.}

\newpage
\subsection{Moser's Fix-it Algorithm}
% "i was going to present the proof from the paper, but a few days ago i came up with a much better proof". allegedly was a great proof solving a 30 year old open problem and slams it in 30 minutes. two weeks later, moser leaves academia and works in finance

\textbf{Algorithm:}
\begin{itemize}
    \item Sample $X_1, X_2, \ldots, X_m$.
    \item While $\exists$ bad event $A_i$:
    \begin{itemize}
        \item Pick $A_i$ arbitrarily.
        \item Resample the $X_i$'s that determine $A_i$.
    \end{itemize}
\end{itemize}

\thm{Moser's Theorem (KSAT)}{Consider KSAT:
\begin{itemize}
    \item $m$ vars, $X_1, \ldots, X_m$.
    \item $n$ clauses $C_1, \ldots, C_n$.
    \item Each $C_i$ depends on $\leq d$ other $C_i$'s. 
\end{itemize}
If $8d < 2^k$, then Moser's algorithm performs $t = O(n)$ (really $O \left( \frac nd \right)$) fix-its in expectation. }
\noindent \textbf{Proof Idea:}
Use random bits, for example $R=0010010100101\ldots$. Construct a ``transcript'' $T$ such that if you tell me $T$, I can recover all the random bits you used. The interesting property of $T$ is that:
\begin{align*}
    |T| \leq (\text{number of fix-its}) \cdot (2 \lg d) + O(n).
\end{align*}
The number of random bits we use is less than the number of fix-its times $k$. We know that $k > (2 + \lg d) \cdot 100$. 

If Moser's algorithm runs for a long time, say $> 100n$ fix-its, then
\begin{align*}
    |T| \leq 100n \cdot (2 \lg d) + O(n).
\end{align*}
Then,
\begin{align*}
    \text{number of random bits} &= 100n \cdot k \\
    &> 100n \cdot (2 + \lg d) \cdot 100.
\end{align*}
This means $|T| << \text{number of random bits}$. So if we can show that such a $T$ exists, then we can show that the expected number of fix-its to be pretty small.

\subsection{Proof of Moser's Theorem}
\begin{proof}
    We start by defining variables we are going to use:
    \begin{itemize}
        \item $R =$ random bits used by the algorithm. 
        \item $M_1 =$ final values of $X_i$'s. 
        \item $M_2 =$ sequence of which clauses we fix.
    \end{itemize}
    \mlenma{}{$M_1$ and $M_2$ fully determine $R$.}
    \begin{proof}
        Work backwards! Consider the last $C_i$ that was fixed. We know its variables after the fix and the variables before the fix. So we can ``undo'' the fix. We can keep doing this until we get to the beginning.
    \end{proof}
    \mlenma{}{$M_2$ can be compressed to $n + t(2 + \lg d)$ bits.}
    We'll proceed by assuming the lemma first. So now we have $|M_1| = m$ and $|R| = m + tk$. Also since $8d < 2^k$< we have $3 + \lg d \leq k$. So, $|R| \geq m + t(3 + \lg d)$. 

    Now,
    \begin{align*}
        |M_1| + |M_2| &= n + m + t(2 + \lg d) \\
        &\leq |R| + n - t.
    \end{align*}
    If $E[t] > n$, then $E[|M_1 \circ M_2|] < E[|R|]$. This is a contradiction by the incompressibility of random bits. So, $E[t] \leq n$ as desired.
\end{proof}

    \noindent Now we return to proving the second lemma. 
    \begin{proof}
    We start by defining processes.

    \noindent Define process($C_i$):
    \begin{itemize}
        \item Resample $C_i$'s variables.
        \item While $\exists$ another $C_j$ that is broken and depends on $C_i$:
        \begin{itemize}
            \item Select such a $C_j$.
            \item process($C_j$).
        \end{itemize}
    \end{itemize}


    \noindent Define Algorithm:
    \begin{itemize}
        \item For $i = 1, \ldots, n$:
        \begin{itemize}
            \item If $C_i$ is broken, process($C_i$).
        \end{itemize}
    \end{itemize}
    \textbf{Claim:} When the algorithm finishes, all clauses holds.

    \noindent \textbf{Claim:} Algorithm terminates with probability 1.

    \noindent We now look towards constructing $M_2$:
    \begin{itemize}
        \item $A =$ Part 1: $n$ bits, which $C_i$'s get fixed in for loop.
    \end{itemize}
    Now recall the process algorithm. Notice here that for each $C_j$, there are only $d+1$ options. This means that we can communicate $C_j$ with $\lg(d+1)$ bits, which we'll call $q$. We now rewrite process considering a string $B$.

    \noindent Define process($C_i$):
    \begin{itemize}
        \item Resample $C_i$'s variables.
        \item While $\exists$ another $C_j$ that is broken and depends on $C_i$:
        \begin{itemize}
            \item Select such a $C_j$.
            \item $B = B + ``1" + q$.
            \item process($C_j$).
        \end{itemize}
        \item $B = B + ``0"$.
    \end{itemize}
So given $M_2$, $A$ lets us recover which clauses the for loop fixes. $B$ lets us figure out which clauses are fixed within each process subroutine. This means we can recover the entire sequence of clause fixes that the algorithm performs.

So, $|A| = n$ and $|B| = t(2 + \lg d)$. This means $|A \circ B| = n + t(2 + \lg d)$, which completes the proof.
\end{proof} 


\chapter{Chernoff Bounds}
We start with an example. Suppose I flip 100 fair coins. What is the probability that the number of heads is greater than or equal to 75? Turns out,
\begin{align*}
    P(\text{heads} \geq 75) &\approx \frac{1}{3.5 \times 10^6}.
\end{align*}
\thm{}{
    Consider $n$ fair coin flips, $X_1, X_2, \ldots, X_n$ iid with $X_i = \begin{cases} 0 & \text{w.p. $1/2$} \\ 1 & \text{w.p. $1/2$} \end{cases}$. Let $X = \sum X_i$. Then,
    \begin{align*}
        P\left(X > \frac n2  + K \sqrt n\right) \leq e^{-\Omega(K^2)}.
    \end{align*}
}
\cor{}{
    \[P\left( X \geq \frac 34 n \right) \leq e^{-n/32}\]
}
\noindent You don't actually need $X_1, \ldots, X_n$ to be independent. It suffices to have that, for any $i$ and any outcomes of $X_1, \ldots, X_{i-1}$,
\begin{align*}
    P(X_i = 1 \mid X_1, \ldots, X_{i-1}) \leq 1/2.
\end{align*}
\nt{``Azuma's Inequality'' is a citable Chernoff bound. Alternatives include ``multiplicative version of Azuma's inequality'' and ``Freedman's inequality.''}
\noindent We turn to the proof of the theorem.
\begin{proof}
    Idea: suppose we have a function $f$ such that:
    \begin{itemize}
        \item $f$ is non-negative.
        \item $f$ is increasing on $[n/2, n]$.
    \end{itemize}
    Then, 
    \begin{align*}
        P\left(x \geq \frac  n2 + K \sqrt n\right) \leq P\left(f(x) \geq f\left(\frac n2 + K \sqrt n\right)\right) \leq \frac{E[f(x)]}{f\left(\frac n2 + K \sqrt n\right)}. 
    \end{align*}
    \ex{Trying $f$'s}{
        $f(x) = \left(x - \frac n2\right)^2$. This clearly satisfies the requirements. This tells us that:
        \begin{align*}
            P\left(x > \frac n2 + K \sqrt n\right) &\leq \frac{E[f(x)]}{K^2n} \\
            &= \frac{\Var(X)}{K^2n}.
        \end{align*}
        We use the linearity of variance. We have $\text{Var}(X_i) = \frac 14$. So, $\text{Var}(X) = \frac n4$. Thus,
        \begin{align*}
            P\left(x > \frac n2 + K \sqrt n\right) \leq \frac{1}{4k^2}.
        \end{align*}
        This is exactly Chebyshev's inequality.
    }
    \noindent We now find an $f$ that proves our result. Try $f(x) = e^{\lambda x}$ for some $\lambda \in (0, 1)$. This is approximately ${(1+\lambda)}^x$. We first find the expectation:
    \begin{align*}
        E[f(x)] = E[e^{\lambda \sum_i X_i}] &= E\left[\prod_i e^{\lambda X_i}\right] \\
        &= \prod_i E[e^{\lambda X_i}] \tag{by independence} \\
        &= \left( \frac{1 + e^\lambda}{2} \right)^n.
    \end{align*}
    \noindent \textbf{Fact:} \begin{align}
        e^\lambda &= 1 + \lambda + \frac{\lambda^2}{2!} + \frac{\lambda^3}{3!} + \cdots \\
        &\leq 1 + \lambda + \lambda^2 \cdot\left(\frac {1}{2!} + \frac {1}{3!} + \cdots \right) \\
        &\leq 1 + \lambda + \lambda^2.
    \end{align}
    It follows that
    \begin{align*}
        E[e^{\lambda X_i}] &\leq 1+ \frac 12 \lambda + \frac 12 \lambda^2.
    \end{align*}
    Now we make use of the other fact that $1 + x \leq e^x$. So
    \begin{align*}
        E[e^{\lambda X_i}] &\leq 1+ \frac 12 \lambda + \frac 12 \lambda^2 \\
        &\leq e^{\lambda/2 + \lambda^2/2}.
    \end{align*}
    Thus,
    \begin{align*}
        E[f(x)] &\leq e^{n(\lambda/2 + \lambda^2/2)}.
    \end{align*}
    So now,
\begin{align*}
    P\left(X > \frac n2 + K \sqrt n\right) &\leq \frac{E[f(x)]}{f\left(\frac n2 + K \sqrt n\right)} \\
    &\leq \frac{e^{n(\lambda/2 + \lambda^2/2)}}{e^{n\lambda/2 + \lambda K \sqrt n}} \\
    &= e^{n\lambda^2 / 2 - \lambda K \sqrt n}.
\end{align*}
Now we try to find an appropriate value for $\lambda$. We try:
\begin{align*}
    \lambda K \sqrt n &= n\lambda^2\\
    \frac{K}{\sqrt n} &= \lambda.
\end{align*}
This yields:
\begin{align*}
    e^{n\lambda^2 / 2 - \lambda K \sqrt n} &= e^{-\lambda K \sqrt  n /2} = e^{-k^2/2},
\end{align*}
as desired.
\end{proof}
\section{Quicksort Algorithm (1961)}
We review the algorithm for quicksort:
\begin{itemize}
    \item Pick random pivot $P$.
    \item Partition elements into 2 pieces: elements smaller/larger than $P$.
    \item Recurse on the pieces.
\end{itemize}
\thm{}{The running time of quicksort satisfies $T = O(n \lg n)$ with probably at least $1 - \frac{1}{n^2}$.}
\begin{proof}
    Note that the time is $\leq \#\text{levels} \cdot O(n)$. So what we have to prove is that with high probability, the depth of the algorithm doesn't exceed $\lg n$. So,
    \begin{align*}
        P(\text{max depth} \geq 1000\lg n) &\leq P(\exists \text{element with max depth} \geq 1000 \lg n) \\
        &\leq n \cdot P(\text{a given $v$ has depth} \geq 1000 \lg n).
    \end{align*}
    We want to show that the probability on the last line is $\leq \frac{1}{n^3}$. We have that
    \begin{align*}
        P(\text{next subproblem} \leq 3/4 \text{current size}) \geq \frac 12.
    \end{align*}
    Let $P_1, P_2, \ldots$ be subproblems in $v$'s path. Let 
    \begin{align*}
        X_i = \begin{cases}
            1 & P_{i+1} \text{exists and } |P_{i+1}| > \frac 34 |P_i| \\
            0 & \text{otherwise}
        \end{cases}
    \end{align*}
    We observe that $P(X_i = 1 \mid X_1, \ldots, X_{i-1}) \leq 1/2$. So by Chernoff bound, if we look at $X = X_1 + X_2  + \cdots + X_{1000 \lg n =: m}$, we get:
    \begin{align*}
        P\left( X \geq \frac 34 m\right) \leq e^{-m/32} <<< \frac{1}{n^3}.
    \end{align*}
    If $X \leq \frac 34 m$, either $P_m$ doesn't exist or $|P_m| \leq n \cdot \left( \frac 34 \right)^{m/4} = n \cdot \left( \frac 34 \right)^{250 \log n} << 1$, which is a contradiction. This means $P_m$ in fact does not exist. As such, we are able to conclude that we have a running time of $O(n \lg n)$ with the desired probability.
\end{proof}

\newpage
\section{Revisiting Theorem 4.0.1}
We start with warm-ups to give an alternative proof to Theorem 4.0.1.

\subsection{Poor Man's Chernoff Bound}
\thm{}{\[P(X \geq 2K\sqrt n) \leq \frac{1}{2^K}.\]}
\begin{proof} We make use of the core facts below:

    \noindent\textbf{Core fact 1}: $P(X \geq 2\sqrt n) \leq \frac{1}{4}$, by Chebyshev's inequality.

    \noindent\textbf{Core fact 1'}: $P($any prefix has sum $\geq 2 \sqrt n) \leq \frac 12$. This is a direct result of \textbf{Core fact 1}, and the implication will be shown in the homework. 
    
    We can graph the running sum as a function of time and track the times that we hit the values $2\sqrt n$, $4 \sqrt n$, and $6\sqrt n$ and call them $t_1, t_2, t_3,$ et cetera. What the core fact is saying is that $P(t_1$ exists$) \leq \frac 12$. Then, we can build the the following chain:
    \begin{align*}
        P(t_1 \text{ exists}) &\leq \frac 12 \\
        P(t_2 \text{ exists} \mid t_1 \text{ exists}) &\leq \frac 12 \\
        &\vdots \\
        P(t_i \text{ exists} \mid t_{i-1} \text{ exists}) &\leq \frac 12.
    \end{align*}
    So, 
    \begin{align*}
        P(X \geq 2K \sqrt n) \leq P(t_K \text{ exists}) \leq \frac{1}{2^K},
    \end{align*}
    as desired.
\end{proof}

\subsection{Chernoff Bound for Geometric Random Variables}
\thm{}{Let $X_1, \ldots, X_n$ be non-negative independent random variables such that for all $j \in \NN$,
\begin{align*}
    P(X_i \geq j) \leq p^j.
\end{align*}
Then $X = \sum X_i$ satisfies
\begin{align*}
    P(X \geq 2n) \leq (4p)^n.
\end{align*}
}
\begin{proof}
    $X \geq 2n \implies \sum \lfloor X_i \rfloor \geq n \implies \exists \langle Y_1, \ldots, Y_n\rangle =: \vec Y$ such that 
    \begin{itemize}
        \item $\sum Y_i  = n$.
        \item $X_i \geq Y_i \, \forall i$. 
    \end{itemize}
    For a given option $\langle Y_1, \ldots, Y_n\rangle$ for $\vec Y$,
    \begin{align*}
        P(\vec Y \text{ occurs}) &= P(X_i \geq Y_i \forall i) \\
        &\leq \prod_{i} P(X_i \geq Y_i) \\
        &\leq \prod_i p^{Y_i} \\
        &= p^{\sum_i Y_i} \\
        &= p^n.
    \end{align*}
    Now we bound the number of options for $\vec Y$. We can express $\vec Y$ as a binary string, for example:
    \begin{align*}
        \underbrace{000}_{Y_1 = 3}1\underbrace{0}_{Y_2=1} 1\underbrace{}_{Y_3 = 0}1 \ldots   
    \end{align*}
    So the number of $0$'s is $\sum Y_i = n$ and the number of $1$'s is $n$. This means that the number o options satisfying $\sum Y_i = n$ is less than the number of binary strings of length $2n$ which is $4^n$. 

    So,
    \begin{align*}
        P(X \geq 2n) \leq P(\text{some } \vec Y \text{ occurs}) \\
        &\leq (\text{\# of options for } \vec Y) \cdot P(\vec Y \text{ occurs}) \\
        &\leq 4^n \cdot p^n \\
        &= (4p)^n.
    \end{align*}
\end{proof}




\subsection{Anti-(Chernoff Bound)}
\thm{}{
\begin{align*}
    P\left(X \geq \frac K4 \sqrt n\right) \geq \frac{1}{4^{K^2}}
\end{align*}
}

\begin{proof}
    Break the coins into $K^2$ groups. So each group has $m = \frac{n}{K^2}$ coins. 

    \noindent \textbf{Core fact}: $P\left( X \geq \frac 14 \sqrt n\right) \geq \frac 14$.
    
    Let $C_i$ be the sum of the $i$-th group. So by the core fact, 
    \begin{align*}
        P\left(C_i \geq \frac 14 \sqrt m\right) \geq \frac 14.
    \end{align*}
    So,
    \begin{align*}
        P\left(\text{every } C_i \geq \frac 14 \sqrt m\right) \geq \frac{1}{4^{K^2}}
    \end{align*}
    If this occurs, then 
    \begin{align*}
        X = \sum C_i \geq K^2 \cdot\left(\frac 14 \sqrt m\right) = \frac K4 \sqrt n.
    \end{align*}
\end{proof}
\newpage 
\subsection{Real Chernoff Bound}
\thm{}{
    \begin{align*}
        P(X \geq 16k \sqrt n) \leq \frac{1}{2^{K^2}}.
    \end{align*}
}
\begin{proof}
    We start by defining $Y_i = \max\left(0, \frac{C_i}{8\sqrt m}\right)$. We can apply the Poor Man's Chernoff bound to each $C_i$ to get
    \begin{align*}
        P(C_i \geq 2\ell \sqrt m) &\leq \frac{1}{2^\ell} \\
        \implies P\left(Y_i \geq \frac{2\ell \sqrt m}{8\sqrt m}\right) &\leq \frac{1}{2^{\ell}} \\
        \implies P(Y_i \geq \ell) & \leq \frac{1}{2^{4\ell}} \leq \frac{1}{16^\ell}.
    \end{align*}
    This is saying that $Y_i$ are geometric random variables. So by the geometric random variable bound,
    \begin{align*}
        P\left(\sum Y_i \geq 2 K^2\right) \leq (4 \cdot 1/16)^{K^2} = \left(\frac 14 \right)^{K^2}.
    \end{align*}
    But also 
    \begin{align*}
        X \geq 16K \sqrt n &\implies \sum C_i \geq 16K \sqrt n \\
        &\implies \sum Y_i \geq \frac{16K \sqrt n}{8 \sqrt m} = 2K^2.
    \end{align*}
\end{proof}


\thm{}{
    Let $X_1, X_2, \ldots, X_n$ be independent $\{0, 1\}$ coin flips with $P(X_i = 1) = p$ for some $p \leq \frac 12$. Define $X = \sum X_i$, $\mu = E[X] = np$. 

    Small-deviation regime: For $K \in \{1, \ldots, \sqrt \mu\}$,
    \begin{align*}
        P(X \geq \mu + K\sqrt \mu) \leq e^{-\Theta(K^2)}. 
    \end{align*}
    Large-deviation regime: For $J \geq 2$ such that $J\mu \leq n$, 
    \begin{align*}
        P(X \geq J\mu) \leq \frac{1}{J^{\Theta(J \mu)}}.
    \end{align*}
}
\begin{proof}
    (large-deviation case)

    \noindent \underline{\textbf{Warmup 1}}
    \newline \newline
    \noindent \textbf{Theorem 1} (Poor Man's Chernoff Bound)
        If $\mu \leq 1$, then
        \begin{align*}
            P( X \geq K) \leq \mu^K.
        \end{align*}
    \begin{proof}
        \textbf{Core fact} $P(X \geq 1) \leq \mu$ by Markov's inequality. 

        Now repeat the ideas with $t_1, t_2, \ldots$ to say that $P(X \text{ reaches } i) = \mu^i$. 
    \end{proof}
    \newpage
    \noindent \underline{\textbf{Warmup 2}} This was already done prior as it is the exact same.
    \newline \newline
    \noindent \underline{\textbf{Warmup 3}}
    \newline \newline
    \noindent \textbf{Theorem 3}
    \begin{align*}
        P(X \geq J) \geq \frac{1}{J^{O(J\mu)}}
    \end{align*}
    \begin{proof}
        We'll have $J \mu$ groups where each group has mean $\frac 1J$. 

        \noindent \textbf{Core fact}: If $\mu \leq 1$, then $P(X \geq 1) \geq \Omega(\mu)$. 

        By the core fact, each $C_i$ is $\geq 1$ with probability $\geq \Omega \left( \frac 1J\right)$. So, 
        \begin{align*}
            P(\text{every } C_i \geq 1) = P(X \geq J\mu) \geq \Omega \left(\frac 1J\right)^{J \mu}.
        \end{align*}
        ``Squiggly square to signify almost proof done.''
    \end{proof}

    So the Poor Man's Chernoff Bound implies $P(C_i \geq K) \leq \frac{1}{J^K}$. Then by Chernoff for geometric random variables,
    \begin{align*}
        P\left(\sum C_i \geq 2J \mu\right) \leq \left(\frac{4}{J}\right)^{J \mu}
    \end{align*}

\end{proof}



\section{More Chernoff Bounds}
\thm{Chernoff Bound}{
    Let $X_1, X_2, \ldots, X_n \in [0, 1]$ be independent random variables. Let $X = \sum X_i$ and $\mu = E[X]$. Then, for $k \in O(\sqrt \mu)$, 
    \begin{align*}
        P(|X - \mu| \geq k \sqrt{\mu}) \leq e^{-\Omega(k^2)}.
    \end{align*}
    And for $J \geq 2$,
    \begin{align*}
        P(X \geq J\mu) \leq \left( \frac 1 J\right)^{\Omega(J\mu)}.
    \end{align*}
}
\noindent Small-deviation regime:
\begin{itemize}
    \item There are $k^2$ chunks.
    \item So each has expected value $\approx \frac{\mu}{k^2}$.
    \item If each exceeds expected value by $\sqrt{\frac{\mu}{k^2}}$, then the total sum is $\geq k^2 \cdot \sqrt{\frac{\mu}{k^2}} = k \sqrt \mu$. 
\end{itemize}
\noindent Large-deviation regime:
\begin{itemize}
    \item $J \mu$ chunks.
    \item Each chunk has expected value $\approx \frac 1J$.
    \item If each chunk is at least 1, then the total sum is at least $J \mu$. 
\end{itemize}
\thm{Bennett's Inequality, Bernstein's Bound}{
    Same bound, but let $v = \Var(X)$. 
    \begin{align*}
        P(X - E[X] \geq k\sqrt v) &\leq e^{-\Theta(k^)} \text{ for } k \in \{1, 2, \ldots, \sqrt v\} \\
        P(X - E[X] \geq Jv) &\leq \left( \frac 1J \right)^{\Omega(Jv)} \text{ for } J \geq 2.
    \end{align*}
}

\subsection{Adaptive Version}
\thm{Azuma's Inequality}{
    Let $X_1, X_2, \ldots, X_n \in [-1, 1]$. Suppose $E[X_i | X_1, \ldots, X_{i-1}] = 0$. Then, $X = \sum X_i$ satisfies
    \begin{align*}
        P(X \geq K\sqrt n) \leq e^{-\Omega(K^2)}.
    \end{align*}
}
\noindent Fancier versions:

Have random variables $X_1, X_2, \ldots, X_n \in [0, 1]$ which are revealed one after the other. After $X_1, \ldots, X_{i-1}$ are revealed, Alice gets to select the distribution $P_i$ for $X_i$ and then $X_i$ is revealed from $P_i$. 

Let $\mu_i = E[X_i \mid P_i]$, $v_i = \Var(X_i \mid P_i)$, $X = \sum X_i$, $\mu = \sum \mu_i$ and $v = \sum v_i$. As a corollary to Azuma's inequality,
\begin{align*}
    P( X \geq \mu + k \sqrt n) \leq e^{-\Omega(k^2)}.
\end{align*}

Now if $\mu$ is deterministically at most $\bar \mu$, then
\begin{align*}
    P(X \geq K \sqrt{\bar \mu} + \bar \mu) &\leq e^{-\Omega(K^2)} \\
    P(X \geq J \bar\mu) &\leq \left( \frac 1J \right)^{\Omega(J\bar \mu)}.
\end{align*}

\thm{Freedman's Inequality}{
    If $v$ is deterministcally at most $\bar v$, then
    \begin{align*}
        P(X \geq K \sqrt{\bar v} + \mu) &\leq e^{-\Omega(K^2)} \text{ for } K \in \{1, 2, \ldots, \sqrt v\} \\
        P(X \geq \mu + J \bar v) &\leq \left( \frac 1J \right)^{\Omega(J\bar v)} \text{ for } J \geq 2.
    \end{align*}
}   


\nt{Bill has used the following bound in $\sim 90 \%$ of his papers. 

``Sometimes when you don't need this bound you want to find a way to use it anyway.''}
\thm{McDiarmid's Inequality}{
    Let $X_1, X_2, \ldots X_n$ be independent. Let $F : X_1, \ldots, X_n \to \RR$. Suppose: If I change some $X_i$ to a new value $\bar X_i$,
    \begin{align*}
        |F(X_1, \ldots, X_i, \ldots, X_n) - F(X_1, \ldots, \bar X_i, \ldots, X_n)| \leq 1.
    \end{align*}
    Then, 
    \begin{align*}
        P(|F - E[F]| \geq K\sqrt n) \leq e^{-\Omega(K^2)}.
    \end{align*}
}

\ex{}{
    Consider a random graph:
    \begin{itemize}
        \item $n$ vertices.
        \item each $(i, j)$ is an edge with probability $\frac 12$.
    \end{itemize}
    Consider the chromatic number $\chi(G)$. There's a way to show that
    \begin{align*}
        P(|\chi(G) - E[\chi(G)] | \geq k \sqrt n) \leq e^{-\Omega(k^2)}.
    \end{align*}
    We cannot define the random variables $X_i$ on whether an edge is included or not. This is because we end up getting $n^2$ random variables, which makes the inequality flop. We proceed by defining $X_1, X_2, \ldots, X_n$ where $X_i$ is the number of edges from vertex $i$ to vertices $ > i$. Then the result follows directly from McDiarmid's inequality.
}
\noindent \textit{Proof Idea}: Let 
\begin{align*}
    Y_0 &= E[F] \\
    Y_1 &= E[F \mid X_1] \\
    Y_2 &= E[F \mid X_1, X_2] \\
    \vdots \\
    Y_n &= E[F \mid X_1, \ldots, X_n] = F.
\end{align*}
Then let 
\begin{align*}
    Z_1 = Y_1 - Y_0 \\
    Z_2 = Y_2 - Y_1 \\
    \vdots \\
    Z_n = Y_n - Y_{n-1}.
\end{align*}
\noindent Claim 1:  $E[Z_i \mid Z_1, \ldots, Z_{i-1}] = 0$. 

\noindent Claim 2: $|Z_i| \leq 1$. 

So by Azuma, $P(\sum Z_i > K \sqrt n) \leq e^{-\Omega(K^2)}$. But $\sum Z_i = Y_n - Y_0 = F - E[F]$. 


\chapter{Oblivious Routing}
We start with an $n$-node hypercube:
\begin{itemize}
    \item $n$ vertices: $1, \ldots n$.
    \item edges: For each pair $i,j$ if $i,j$ differ in exactly one bit, then we have edges $(i, j)$ and $(j, i)$. 
\end{itemize}
This graph contains $n \log n$ edges. Each vertex $i$ wants to send a message to another vertex $\pi(i)$, where $\pi$ is a permutation. Each edge can only send 1 message per time step (may have to wait in queue before goign down edge). 

\noindent Goal: Complete all the message sending within time $O(\log n)$.

\noindent Oblivious Routing: Each message decides its path at the beginning of time. 

\thm{Brebner, Valiant (Leslie) (1981)}{
    With probability $\geq 1 - \frac{1}{n^{1000}}$, we can achieve $O(\log n)$ total time. 
}
\begin{proof}
    Algorithm:
    \begin{itemize}
        \item To send $i \to \pi(i)$; 
        \begin{itemize}
            \item Pick random $r_i \in \{1, \ldots, n\}$.
            \item Phase 1: Send $i \to r_i$.
            \item Phase 2: Send $r_i \to \pi(i)$.
        \end{itemize}
    \end{itemize}
    We wait until a preplanned time to begin phase 2. To send $i \to r_i$, use bit-fixing where we fix bits from left to right. Let $u_i :=$ message going from $i \to r_i$, $P_i=$ path that $u_i$ takes. Focus on a fixed $i$. 

    \noindent Claim: With high probability, $u_i$ completes $p_i$ within $O(\log n)$ time. 

    \mlenma{Lemma 1}{
        If $i \neq j$, then edges in $P_i \cap P_j$ form a continguous subpath.
    }
    \begin{proof}
        Consider an edge in $P_i$. For example if we have
        \begin{align*}
            010111011001 \\
            010111111001
        \end{align*}
        we can say that the 011001 agrees with $i$ and $j$, while the 0101111 agrees with $r_i$ and $r_j$. 

        The values of $k$ where paths agree,
        \begin{align*}
            \{ k \mid r_i, r_j \text{ agree first $k$ bits}\} \cap \{k \mid i, j \text{ agree on final $n-k+1$ bits}\}.
        \end{align*}
        Note that the first set is a prefix of $1, \ldots n$ and the second set is a suffix of $1, \ldots n$. Therefore, their intersection is a continguous interval. 
    \end{proof}
    \mlenma{Lemma 2}{
        The number of time steps that $u_i$ spends in queues is at most $|S_i|$, where $S_i = \{ j \neq i \mid P_i \cap P_j \neq \emptyset\}$.
    }
    \begin{proof}
        Say that a message $w$ that is currently on $P_i$ has ``delay'' $d$ if 
        \begin{itemize}
            \item $w$ is on the $t$-th step of $P_i$ for some $t$. 
            \item We are in the $t+d$-th time step of the algorithm.
        \end{itemize}
        If $u_i$ waits in a queue and its delay goes from $d$ to $d+1$, give a note with the number $d$ on it to whoever used the edge that $u_i$ wanted to use. Later on, if a message has a note ``$d$'', and $w$ waits in a queue:
        \begin{itemize}
            \item if edge $\notin P_i$, keep the note
            \item otherwise, pass the note to message that's using the edge we're waiting on.
        \end{itemize}
        If note $d$ is currently held by a message $w$ and if $w$ is still on $P_i$, then $w$'s delay is exactly $d$. This means that no two notes will be held by the same message. This implies that the number of notes is equal to the number of messages hold notes, which is then less or equal to $|S_i|$.
    \end{proof}
    Now let $e$ be an edge. Define $m_e :=$ the number of messages tha tuse $e$.
    \mlenma{Lemma 3}{
        $E[m_e] \leq 1$.
    }
    \begin{proof}
        What does it mean to use edge $e$? The first $k$ bits are free variables for $j$ while the last $n-k + 1$ agree with $j$. So the number of options for $j = 2^{k-1}$. FOr each such $j$, 
        \begin{align*}
            P(r_j \text{ has correct first $k$ bits}) = \frac{1}{2^k}.
        \end{align*}
        So,
        \begin{align*}
            E[\text{\# $j$ that use $e$}] \leq \sum_{\text{valid } j} P(\text{$j$ uses $e$}) \leq 2^{k-1} \frac{1}{2^k} = \frac 12.
        \end{align*}
    \end{proof}

    \mlenma{Lemma 4}{
        With high probability, $|S_i| \leq O(\log n)$.
    }
    \begin{proof}
        First, fix $P_i$. 
        \begin{align*}
            |S_i| = \sum_{j \neq i} \mathbb I [P_i \cap P_j \neq \emptyset],
        \end{align*}
        where $\mathbb I$ is the indicator function. Note that these are all independent random variables determined by $r_j$. This is exactly the type of thing we want to apply Chernoff bounds to. 

        Define $\mu := E[|S_i|] \leq \sum_{e \in P_i} E[\text{\# messages $j\neq i$ that use $e$}] \leq \frac{\log n}{2}$. 
    \end{proof}
    So let's say $\mu = \frac{\log n}{2}$. So,
    \begin{align*}
        P(|S_i| \geq J\mu) \leq \left( \frac 1J \right)^{\Omega(J\mu)} \leq \frac 12^{\Omega(J \mu)} \leq \frac{1}{\poly n}.
    \end{align*}
\end{proof}
\newpage 
\noindent Suppose I flip a fair coin repeatedly:

\noindent Question 1: How many flips do I need to get $\geq 1$ heads with high probability. 

\noindent Answer: $O(\log n) \implies P(\text{all tails}) \leq \frac{1}{2^{\Theta (\log n)}}$.

\noindent Question 2: How man flips do I need to get $\geq \log n$ heads with high probability?

\noindent Answer: $O(\log n)$. If I flip $1000 \log n$ coins, $P(\text{$< \log n$ heads}) = P(> 999 \log n \text{ tails})$. This means I exceeded the mean greatly as $E[\text{\# of tails}] = 500 \log n$. If $\mu = E[T]$, then
\begin{align*}
    T &> 1.9 \mu \\
    &> \mu + 0.9 \sqrt \mu ( \sqrt \mu).
\end{align*}
And the probability of this is $\leq e^{-\Omega(\log n)}$.

\chapter{Metric Embeddings}
\dfn{Metric Space}{
    A metric space $X$ is a set equipped with a function $d: X \times X \to \RR$ such that 
    \begin{enumerate}
        \item $d(x, y) \geq 0$ for all $x, y \in X$, $d(x,y) = 0$ if and only if $x = y$.
        \item $d(x,y) = d(y,x)$ for all $x,y \in X$
        \item $d(x,z) \leq d(x,y) + d(y, z)$ for all $x,y,z \in X$
    \end{enumerate}
}
\ex{}{
    \begin{itemize}
        \item $\ell^p$-distance. For $p \geq 1$, $X = \RR^k$. Then 
        \begin{align*}
            d_{\ell^p}(x, y) = \left( \sum_{i=1}^k |x_i - y_i|^p \right)^{1/p}. 
        \end{align*}
        \item Graph-distance. Take any undirected graph with positive weights, then you can talk about the graph distance between two vertices where 
        \begin{align*}
            d(v, u) = \text{graph distance}.
        \end{align*}
    \end{itemize}
}
\dfn{Metric Embeddings}{
    Given two metric spaces, $(X_1, d_1), (X_2, d_2)$, a map $\phi: X_1 \to X_2$ is a metric embedding with distortion at most $\alpha$ if $\exists \beta$ scaling parameter such that  $\forall x, y \in X_1$, $d_1(x, y)$ is within a factor of $\alpha$ of $\beta d_2(\phi(x), \phi(y))$. That is,
    \begin{align*}
        \beta d_2(\phi(x), \phi(y)) \leq d_1(x, y) \leq \alpha \beta d_2(\phi(x), \phi(y)).
    \end{align*}
}
\newpage 
\thm{Bourgain's Theorem}{
    Let $(X, d)$ be an $n$-point metric. Then there exists a function $\phi : (X; d) \to (\RR^{O(\log ^2 n)}, d_{\ell^1})$ such that the distortion is $O(\log n)$. 
}


\begin{proof}
    We review the algorithm. Let $c$ be a large constant. 
    \begin{itemize}
        \item For $i = 1, \ldots, \log n$,
        
        For $j = 1, \ldots, c \log n$,

        $S_{i, j} = \text{sample each element $x \in X$ with probability $2^{-i}$.}$
    \end{itemize}
\end{proof}

\mlenma{Key Lemma}{For $i \in \{1, \ldots, t\}$ and $j \in \{1, \ldots, c \log n\}$, we have with probability $\Omega(1)$ that 
\begin{align*}
    |d(x, S_{i, j}) - d(y S_{i, j})| \geq r_{i+1} - r_i.
\end{align*} }


I LEFT LECTURE HERE, FILLIN LATER 


\newpage
\section{Day 2}
\thm{}{Let $(X, d)$ be an $n$-point metric space. Suppose that $\forall x \neq y \in X$, $d(x, y) \in [1, n^2]$. We will embed $X$ into a tree metric $(T, d_T)$ with the following properties:
\begin{itemize}
    \item depth of tree is $O(\log n)$.
    \item For any root-leaf path, edge weights are decreasing powers of 2.
    \item Points in $X$ get mapped to leaves of the tree.
\end{itemize}
There exists a randomized embedding $\phi: X \to (T, d_T)$, where $T$ is also a random variable, such that 
\begin{itemize}
    \item $d_T(\phi(x), \phi(y)) \geq d(x, y)$.
    \item $E[d_T(\phi(x), \phi(y))] \leq O(\log n) d(x, y)$. 
\end{itemize}}
\begin{proof}
    The algorithm:
    \begin{enumerate}
        \item Pick a random permutation $\pi = \pi_1, \ldots, \pi_n$ of the elements of $X$.
        \item Pick uniformly random $\beta \in [1, 2]$. Define $\beta_i := 2^{i-1} \cdot \beta$.

        For each $x \in X$ and $i \in \{0, \ldots, 2\log n\}$, define 
        \begin{align*}
            \text{center}_i(x) = \text{first $\pi_j$ in permutation to satisfy $d(x, \pi_j) \leq \beta_i$}.
        \end{align*}
        \item Construct $\phi$ as follows:
        
        Each node represents a set of elements. Root at level $2\log n + 1 = $ all of $X$. For any node $w$ at level $i+1$, if node $>$ 1 element.
        \begin{itemize}
            \item Group $w$'s elements $z$ by $\text{center}_i(z) \to $ child nodes. 
        \end{itemize}
        Two elements $a, b \in w$ are in the same child iff $\text{center}_i(a) = \text{center}_i(b)$.
        \item Finally, edge from level $i+1$ to level $i$ has weight $2^{i}$. 
    \end{enumerate}
    Analysis:

    We first claim that for $x, y\in X$, $d_T(\phi(x), \phi(y)) \geq d(x, y)$. 
    \begin{proof}
        Observe that at level $i$, if $x,y$ in the same node, then $d(x, y) \leq 2\beta_i \leq 2^{i+1}$. So let $i$ be the highest level the path goes through, this implies that $d(x, y) \leq 2^{i+1}$. But also $d_T(\phi(x), \phi(y)) \geq 2\cdot 2^i \geq d(x, y)$ as desired.
    \end{proof}

    \mprop{}{\begin{align*}
        E[d_T(\phi(x), \phi(y))] \leq O(\log n) d(x, y).
    \end{align*}}
    \begin{proof}
        \begin{align*}
            d_T(\phi(x), \phi(y)) &\leq O(\text{largest edge weight along the path}) \\
            &\leq \sum_{i=0}^{2\log n} \mathbb I (\text{center}_i(x) \neq \text{center}_i(y)) \cdot 2^{i+1}.
        \end{align*}
        \newpage
        So we have 
        \begin{align*}
            E[d_T(\phi(x), \phi(y))] &\leq \sum_{i=0}^{2 \log n} \int_{r = 2^{i-1}}^{2^i} P(B_i = r) \cdot P(\text{center}_i(x) \neq \text{center}_i(y) \mid \beta_i = r) 2^{i+1} \text dr \\
            &=  \sum_{i=0}^{2 \log n} \int_{r = 2^{i-1}}^{2^i} \frac{1}{2^{i-1}} \cdot P(\text{center}_i(x) \neq \text{center}_i(y) \mid \beta_i = r) 2^{i+1} \text dr \\
            &=  \sum_{i=0}^{2 \log n} \int_{r = 2^{i-1}}^{2^i}  P(\text{center}_i(x) \neq \text{center}_i(y) \mid \beta_i = r) \text dr.
        \end{align*}
        If we look through $\pi_1, \pi_2, \ldots, \pi_n$, the first of $\{\text{center}_i(x), \text{center}_i(y)\}$ to appear is $w$ implies that $w$ ``cuts'' $(x, y)$ (and the centers are non-equal).

        Let $w_1, w_2, \ldots, w_n$ be elements of $x$ sorted by distance from $w_i$ to $\{x, y\}$. 

        \begin{align*}
            \sum_{i=0}^{2 \log n} \int_{r = 2^{i-1}}^{2^i}  P(\text{center}_i(x) \neq \text{center}_i(y) \mid \beta_i = r) \text dr &\leq \sum_{j=1}^n \sum_{i=0}^{2 \log n} \int_{r = 2^{i-1}}^{2_i} P(w \text{ cuts } (x, y) \mid \beta_i = r) \text dr.
        \end{align*}
        Call the integrand $\Xi$. We consider what must occur for $w_j$ to cut $(x, y)$. Suppose WLOG $d(w_j, x) \leq d(w_j, y)$. We need 
        \begin{itemize}
            \item $d(w_j, x) \leq \beta_i \leq d(w_j, y)$.
            \item $w_j$ must appear before $w_1, \ldots, w_{j-1}$ in $\pi$, which has probability $\leq \frac 1j$. 
        \end{itemize}
        So we get 
        \begin{align*}
            \sum_{j=1}^n \sum_{i=0}^{2 \log n} \int_{r = 2^{i-1}}^{2_i} P(w \text{ cuts } (x, y) \mid \beta_i = r) \text dr &\leq \sum_{j=1}^n \int_{z = d(w_j, x)}^{d(w_j, y)} \frac 1j \text dz \\
            &= \sum_{j=1}^n \frac 1j |d(w_j, y) - d_w(w_j, x)| \\
            &\leq d(x, y) \sum_{j=1}^n \frac 1j = d(x, y) O(\log n).
        \end{align*}
    \end{proof}
    
\end{proof}
\newpage
\section{Day 3}
\thm{Johnson-Lindenstrauss Lemma (1994)}{
    Let $\epsilon \in (0, 1)$. Let $S \subseteq \RR^d$ be a set of $n$ points. There exists a map $\phi : S \to \RR^{O(\log n) \cdot \epsilon^{-2}}$ which satisfies:
    \begin{align*}
        \forall a , b \in S, (1 \pm \epsilon)\Vert a- b \Vert_2 = \Vert \phi(a) - \phi(b) \Vert_2.
    \end{align*}
}
\noindent Our goal is to construct a linear $\phi$ such that for any $a \in \RR^d$, with high probability in $n$, 
\begin{align*}
    \Vert \phi (a) \Vert_2 = (1 \pm \epsilon) \Vert a \Vert_2. 
\end{align*}
If we can do this, then $\forall a, b \in S$, we have with high probability that
\begin{align*}
    \Vert \phi(a) - \phi(b) \Vert_2  &= \Vert \phi(a-b)\Vert_2 \\
    &= (1 \pm \epsilon) \Vert a- b \Vert_2.
\end{align*}
Then by applying the union bound over all $\binom n2$ pairs,  can complete the proof. 

\noindent We start with a warmup task. We want to construct a linear function $f: \RR^d \to \RR$ such that $E[f(a)^2] = \Vert a \Vert_2^2$. If we can find a function like this that also suffices:
\begin{align*}
    P(f(a)^2 > K\Vert a \Vert_2^2) \leq e^{-\Omega(K)},
\end{align*}
then we can finish the theorem off. 

\noindent \textbf{Review of normal random variables:} Let $v = $ variance, $\sigma  = \sqrt v$ (standard-deviation). Then $N(0, v)$ represents a normal random variable with mean 0 and standard deviation $\sigma$. 

\noindent \textbf{Fact 1:} Let $X \sim \mathrm N(0, \sigma^2)$. Then
\begin{align*}
    P(|X| \geq K \sigma) < 2e^{-K^2/3}.
\end{align*}

\noindent \textbf{Fact 2:} If $X \sim \mathrm(0, v_1)$ and $Y \sim \mathrm(0, v_2)$, and $X \perp Y$, then $X + Y \sim \mathrm(0, v_1 + v_2)$. 

\noindent \textbf{Fact 3:} Let $X \sim \mathrm N(0, v)$, $c \in \RR$. Then
\begin{align*}
    cX \sim \mathrm N(0, c^2v).
\end{align*}
For vector $\bar a = (a_1, \ldots, a_d)$, define 
\begin{align*}
    f(a) = \sum_{i=1}^d X_i \cdot a_i
\end{align*}
where $X_i$ is an independent random variable such that $X_i \sim \mathrm N(0, 1)$. That means $f(a) \sim \mathrm N(0, \Vert a \Vert_2^2)$. 

\cor{1}{$E[f(a)^2] = \Vert a \Vert_2^2$.}
\cor{2}{$P(f(a)^2 > K\Vert a \Vert_2^2) \leq e^{-\Omega(K)}$.}
\begin{proof}
    By the previous Chernoff bound,
    \begin{align*}
        P(|f(a)| > J \Vert a \Vert_2) &\leq e^{-J^2/3} \\
        P(f(a)^2 > J^2 \Vert a \Vert_2^2) &\leq 2e^{-J^2/3}.
    \end{align*}
    By setting $K = J^2$, we get the corollary.
\end{proof}
\newpage 
\noindent We now turn to the proof of the Johnson-Lindenstrauss lemma.
\begin{proof}
    Define $\phi(\vec a)$ as 
    \begin{align*}
        \phi(\vec a) = \langle \phi_1(\vec a), \ldots, \phi_r(\vec a)\rangle
    \end{align*}
    where $r = c\epsilon^{-2}\log n$ where $c$ is a large constant. Now we define $\phi_i$. We take the same argument as in the previous proof:
    \begin{align*}
        \phi_i(a) = \frac{1}{\sqrt r}\sum_{j=1}^d X_{i, j} \cdot a_j
    \end{align*}
    where all the $X_{i, j} \sim \mathrm N(0, 1)$ and are independent. We want to show:
    \begin{align*}
        \Vert \phi(\vec a) \Vert_2^2 &= (1 \pm \epsilon) \Vert a \Vert_2^2 \\
        \iff \sum \phi_i(\vec a)^2 &= (1 \pm \epsilon) \Vert a \Vert_2^2.
    \end{align*}
    We now go over a Chernoff bound for geometric random variables. Let $X_1, \ldots, X_n$ be independent. Let $\mu = E[\sum X_i]$. Suppose each $X_i$ satisfies 
    \begin{align*}
        P(|X_i| \geq K) \leq e^{-\Omega(k)}.
    \end{align*}
    Let $X = \sum X_i$. In the small-deviation regime ($K \in [1, \sqrt \mu]$), we have 
    \begin{align*}
        P(|X - E[X]| \geq K \sqrt{\mu}) \leq e^{-\Omega(k^2)}.
    \end{align*}
    Then for $J \geq 2$, we have 
    \begin{align*}
        P( X \geq J \mu) \leq \left( \frac 12 \right)^{\Omega(J\mu)}.
    \end{align*}
    Note that the large-deviation regime bound we're used to does not apply because it is not true in the $n=1$ case. Now for each $\phi_i$, we have 
    \begin{align*}
        r \cdot \phi_i(\vec a) \sim \mathrm N(0, \Vert a \Vert_2^2).
    \end{align*}
    If we wanted to normalize this, we would get 
    \begin{align*}
        Y_i := \left(\frac{\sqrt r \cdot \phi_i(\vec a)}{\Vert a \Vert_2}\right)^2 \sim \mathrm N(0, 1)^2.
    \end{align*}
    $Y_i$ has mean 1, and $|Y_i|$ is a geometric random variable. This means
    \begin{align*}
        P\left[\left| \sum_{i=1}^r Y_i - r \right| \geq K \sqrt r \right] \leq e^{-\Omega(k^2)}.
    \end{align*}
    Let's set $K\sqrt r = \epsilon r$, or that $K = \epsilon \sqrt r$. So,
    \begin{align*}
        P\left[\sum_{i=1}^r Y_i \neq (1 \pm \epsilon)r \right] = e^{-\Omega(\epsilon^2 r)}.
    \end{align*}
    But $-\Omega(\epsilon^2 r) = \frac{1}{\poly(n)}$. What follows is:
    \begin{align*}
        \sum \frac{r \phi_i(\vec a)^2}{\Vert a \Vert_2^2} &= (1 \pm \epsilon) r \\
        \implies \sum \phi_i(\vec a)^2 &= (1 \pm \epsilon) \Vert a \Vert_2^2.
    \end{align*}
    This is exactly what we wanted to prove. 
\end{proof}
\newpage
\noindent We now prove the facts about normal random variables. 
\cor{from Central Limit Theorem}{
    Suppoose you define $X^{(n)} := \sum_{i=1}^n X_i$ where $X_i$ takes on $1$ or $-1$ with probability $1/2$. Then as $n \to \infty$, $\frac {\sqrt v}{\sqrt n} X^{(n)}$ converges pointwise to a single distribution. This is the definition of $\mathrm N(0, v)$. 
}
\noindent As a shorthand, 
\begin{align*}
    \lim_{n\to\infty} \frac{\sqrt v}{n}X^{(n)} \sim N(0, v).
\end{align*}
We skip the first fact because it's an easy Chernoff bound. Fact 3 is also straight-forward from the definition of a random variable. So we will just prove that if we have $A, B\sim \mathrm N(0, v_1), \mathrm N(0, v_2)$, then $A+ B \sim \mathrm N(0, v_1 + v_2)$. As an easy example, take $v_1 = v_2 = 1$. So then the distribution of $A+  B$ would be 
\begin{align*}
    \lim_{n \to \infty} \frac{1}{\sqrt n} (X^{(n)} + Y^{(n)})  &= \lim_{n \to \infty} \frac{1}{\sqrt n} ( X_1 + \cdots + X_n + Y_1 + \cdots + Y_n) \\
    &= \lim_{n \to \infty} \frac{1}{\sqrt n} X^{(2n)} \\
    &= \lim_{n \to \infty} \frac{\sqrt 2}{\sqrt m} X^{m} \tag{where $m = 2n$} \\
    &= \lim_{m \to \infty} \frac{\sqrt 2}{\sqrt m} \cdot X^{(m)} \sim \mathrm N(0, 2). 
\end{align*}

\chapter{The Train Track Problem}
While very sleep deprived and on a train ride, Bill crossed a bridge and got very confused because he thought there were no tracks below him. So he thought of a problem regarding the number of wheels on the left and right quarter of the train, and how much track is actually necessary for a train to go forever. 
\qs{}{
    Consider a set $W$ of wheels where $W \subseteq \{0, \ldots, \ell-1\}$ and $|W| = n$. Then consider the track $T \subseteq \{1, \ldots, 2\ell-1\}$. A track $T$ is valid if 
    \begin{align*}
        \forall i \in \{1, \ldots, \ell\}, (i + W) \cap T \neq \emptyset.
    \end{align*}
    Our goal is to use as little track as possible. 
}
\noindent A clear observation we can make is that if $W$ is an arithmetic progression (or rather, evenly spread out), we can say that there exists a valid track $T$ of size $O\left( \frac \ell n \right)$ (just put a track at every length of the quarter train).
\begin{center}
    \includegraphics*[width=10cm]{train.png}
\end{center}
There are two theorems we want to prove:
\thm{}{
    For any $W$, there exists a valid $T$ such that $|T| \leq O\left( \frac{\ell}{n} \cdot \log n \right)$.
}
\thm{}{
    $\exists W$ such that all valid $T$ satisfy $|T| \geq \Omega \left( \frac \ell n \cdot \log n \right)$. 
}
\newpage
\noindent \textit{Proof of Theorem 7.0.1.} Given $W$, WLOG, $0 \in W$. 

\noindent \textbf{Step 1:} Let 
\begin{align*}
    T_1 = \text{ each $i \in \{1, \ldots, 2\ell - 1\}$ with probability $\frac{\ln n}{n}$.} 
\end{align*}
\textbf{Step 2:} Let 
\begin{align*}
    F = \{ i \in \{1, \ldots, \ell \} \mid (i + W) \cap T_1  = \emptyset \}.
\end{align*}
That is, ``$W$ falls through $T$, at point $i$''.

\noindent \textbf{Step 3:} Define $T = T_1 \cup F$. 
\nt{Because $0 \in W$, $T$ is valid. }
\noindent We move to analyzing $T$. First, we have 
\begin{align*}
    E[|T|] &= E[|T_1|] + E[|F|] \\
    &\leq \frac{2\ell \ln n}{n} + \text{ ?}.
\end{align*}
So,
\begin{align*}
    E[|F|] &= \sum_{i=1}^\ell P[(i + W) \cap T = \emptyset] \\
    &= \sum_{i=1}^\ell  \prod_{j \in (i + W)} P(j \notin T_1) \\
    &= \sum_{i = 1}^\ell \left( 1 - \frac{\ln n}{n}\right)^n \\
    &\approx \sum_{i=1}^\ell \frac 1n \\
    &= \frac \ell n.
\end{align*}
So 
\begin{align*}
    E[|T|] &\leq \frac{2\ell \ln n}{n} + \frac \ell n \\
    &= \ell \left( \frac{2 \ln n + 1}{n} \right) \\
    &= O \left( \frac{\ell \ln n}{n} \right).
\end{align*}
From this, it follows that there exists a valid $T$ such that $|T| \leq O\left( \frac{\ell}{n} \cdot \ln n \right)$ that is an outcome of this randomized algorithm. 

\hfill $\qed$
\newpage
\noindent \textit{Proof of Theorem 7.0.2.} Without loss of generality, we can set $\ell = 2n$ for the rest of this section. 

\mprop{}{Let $W = \text{include each $i \in \{0, \ldots, 2n - 1\}$ independently with probability $1/2$}$. Let $T \subseteq \{1 , \ldots, 4n\}$ satisfy 
\begin{align*}
    |T| \leq \frac{\ln n}{100}.
\end{align*}
Then, $P(T \text{ valid}) < \dfrac{1}{n^{\Omega(\sqrt n)}}$. 
}
\noindent We'll defer the proof of the proposition to later. But assuming we have proved it, then 
\begin{align*}
    P\left(\exists \text{ any $T$ of size $\frac{\ln n}{100}$ that is valid}\right) &\leq \sum_{T\subseteq \{1, \ldots, 4n-1\}, |T| \leq \frac{\ln n}{100}} P(T \text{ valid}) \\
    &\leq \left(4n\right)^{\frac{\ln n}{100}} \cdot \frac{1}{n^{\Omega(\sqrt n)}} = \frac{1}{n^{\Omega(\sqrt n)}}.
\end{align*}
Notice that with probabilty at least $1/2$, $|W| \geq n$. So, there exists $W$ such that $|W| \geq n$ such that there is no valid $T$ of size $\leq \frac{\ln n}{100}$. It follows that there is $W$ of exactly size $n$ such that this is true. 

So given Proposition 7.0.1, we will have proved Theorem 7.0.2.

\hfill $\qed$

\noindent \textit{Proof of Proposition 7.0.1.} Let 
\begin{align*}
    F = \{ i \in \{1, \ldots, 2n \} \mid (i + W) \cap T  = \emptyset \}.
\end{align*}
We have 
\begin{align*}
    E[|F|] &= \sum_{i=1}^{2n} P[(i + W) \cap T = \emptyset] \\
    &= \sum_{i=1}^{2n} \prod_{j \in T} \underbrace{P(j \notin W + i)}_{\geq 1/2} \\
    &\geq 2n \cdot \left( \frac 12 \right)^{T} \\
    &\gg n^{0.9}.
\end{align*}
If $f := |F|$, then $f$ is a function $f(X_1, \ldots, X_n)$ where $X_i = \mathbb I[i  \in W]$. And if you toggle $X_i$, $f$ changes by at most $|T| \leq \frac{\ln n}{100}$. Now we are in a perfect spot to apply McDiarmid's inequality:
\begin{align*}
    \underbrace{P(f = 0)}_{T \text{ is valid}} &\leq P(f < E[f] - n^{0.9}) \\
    &\leq n^{-\Omega(\sqrt n)}. \tag{$k = \frac{n^{0.4}}{\Theta (\lg n)}$}
\end{align*}
\hfill $\qed$
\newpage
\chapter{Poissonization}
\dfn{}{
    A random variable $X$ is Poisson distributed, $X \sim \mathrm{Poisson}(\lambda)$ if 
    \begin{align*}
        P(X = k) = \frac{e^{-\lambda}\lambda^k}{k!}
    \end{align*}
    for $\lambda, k \geq 0$. 
}
\nt{Another definition is $\mathrm{Binomial}\left( n, \frac \lambda n\right)$ as $n \to \infty$. }
The above note is true because we can write 
\begin{align*}
    \binom nk \left(\frac \lambda n \right)^k \left( 1- \frac \lambda n \right)^{n-k} &\approx  \left(\frac{en}{k}\right)^k \left(\frac \lambda n\right)^k \left( 1 - \frac \lambda n \right)^{n-k} \\
    &\approx \left(\frac ek \right)^k \lambda^k e^{-\lambda(1 - k/n)} \\
    &\approx \left( \frac{1}{k!}\right)^k \lambda^k e^{-\lambda} \\
    &= \frac{e^{-\lambda}\lambda^k }{k!}
\end{align*}
as desired.

\noindent Now we review some properties of the Poisson distribution:
\begin{itemize}
    \item If $X \sim \mathrm{Poisson}(\lambda)$, then $E[X] = \Var(X) = \lambda$.
    \item If $X \sim \mathrm{Poisson}(\lambda)$ and $Y \sim \mathrm{Poisson}(\mu)$, then $X + Y \sim \mathrm{Poisson}(\lambda + \mu)$.
    \item If $X \sim \mathrm{Poisson}(\lambda)$, then for any $c > 0$, 
    \begin{align*}
        P(|X - \lambda| \geq c) \leq 2e^{\frac{-c^2}{2(c + \lambda)}}
    \end{align*}
\end{itemize}
In the old balls and bins problems, we would drop $n$ balls into $m$ bins. This was awkward because the number of balls in different bins aren't independent random variables. So consider a different variant of the problem:
\qs{New balls and bins}{
    Suppose $k \sim \mathrm{Poisson}(n)$. We toss $k$ balls uniformly at random into $m$ bins.
}
\newpage
\clm{}{
    The number of balls in bin $i$ is distributed $\mathrm{Poisson}\left(\frac nm\right)$ for all $i$.
    The number of balls in bin 1, bin 2,$\ldots$, bin $m$ are independent random variables.
}
\begin{proof}
    We take $m=2$ as this is easily generalizable. Let $X_1, X_2$ be the number of balls in bins 1 and 2 respectively. We claim that
    \begin{align*}
        P(X_1 = i, X_2 = j) = P(\mathrm{Poisson}(n/2) = i) \cdot P(\mathrm{Poisson}(n/2) = j).
    \end{align*}
    We have 
    \begin{align*}
        P(X_1 = i, X_2 = j) &= P(k = i + j) \cdot P(\mathrm{Binomial}(i+j, 1/2) = i) \\
        &= \frac{e^{-n} n^{i+j}}{(i+j)!} \cdot \binom{i+j}{i} \left(\frac 12\right)^{i+j} \\
        &= e^{-n/2} \cdot e^{-n/2} \cdot n^i \cdot n^j \cdot \left(\frac {1}{i!}\right)\left( \frac{1}{j!}\right) \cdot \left(\frac 12\right)^i \cdot \left(\frac 12\right)^j \\
        &= P(\mathrm{Poisson}(n/2) = i) \cdot P(\mathrm{Poisson}(n/2) = j)
    \end{align*}
    as desired. This proves the first claim. Now,
    \begin{align*}
        P(X_1 = i) &= \sum_j P(X_1 = i, X_2 = j) \\
        &= \sum_j P(\mathrm{Poisson}(n/2) = i) \cdot P(\mathrm{Poisson}(n/2) = j) \\
        &= P(\mathrm{Poisson}(n/2) = i).
    \end{align*}
    Together, this gives us the second claim that $X_1 \perp X_2$. 
\end{proof}
To learn Poissonization, we'll review the coupon collector. So let $X$ be the number of steps until we get all $n$ coupons.
\clm{}{
    For any constant $c$,
    \begin{align*}
        \lim_{n \to \infty} P(X \geq n \log n + cn) = 1 - e^{-e^{-c}}.
    \end{align*}
}
\begin{proof}
    To prove this claim, we use Poissonization and view it as a balls and bins problems. So instead of $n$ balls and $m$ bins, we have $n\log n + cn$ steps and $n$ coupons. Similarly, we'll have $k \sim \mathrm{Poisson}(n\log n + cn)$ steps and $n$ coupons. Here are the steps we take:
    \begin{enumerate}
        \item We want to show
        \begin{align*}
            \lim_{n \to \infty} P(\text{see every coupon after $k$}) = 1 - e^{-e^{-c}}.
        \end{align*}
        \item Then we show
        \begin{align*}
            P(\text{see every type of coupon after $n \log n + cn$}) = P(\text{see every type of coupon after $k$}) \pm o(1).
        \end{align*}
    \end{enumerate}
    Together, this will show the desired result. So suppose we do $k \sim \mathrm{Poisson}(n  \log n + cn)$ steps. Then the number of coupons of each type is independently distributed as $\mathrm{Poisson}(\log n + c)$. So, 
    \begin{align*}
        P(\text{we see all the coupons}) &= (1 - P(\mathrm{Poisson}(\log n + c) = 0))^n \\
        &= (1 - e^{-\log n + c})^n \\
        &= (1 - e^{-c}/n)^n \\
        &\approx 1 - e^{-e^{-c}}
    \end{align*}
    as desired.
    \newpage
    \noindent So now we de-Poissonify. 
    \begin{enumerate}[label=(\alph*)]
        \item We want to show 
        \begin{align*}
            P(\text{see all coupons after $b$ steps}) = P(\text{see all coupons after $b \pm n^{0.9}$ steps}) \pm o(1).
        \end{align*}
        So let $X$ be the number of boxes needed to get all the coupons. The two events we consider are $X \geq b - n^{0.9}$ and $X \geq b + n^{0.9}$. So,
        \begin{align*}
            P(X \geq b + n^{0.9}) &= P(X \geq b - n^{0.9}) + P(\text{find the last coupon in boxes $b - n^{0.9} + 1, \ldots, b+n^{0.9}$}) \\
            &\leq P(X \geq b - n^{0.9}) + \frac{2n^{0.9}}{n} \\
            &= P(X \geq b - n^{0.9}) + o(1).
        \end{align*}
        This sort of proves the claim.
        \item So now we can have that 
        \begin{align*}
            P(|k - (n \log n + cn) | \geq n^{0.9}) = o(1).
        \end{align*}
        This follows directly from the tail-bound that we mentioned earlier.
    \end{enumerate}
    This completes the overall result.
\end{proof}



\newpage
\chapter{Hash Tables}
\section{Random-Probing Hash table}
For this section, a hash table has two important operations:
\begin{enumerate}
    \item $\texttt{insert(key)}$ (adding)
    \item $\texttt{query(key)}$ (exists)
\end{enumerate} 
Also for this section, we'll support up to $(1-\epsilon)n$ total insertions. The hash table will be an array with $n$ slots. 

\begin{center}
    \begin{tikzpicture}
        \foreach \x in {1,...,12} {
            \draw (\x,0) rectangle (\x+1,1);
            \node at (\x+0.5,0.5) {};
        }
    \end{tikzpicture}
\end{center}
So to insert a key $k$, we consider a sequence of random hashes $h_1(k), h_2(k), \ldots$ and each $h_i(k)$ is uniformly random in $1, \ldots, n$ and $h_i$'s are independent. We place the key in the first available position out of the sequence $h_1, h_2, h_3, \ldots$. 

\noindent To query $k$, we try $h_1(k), h_2(k), \ldots$ until 
\begin{itemize}
    \item we find $k$, in which case we return true.
    \item or we find a free slot, in which case we return false.
\end{itemize}
Observations:
\begin{enumerate}
    \item If the hash table is $1 - \delta$ full, then 
    \begin{align*}
        E[\text{time for next insertion}] = \delta^{-1}.
    \end{align*}
    \item Suppose I fill $1-\epsilon$ full. Then,
    \begin{align*}
        E[\text{time to query random elements of those present}] &\approx \frac n2 \cdot O(2) + \frac n4 \cdot O(4) + \frac n8 \cdot O(8) + \cdots + \epsilon n \cdot O(\epsilon^{-1})\\
        &\approx \underbrace{1 + 1 + \cdots + 1}_{\lg \epsilon^{-1} \text{ terms}} \\
        &= O(\lg \epsilon^{-1}).
    \end{align*}
\end{enumerate}
``Open-addressed hash tables'': Any hash table that is ``like random-probing'' but that possibly uses a different distribution for its probe sequence
\begin{align*}
    h_1(k), h_2(k), \ldots
\end{align*}
Some examples include linear probing and double hashing. 

\newpage
% create conjecture env
\clm{Ullman '72}{
    For any open address hash table, 
    \begin{align*}
        E[\text{time to query rnadom elements of those present}] \geq \Omega(\lg \epsilon^{-1}).
    \end{align*}
}
\noindent This was proven by Andrew Yao in 1985. The title of his paper was ``Uniform Hashing is Optimal.'' He ended the paper with the following conjecture:
\clm{Yao '85}{
    For any open address hash table, if you fill it to $(1-\epsilon)$ full, the expected time of the next insertion is at least $\Omega(\epsilon^{-1})$. 
}
\thm{}{
    Possible to achieve 
    \begin{align*}
        E[\text{insertion time}] = O( \lg^2 \eps^{-1}).
    \end{align*}
}
\noindent We now turn attention towards the coupon collector problem:
\qs{Coupon Collector}{We have a hat with $n$ coupons in it. We repeatedly draw random coupons from it, with replacement. The classical question is how many draws in expectation do we need to collect all $n$ coupons? The answer to this is $O(n \log n)$.}
\thm{}{Let $\delta \in (0, 1)$ be such taht $\delta \geq \frac{1}{n^{1/10}}$. Let $X$ be the number of draws until we have collected a $(1-\delta)$ fraction of all the coupons. Then, with high probability, $X \leq 2n\ln \delta^{-1}$. }
\begin{proof}
    Do $2n \ln \delta^{-1}$ draws. Let $Y$ be the number of distinct coupons not sampled. Then,
    \begin{align*}
        E[Y] &= \sum_{i=1}^n P(\text{coupon $i$ is not sampled}) \\
        &=  n \cdot \left(1  - \frac 1n\right)^{2n \ln \delta^{-1}} \\
        &\approx n \delta^2.
    \end{align*}
    Applying McDiarmid's to $Y$,
    \begin{itemize}
        \item $Y$ is a function of $m = 2n \ln \delta^{-1}$ random variables.
        \item Each of these random variables can only affect $Y$ by $\pm 1$. 
    \end{itemize}
    So by McDiarmid's, we have with high probability that
    \begin{align*}
        &Y \leq E[Y] + \tilde{O}(\sqrt m) \\
        &\implies Y \leq \delta^2 n + \tilde{O}(\sqrt m) \\
        &\implies \text{number of sampled coupons} \geq n - \underbrace{(\delta^2 n + \tilde O(\sqrt m))}_{\lll \delta n} \\
        & \geq (1-\delta)n.
    \end{align*}
\end{proof}
\newpage
\noindent We now learn about a new hash table design. Break up the table as follows:
\begin{itemize}
    \item $p_1$ is the first $\frac n2$ slots.
    \item $p_2$ is the next $\frac n4$ slots.
    \item $\vdots$
    \item Do this until there is a $\epsilon n / 100$ size part.
\end{itemize}
Probe sequence:
\begin{itemize}
    \item Try $100 \lg \epsilon^{-1}$ probes in $p_1$.
    \item Try $100 \lg \eps^{-1}$ probes in $p_2$.
    \item $\vdots$
\end{itemize}
If this fails for all parts, 
\begin{itemize}
    \item Do $100 \log n$ probes in the final part.
    \item Loop through the array until there is a free spot in the worst case.
\end{itemize}
The most recent step happens with very very low probability, but is required for correctness of the data structure. 

Now suppose $p_i$ gets $\geq 50\%$ full for some $i$. This means that $p_{i-1}$ must have gotten at least $\frac{|p_i|}{2} \cdot 100 \lg \epsilon^{-1}$ probes. So, with high probability, if $p_i$ gets $50\%$ full, we have that 
\begin{align*}
    \frac{|p_i|}{2} \cdot 100 \lg \epsilon^{-1} = |p_{i-1}| \cdot 25 \lg \epsilon^{-1},
\end{align*}
which means that $p_{i-1}$ is $1 - \epsilon^{12.5}$ full. Then $p_{i-2}$ is also at least $1 - \eps^{12.5}$ full, and all $p_{i-1}, p_{i-2}, \ldots, p_1$ are all at least $1-\eps^{12.5}$ full. We claim that the final part is $\leq \frac 12$ full. This is because if the final part were $\geq \frac 12$ full, then all the earlier parts would be $\geq 1-\eps^{12.5}$ full, meaning that the overall number of elements is 
\begin{align*}
    &\geq  (1 - \eps/10) (|p_1| + \cdots + |p_{k}|) \\
    &= n \cdot \left( 1 - \frac \eps {50}\right) \\
    &> (1-\eps)n.
\end{align*}
But this yields a contradiction, meaning the final part never reaches $50\%$ full. As such, we have disproved Yao's claim. So,
\begin{itemize}
    \item Once we get to the final part, the expected additional time is $O(1)$. 
    \item So, the total expected time is 
    \begin{align*}
        O(1) + \text{number of probes before final part} \\
        = O(1) + 100 \lg \eps^{-1} \cdot O(\lg \eps^{-1}) = O(\lg^2 \eps^{-1}).
    \end{align*}
\end{itemize}
\newpage
\section{Linear Probing}
To insert an element $v$ into a hash table that incorporates linear probing, we just
\begin{itemize}
    \item place $v$ in the first free slot out of $h(v), h(v) + 1, \ldots$ (modulo the table size).
\end{itemize}
Searching for an element $v$ uses the same process. At relatively low capacity, linear probing is the most efficient hash table known to man.
But now suppose we perform $(1 - 1/x)\cdot n$ insertions and perform one more insertion.
\clm{Peterson 1957}{
    The expected insertion time is $O(x)$. 
}
\thm{Knuth 1962}{
    The expected insertion time is $\Theta(x^2)$.
}
\noindent Intuition for clustering:
\begin{itemize}
    \item The longer your run gets, the more likely you are to hash into the run. ``Winner keeps winning.''
    \item ``Globbing effect.'' This is when an insertion connects two clusters. 
\end{itemize}
\noindent Intuition \#2:
\begin{itemize}
    \item Consider an interval $I$ with size $x^2$. Let $R$ be the number of items whose hashes are in $I$. We consider properties of $R$. So first,
    \begin{align*}
        E[R] &= (1-1/x)\cdot x^2 = x^2 - x \\
        \mathrm{std} (R) &\approx \sqrt{E[R]} \approx x.
    \end{align*}
    Moral: intervals of size $\leq x$ often have more elements hashing to them than can fit. 
\end{itemize}
\mlenma{}{
    Throw $(1-1/x)\cdot n$ balls into $n$ bins. Let $k > 0$. Define $P_i$ to be the number of balls in the first $i$ bins. Then
    \begin{align*}
        P(\exists \text{ any $i \geq kx^2$ such that } P_i \geq i) \leq e^{-\Omega(k)}.
    \end{align*}
    Call the event above $*$.
}
\noindent Warmup: What is $P(P_{kx^2} \geq kx^2)$? (call this event $\dagger$)
\begin{align*}
    \mu = E[P_{kx^2}] = (1-1/x) \cdot kx^2  = kx^2 - kx.
\end{align*}
So $\sqrt \mu \leq \sqrt{kx^2} = \sqrt k x$.So,
\begin{align*}
    P(P_{kx^2} \geq kx^2) \approx P(P_{kx^2} \geq \mu + \sqrt k \sqrt \mu) = e^{-\Omega(k)}.
\end{align*}
Now we make a critical claim. If we condition on any prefix being problematic, then
\begin{align*}
    P(\dagger \mid *) = \Omega(1).
\end{align*}
This would mean that $P(*) = \Theta(P(\dagger))$, so by the warmup, we'd be done.
\begin{proof}
    Assume event $*$. Let $i$ be the largest $i$ such that $P_i \geq i$. Note that $kx^2 \leq i$ because we assumed $*$. If you tell me $I, P_i$, what is the conditional distribution for $P_{kx^2}$. Then,
    \begin{align*}
        P_{kx^2} \sim \text{binomial random variable given by sum of $P_i$ balls,} \\
        \text{ each of which has probability $kx^2/i$ of landing in the first $kx^2$ bins.}
    \end{align*}
    Therefore, $P_{kx}$ is now a binomial random variable with mean $P_i \cdot \frac{kx^2}{i} \geq kx^2$. Any binomial random variable with mean $\mu \geq 1$ has $\Omega(1)$ probability of meeting or exceeding its mean. So it follows that $P(\dagger \mid *) \geq \Omega(1)$.  This completes the proof of the lemma.
    \newpage
    \noindent Returning to Knuth's theorem, consider insertion of an element $u$ at $(1 - 1/x)$ full. Then we want to show that
    \begin{align*}
        P(\text{insertion time} \geq kx^2) \leq e^{-\Omega(k)}.
    \end{align*}
    Now consider a table where the range $[h(v) -r, h(v) + kx^2)$ is saturated. Then, there exists a suffix of array slots ending at slot $h(v) + kx^2 - 1$ such that 
    \begin{itemize}
        \item $|\text{suffix}| \geq kx^2$
        \item the number of elements that hash to suffix $\geq |\text{suffix}|$.
    \end{itemize}
    This is where we apply the lemma. This directly gives us that 
    \begin{align*}
        P(\text{insertion time} \geq kx^2) \leq e^{-\Omega(k)}
    \end{align*}
    which completes the proof.  
\end{proof}

\section{Ordered Linear Probing}

\thm{Knuth 1963}{Suppose you fill a linear-probing hash table to $(1 - 1/x)$ full. Then, the expected insertion time is $\Theta(x^2)$.}
\noindent Amble and Knuth had the following idea:
modify the hash table such that in each ``run'', maintain an invariant that the elements in the run are in sorted order by hash.

\noindent Aside: if elements have the same hash, imagine we break ties with another hash function. 
\nt{
    Also note that this is a history-independent data structure, meaning that the final state of the hash table is independent of the order of the elements added. 
}
\thm{Amble and Knuth 1973}{
    $E[\text{time to query an element}] = O(x)$. 
}
\begin{proof}
    Just to simplify things, we will assume positive queries, meaning we only query elements that exist in the hash table.  

    Let $S = \{\text{elements in the hash table}\}$. For $s \in S, Q_s$ is the query time for $s$. Also define $I_s$ as the insertion time for $s$ when it was inserted.

    \noindent \textbf{Observation 1}: $E[Q_s]$ is the same $\forall s\in S$.
    
    \noindent \textbf{Proof}: By symmetry and because the hash table is history-independent. $\hfill \qed$

    \noindent \textbf{Observation 2}: 
    \begin{align*}
        \sum_{s\in S} Q_s = \sum_{s \in S} I_s.
    \end{align*}
    \noindent \textbf{Proof}: Just think about it lowkey. Basically if the insertion time is $k$, then we increase the sum of the query times by $k$.  $\hfill \qed$

    \noindent \textbf{Observation 3}: 
    \begin{align*}
        E\left[ \sum_{s \in S} I_s \right] = \Theta(n \cdot x).
    \end{align*}
    \noindent \textbf{Proof}: Well we have 
    \begin{align*}
        E\left[ \sum_{s \in S} I_s \right] &= \Theta \left( \frac n2 \cdot 2^2 + \frac n4 \cdot 4^2 + \frac n8 \cdot 8^2 + \cdots + \frac{n}{x} \cdot x^2\right) \\
        &= \Theta\left( n\cdot x\right). \tag{dominated by last summand}
    \end{align*}
    This completes the proof of the observation. $\hfill \qed$

    \noindent Moving to the proof of the theorem, we have:
    \begin{align*}
        E[Q_s] &= \frac{1}{|S|} \cdot \sum_{s \in S} E[Q_s] \\
        &= \frac{1}{|S|} \cdot \sum_{s \in S} E[I_s] \\
        &= \frac{1}{\Theta(n)} \cdot \Theta(n \cdot x) \\
        &= \Theta(x).
    \end{align*}
\end{proof}
\subsection{Deletions}
Idea: use ``tombstones.'' That is, rather than entirely deleting the element, we just mark the index as ``deleted.'' What that means is that when we query that index, all we know is that there used to be an element at that spot. Also when trying to insert an element, the tombstone spot is just as good as any other open spot to insert the element. 

However, queries cannot treat tombstones are empty. This is because the element we are searching for could still be to the right of a tombstone. So queries skip over tombstones. 

Every so often, we should pause and rebuild the hash table, but with the tombstones removed. How often? The classical measure is every $R =  \frac{n}{2x}$ operations. 

\noindent \textbf{Question}: What if we have a table with insertions and deletions?

\noindent Experiment: initially fill our hash table to $1 - 1/x$ full. Then, we alternate between insertions and deletions. We get this graph:

\begin{center}
    \includegraphics*[width=15cm]{hashtableexperiment.jpg}
\end{center}
\newpage
\thm{Kuszmaul (2022)}{
    Consider any workload such that the hash table is never $> 1-1/x$ full. Then, the amortized expected time per operation is $\widetilde O(x)$. The actual optimal bound is $\Theta(x \lg^{1.5} x)$. 
}
\noindent \textbf{Big Idea 1}: Consider all the opeartions in a given rebuild window. Then we draw a random line in the hash table and ask how many insertions crossed the line. If we can answer this for every dotted line, we can sum to get the sum of the insertion times. 

\noindent \textbf{Big Idea 2}: Before rebuilding, consider the state of the hash table.
Draw an X in every empty slot before the random line. Then for every deletion that occurs in the rebuild window, we draw an X at the time of deletion and at the position of the deleted element. Similarly for insertions, but use $\checkmark$. Now we draw a path from the bottom left to the top right of the graph (nondecreasing). It turns out that 
\begin{align*}
    \text{the number of insertions that cross the line} = \max_{\text{all possible paths }p} (\text{number of $\checkmark$ under path } - \text{ number of X under path}).
\end{align*}
\noindent \textbf{Big Idea 3}: With enough work, we get a simpler problem which we call the \underline{path surplus problem}. That is, we have an $x \times x$ grid where each entry is randomly $\pm 1$. Consider every monotonic path to the right. For each path $p$, we have 
\begin{align*}
    \mathrm{surplus}(p) = \sum \text{ entries under $p$}.
\end{align*}
The question is what is the expected max surplus over all path? 
\thm{}{
    The expected maximum surplus is $\Theta(x \lg^{0.75} x)$. 
}
\section{Quadratic Probing}
To insert an element $v$ into a hash table that incorporates quadratic probing, we just
\begin{itemize}
    \item place $v$ into the first free slot out of $h(v), h(v) + 1, h(v) + 4, \ldots$ (modulo the table size).
\end{itemize}
\clm{Maurer 1968}{
    If you fill to $1 -1/x$ fraction full, then the expected time for the next insertion is $O(x)$. 
}
\nt{The above conjecture is an open problem, as are the following conjectures.}
\clm{}{
    At least $O(x^2)$?
}
\clm{}{
    $f(x)$ for some $f$ that doesn't depend on the hash table size $n$?
}
\clm{}{
    If the hash table is $\leq 1/2$ full, then expected insertion time is $O(1)$. 
}
\clm{ICALP '24 (Kuszmaul, Zoe Xi)}{
    If the hash table is $\leq 1/20$ full, then expected insertion time is $O(1)$.
}
The one above is actually solved.
\newpage
\noindent Suppose the hash table is filled to $0.0001$ full and consider the next insertion.
\thm{}{
    \begin{align*}
        P(\text{insertion time} \geq t) \leq e^{-\Omega(t)}.
    \end{align*}
}
\noindent Suppose the insertion time takes $\geq t$ time. Then construct a ``witness set'' $S \subset \{1, \ldots, n\}$ fo teh positions that collectively did somethign bad.
\dfn{}{
    Given a set $S$ of positions, $q \in S$, $u$ in the hash table, and $i \geq 1$, we say that ``$u$ enters $S$ at $q$ after its $i$ hashes'' if 
    \begin{itemize}
        \item $u$ is in some position in the $S$,
        \item $h(u)$, $h(u) + 1^2$, $\ldots, h(u) + (i-1)^2$ (first $i$ probes for $u$) are not in $S$,
        \item $h(u) + i^2 = q$.
    \end{itemize}
}
\noindent Constructing $S$: suppose we're inserting an element $v$ which takes time $\geq t$.
\begin{itemize}
    \item $S \leftarrow \{h(v), h(v) + 1, \ldots, h(v) + (t-1)^2\}$.
    \item (a queue) $Q = S$.
    
    while $Q$ is non empty:
    \begin{itemize}
        \item $q \leftarrow pop(Q)$ (pick the largest $q \in Q$).
        \item while there exists $u$ and $i \geq 1$ such that ``$u$ enters $S$ at $q$ after $i$ hashes'':
        \begin{itemize}
            \item pick such $u$ and $i$ arbitrarily and add $h(u), h(u) + 1, \ldots, h(u) + (i-1)^2$ to $S$ and $Q$.
        \end{itemize}
    \end{itemize}
\end{itemize}
\noindent Properties of $S$:
\begin{enumerate}
    \item Every position in $S$ is occupied by an element that hashes into $S$.
    \begin{proof}
        Consider the first time the probe sequence intersects with a value of $S$. What has to happen is that when we process it, $h(v)$ will get added to $S$.
    \end{proof}
    \item $|S| \geq t$. 
    \item For any given option for $S$ such that $|S| = k$, then
    \begin{align*}
        P(\text{that $S$ occuring}) \leq \frac{1}{18^k}.
    \end{align*}
    \begin{proof}
        Let $X$ be the number of elements in the hash table such that $h(v) \in S$. If $S$ occurs, that means $X \geq k$. $X$ is the sum of $0,1$ random variables, so
        \begin{align*}
            E[X] = 0.0001 \cdot k.
        \end{align*}
        By a Chernoff Bound,
        \begin{align*}
            P(X \geq k) \leq \frac{1}{18^k}.
        \end{align*}
    \end{proof}
\end{enumerate}
\mlenma{}{
    Number of options for $S$ of size $k$ is $\leq 9^k$.
}
To finish the proof of the theorem, we'd get 
\begin{align*}
    P(\text{insertion time} \geq t) &\leq \sum_{k \geq t} \sum_{\text{options of $S$ of size $k$}} P(\text{$S$ occurs}) \\
    &\leq \sum_{k \geq t} \sum_{\text{options of $S$ of size $k$}} \frac{1}{18^k} \\
    &\leq \sum_{k \geq t} \frac{9^k}{18^k} \\
    &\leq O\left( \frac{1}{2^t}\right).
\end{align*}
So now we go ahead and prove the lemma.
\begin{proof}
    Given the process of constructing $S$, we record a transcript $T$, a trinary string.
    \begin{itemize}
        \item $S \leftarrow \{h(v), h(v) + 1, \ldots, h(v) + (t-1)^2\}$.
        \item (a queue) $Q = S$.
        \item $T = $ ``$000\ldots 1''$ such that $|T| = t$. 
        
        while $Q$ is non empty:
        \begin{itemize}
            \item $q \leftarrow pop(Q)$ (pick the largest $q \in Q$).
            \item while there exists $u$ and $i \geq 1$ such that ``$u$ enters $S$ at $q$ after $i$ hashes'':
            \begin{itemize}
                \item pick such $u$ and $i$ arbitrarily and add $h(u), h(u) + 1, \ldots, h(u) + (i-1)^2$ to $S$ and $Q$.
                \item $T \leftarrow  T + \underbrace{000\ldots 1}_{i}$.
            \end{itemize}
            \item $T \leftarrow T + 2$.
        \end{itemize}
    \end{itemize}
    If I knew $T$ an $h(v)$, I can recover the entire construction of $S$, specifically the final value of $S$. Also, $|T| \leq \# 0s + \# 1s + \# 2s \leq |S| + |S| = 2k$. 
    Since $T$ fully determines $S$, this means the number of options of $S$ is bounded by the number of options for $S$. This is less than $(3)^{2k} = 9^k$. 
\end{proof}

\section{Cuckoo Hashing}
Invented in 2001 by Pagh and Rodler. Note that for a hashtable with $n$ slots, 
\begin{align*}
    P(\text{no collision}) = \prod_{i=1}^n \left( 1 - \frac{i-1}{n}\right) \approx \prod_{i=1}^n e^{-(i-1)/n} \approx e^{-m^2/(2n)}.
\end{align*}
What Cuckoo hashing does is that it creates a second hash table, also with $n$ slots, and has two different hash functions $h_1$ and $h_2$. So if a slot is occupied in the first table, we kick that element out and put it in its proper spot in the second table. We now have two questions:
\begin{enumerate}
    \item Does there exist an assignment of elements $x \mapsto h_1(x)$ or $h_2(x)$ without collisions.
    \item Do we have efficient insertion?
\end{enumerate}
\newpage
\thm{Erdos-Renyi}{
    If the hash table has $\leq (1-\Omega(1))\cdot n$ elements, with probability $1- O(1/n)$, there exists an assignment.
}
\begin{proof}
    If there does not exist such an assignment, by Hall's Theorem, there exists a set $S \subseteq \text{elements}$ such that $T = \bigcup_{s \in S} \{h_1(s) \cup h_2(s)\}$ such that $|T| < |S|$. Take a minimal such $S$. That is $|T| < |S|$ but $\forall R \subseteq S$, $|N(R)| \geq |R|$.

    We can create a graph of the hash table where the slots are the vertices and the edges are drawn between $h_1(x)$ and $h_2(x)$ for all $x$. So in this case, our graph has $|S|$ edges and $<|S|$ vertices ($|S| - 1$ actually) that has no degree 1 vertices. There are only so many structures this graph can take on. There are $\leq (|S| -1)! (|S|-1)^2$ such graphs.

    Let $m$ be the total number of elements where $m \leq (1-1/x)n$.

    So,
    \begin{align*}
        P(\exists \text{ a bicyclic subgraph}) &\leq E[\text{bicyclic subgraphs}] \\
        &\leq \sum_{s=1}^m \binom{2n}{s-1}(s-1)!(s-1)^2 m^s (2n)^{-2s}2^s \\
        &\leq \sum_{s=1}^m  (2n)^{s-1} (s-1)^2 m^s (2n)^{-2s} 2^s \\
        &\leq \sum_{s=1}^m  (2n)^{-1} (s-1)^2 \left(\frac mn\right)^s \\
        &\leq (2n)^{-1} \sum_{s=1}^m  (s-1)^2(1- 1/x)^s\\
        &\leq O\left(\frac 1n\right).
    \end{align*}
\end{proof}
\noindent This could be stated as $G(2n,m)$ has no bicyclic components for $m = cn$, $c < 1$. 
\thm{Pagh-Rodler 2001}{
    \begin{align*}
        E[\text{insertion time}] = O(x).
    \end{align*}
}
\begin{proof}
    What we're really going to prove is that $P(\text{insertion time} \geq t) = e^{-\Omega(xt)}$. Assume the insertion time $\geq t$. Then there are $t$ keys, $x_1, \ldots, x_t$ such that 
    \begin{align*}
        h_\circ(x_1) &= h_{i_2}(x_2) \\
        h_{3 - i_2}(x_2) &= h_{i_3}(x_3) \\
        &\vdots
    \end{align*}
    So,
    \begin{align*}
        m^{t-1}(2n)^{-(t-1)} 2^t \leq \left(\frac mn\right)^{t-1} \leq \left(1 - \frac 1x\right)^t.
    \end{align*}
\end{proof}
\newpage
\thm{}{
    If $m > (1 + 1/x)n$, with probability $1 - o(1)$, there does not exist an assignment.
}
\subsection{$d$-ary Cuckoo hashing}
Imagine we have three hashes. Query and deletion will take $\leq 3$ slots. But how does this affect our load factor and insertion time?

\thm{FPSS 2005}{
    $d$ hashes $\implies$ can't get $\geq 1 - \frac{1}{e^d}$ full. 
}
\thm{2009}{
    For $d$ hashes, there exists $c_d$ such that load factor $> c_d$ implies there does not exist an assignment. And load factor $< c_d$ implies that there does.
}
\noindent We already showed that $c_2 = 0.5$. Turns out $c_3 = 0.918$, $c_4 = 0.977$. As $d\to\infty$, $c_d = 1 - e^{-d} - o(e^{-d})$. So if we are $1- 1/x$ full, we need $\ln(x)$ hashes. So query and deletion are both $O(\ln(x))$ in this case.

\thm{FPS 2013}{
    The expected insertion time is $O(\mathrm{polylog} n)$ up to $c_d$. 
}
\thm{Walzer 2022}{
    For 3 hashes, load factor $\leq 0.818$, the expected insertion is $O(1)$.
}
\thm{Tolson, Frieze}{
    For at least 4 hashes, the expected insertion is $O_x(1)$ up to $c_d$. 
}
\mlenma{}{
    With probability $1 - o(1)$,
    $\forall T\subseteq\mathrm{slots}$, the number of hashes to $T$ is $\leq O(d \log(n/|T|)) |T|$.
}

\mlenma{}{
    With probability $1 - o(1)$,
    $\forall S \subseteq \mathrm{keys}$, the number of slots hashed to $S$ is
    \begin{align*}
        \geq \left(d-1- \frac{\log(e^d(d-1))}{\log(dn/|S|)}\right) |S|
    \end{align*}
}
\newpage
\section{
    woo hash table theory idk
}
\thm{}{
    Suppose we want to store up to $n$ elements where each element is $w$ bits. There exists a hash table such that 
    \begin{enumerate}
        \item space: $nw \cdot (1 + o(1))$ bits.
        \item expected time per operation $O(1)$.
    \end{enumerate}
}
\noindent \textbf{Warmup 1}: Chained hash table. Recall that this is where each bin $j$ stores a doubly linked list of elements $v$ such that $h(v) = j$. 

\noindent \textbf{Observation 1}: Expected time per operation is $O(1)$. However, the psace is $nw + O(n \log n)$ where the latter summand comes from pointers. 

Let's talk about what a hash table looks like in memory. We have a list of $n$ pointers  and an array of size $n$. Each internal node takes $O(\log n)$-bit pointers to a $w$-bit item. 

When doing operations, we want to maintain an invariant that the in-use slots in the intermediate array are a prefix of the array.

\noindent \textbf{Warmup 2}: Balls and bins! Suppose we throw $n$ balls into $\frac nb$ bins. Suppose $b = (\log n)^4$.

\clm{}{
    With high probability, every bin has $\leq b \cdot(1 + 1/\log n)$ balls.
}
\begin{proof}
    By union bound, we can focus on bin 0. So let $X =$ the number of balls in the bin. So 
    \begin{align*}
        X = \sum_{i}^n \mathbb {I}_i
    \end{align*}
    where $\mathbb I_i$ is an indicator of whether ball $i$ went into the bin or not. So, $E[X] = b$ and we can apply a Chernoff bound. So 
    \begin{align*}
        P(X > b(1 + 1/log n)) = P(X > b + \log n \cdot \sqrt b)\leq e^{-\Omega(\log^2 n)}.
    \end{align*}
    This completes the proof.
\end{proof}
Now we move to the proof of the theorem.

\noindent \textit{Proof:} Again let $b = \log^4 n$. Hash the elements into $\frac nb$ bins, each  with capacity $(1 + 1/\log n)\cdot b$. In each bin, use a chained hash table. Each chained hash table uses $kw + O(k \log k)$ bits where $k$ is the size of the hash table. So for each bin, $k = \Theta(b)$, meaning the space per pointer is order $\log b = \log(\log^4 n) = 4\log \log n$ bits per pointer.

So for the total space, we have 
\begin{align*}
    O(n \log \log n) + (1 + 1/\log n) \cdot nw
\end{align*}
bits. As $w > \log n$, we have 
\begin{align*}
    O(n \log \log n) + (1 + 1/\log n) \cdot nw \leq \left(1 + O\left(\frac{\log \log n}{\log n}\right)\right) \cdot nw.
\end{align*}
\noindent How much space does a hash table really need? In total, we have 
\begin{align*}
    \log \binom{2^w}{n} &\approx \log\left( \frac{2^w (2^w - 1)\cdots(2^w - n + 1)}{n!} \right) \\
    &\approx \log\left(\frac{2^{wn}}{n^n/e^n}\right) \\
    &= wn - n \log n + O(n).
\end{align*}
Usually $w = \Theta(\log n)$, and we have room to save $n \log n$ bits, which in the usual case is a constant fraction of the total space we have.

\thm{}{
    We can improve our space to 
    \begin{align*}
        wn \cdot(1 + o(1)) - n \log n + O(n\log\log n)
    \end{align*}
    bits.
}
\noindent \textbf{Quotienting} (Knuth).

Let's say we have $m = \frac nb$ bins. Imagine that each element was split into the first $\log m$ bits and $w - \log m$ bits. Suppose the first $\log m$ bits are random. So we're going to use the first $\log m$ bits to decide which bin it goes in. 

By doing this, we don't actually need to store the first $\log m$ bits of each element because it is given by the bin number.

Now our idea is that instead of storing $u$, we store $\pi(u)$ where $\pi$ is a random permutation from $[2^w] \to [2^w]$. This is called a permutation hash and is invertible. 

The goal is to use random hash functions to build a pmerutation hash function that is ``as good'' as random.

\noindent \textbf{Single-round Feistel Permutation}:

\noindent Think of an element as $u = (x,y )$ where $x$ is the first $\log m$ bits. Then let $h(y) \to $ the first $\log m$ bit number. Then 
\begin{align*}
    \pi(u) = (h(y) \oplus x, y).
\end{align*}
\clm{}{
    $\pi$ is invertible.
}
\begin{proof}
    We can see that 
    \begin{align*}
        \pi(\pi(u)) &= \pi(h(y) \oplus x, y) \\
        &= (h(y) \oplus h(y) \oplus x, y) \\
        &= (x, y) \\
        &=u.
    \end{align*}
\end{proof}
Now let $u_1, \ldots u_n$ be the elements and $u_i = (x_i, y_i)$. Let $X$ be the number of elements in bin 0. An element $u_i $ gets put into bin 0 if and only if 
\begin{align*}
    h(y_i) \oplus x_i = 0.
\end{align*}
\textbf{Observation 1}: $E[X] = \frac nm = b$.
\begin{proof}
    Each balls has probability $\frac 1m$ of being in bin 0.
\end{proof}
\clm{}{
    $X$ is still the sum of independent indicator random variables.
}
\begin{proof}
    Let $G_y = \{u_i \mid y_i = y\}$. Let $X_y$ be the number of elements in $G_y$ that go to bin 0. We see that $X = \sum_{y}X_y$. 

    Also $X_y$ is independent across all $y$ because $X_y$ is determined by $h(y)$ which is independent across all $y$. 

    The last thing to show is that each $X_y$ is in $\{0, 1\}$. If $X_y \geq 2$, then there are distinct $u_1 = (x_1, y), u_2 = (x_2, y)$ such that 
    \begin{align*}
        h(y) \oplus x_1 = h(y) \oplus x_2 = 0,
    \end{align*}
    which would imply $x_1 = x_2 \implies u_1 = u_2$. This completes the proof. 
\end{proof}
This completes the proof of the theorem.

\thm{(STOC 2022)}{
    $\log(\zeta) + O(n \underbrace{\log \log \log \cdots \log n}_{k \log s})$, has time $k$ per operation.
}
\thm{(FOCS 2023)}{
    This is the optimal space/time tradeoff curve across all data structures.
}
\chapter{List Labeling Problem}
\noindent Bill likes to call this the ``dynamic sorting'' problem.
\qs{The list labeling problem}{
    We have an array of size $m = 2n$. 
    \begin{itemize}
        \item Elements are going to be inserted and deleted over time.
        \item $\leq n$ elements at once.
        \item Our job is to keep the elements in sorted order in the array.
    \end{itemize}
    The goal is to minimize the ``cost'' per operation: 
    \begin{itemize}
        \item Really just the number of items whose array position changes.
    \end{itemize}
}
\thm{Itaei, Konheim, Rodeh 1981}{
    There exists a solution with amortized cost $O(\log^2 n)$ per operation.
}
\noindent The algorithm: we're going to think of the list as a binary tree. That is, the root will be the array of size $m$, the children will be the array split into subarrays of size $m/2$, and the $i$-th level contains $m/2^i$ elements in each node. The leaves are going to be subarrrays with $\leq 1000 \lg n$ elements.

\noindent \textbf{The invariant}: consider a subproblem $A$ with children $B$ and $C$. Define the density of a problem to be 
\begin{align*}
    \mathrm{density}(A) = \frac{\text{number of elements in $A$}}{|A|}
\end{align*}
The invariant is going to be that 
\begin{align*}
    \mathrm{density}(B), \mathrm{density}(A) \leq \left(1 + \frac{1}{10 \lg m}\right)\mathrm{density}(A).
\end{align*}
If the invariant ever breaks, we take the elements in $A$ and spread them out evenly across $A$. For an insertion:
\begin{enumerate}
    \item Fix any invariant that got broken.
    \item Incur up to $O(\lg n)$ cost at leaf.
\end{enumerate}
\newpage
\mlenma{}{Every subproblem has density $\leq 1$.}
\begin{proof}
    We have that 
    \begin{align*}
        \mathrm{density}(\mathrm{subproblem}) &\leq \mathrm{density}(\mathrm{root}) \cdot \left(1 + \frac{1}{10 \lg m}\right)^{\lg m} \\
        &\leq \frac 12 \cdot e^{1/10} \leq 0.6.
    \end{align*}
\end{proof}
\mlenma{}{
    Amortized cost per operation is $O(\lg^2 n)$.
}
\begin{proof}
    Every insertion/deletion pays $\lg n$ dollars to each subproblem it encounters. So this is $\lg^2n$ total dollars paid. For a subproblem $A$, consider time between 2 rebuilds.
    \begin{itemize}
        \item After previous rebuild, let $t =$ the number of elements in $A$.
        \item Between then and the next rebuild, at least $\Omega\left(\frac{t}{\lg n}\right)$ insertions/deletions must go through $A$. This implies that $A$ collects $\Omega\left(\frac{t}{\lg n}\right)\cdot \lg n = \Omega(t)$ dollars. Actually, if $A$'s new size of $t'$, then $A$ has $\Omega(t')$ dollars.
    \end{itemize}
    So rebuilding costs $t'$ dollars and takes time $t'$. So we have 
    \begin{align*}
        \text{total cost} &\leq \text{total dollars spent} \leq O(\lg^2 n) \cdot \text{number of operations}.
    \end{align*}
\end{proof}
\noindent Other work on this problem:
\begin{itemize}
    \item 1990: any ``smooth'' algorithm must incur $\Omega(\lg^2 n)$ per operation. In this context, ``smooth'' means that whenever the algorithm rebuilds a subarray, it spreads the elements evenly in the subarray.
    \item 2012: any deterministic algorithm must incur $\Omega(\lg^2 n)$ per operation.
    \item 2022: there exists a randomized algorithm with expected cost $O(\lg^{1.5} n)$ per operation.
    \item 2024: there exiss a randomized algorithm with expected amortized cost $O(\lg n(\lg \lg n)^3)$.
\end{itemize}
\thm{2015}{
    There is a history independent solution that gets $O(\lg^2 n)$ per operation.
}
\noindent Strong history independence: This says that the state of the data structure at any given moment is fully determined by 
\begin{enumerate}
    \item The current set of elements and
    \item some hash functions.
\end{enumerate}
\begin{proof}
    The algorithm: we are given a set $S$ of elements and a hash function $h:S \to [0,1]$. In the middle of our array, we are going to designate a pivot, meaning the left side has size $(m-1)/2$ as well as the right side.

    How to pick a pivot: consider the elements in sorted order:
    \begin{align*}
        s_1, s_2, \ldots s_k.
    \end{align*}
    Then the candidates will be the middle $\frac{k}{10 \lg n}$ elements. We pick the pivot of as the candidate who has the min hash value.

    \noindent \textbf{Correctness}: we observe that if we have a problem $A$ with subproblems $B$ and $C$, then 
    \begin{align*}
        \mathrm{density}(B), \mathrm{density}(C) \leq \left(1 + \frac{1}{10 \lg m}\right)\mathrm{density}(A)
    \end{align*}
    because of the choice of our pivot. This yields correctness.

    \noindent \textbf{Cost bound}: consider an insertion, which has to take a path down the tree from $A_1, \ldots A_k$. The cost of the insertion can be bounded by 
    \begin{align*}
        \mathrm{cost} \leq \sum_i \sum_t P(\text{$A_i$ has size $t$}) \cdot E[\text{cost on $A_i$} \mid \text{size $t$}].
    \end{align*}
    \mlenma{}{
        If an insertion goes through a subproblem $A$ and if $A$ has $t$ elements, then 
        \begin{align*}
            P(\text{pivot changes for A}) = O\left(\frac{\lg n}{t}\right).
        \end{align*}
    }
    \noindent The point is that 
    \begin{align*}
        E[\text{cost from $A_i$} \mid \text{size is $t$}] = O\left(\frac{\lg n}{t}\right) \cdot t = O(\lg n).
    \end{align*}
    \noindent \textit{Proof of lemma}: Let $C_1$ be the pivot candidtions before the insertion, and $C_2$ be the pivot conditions after the insertion. We observe that $C_1$ and $C_2$ by $O(1)$ elements. Let $\Delta$ be the elements where they differ. The only way the pivot can change is if the element with the smallest hash happens to be in $\Delta$. The probability that this happens is 
    \begin{align*}
        \frac{|\Delta|}{|C_1 \cup C_2|} = \frac{\Theta(1)}{\Theta\left(\frac{t}{\lg n}\right)} = O\left(\frac{\lg n}{t}\right).
    \end{align*}
\end{proof}

\thm{Bender, Conway, Kuszmaul, etc. (2022)}{
    There exists a history independent solution with expected cost $\widetilde O(\lg^{1.5} n)$ per operation.
}
\thm{}{
    There exists a history independent solution such that if an insertion is to a random rank, then the expected cost is $\widetilde O(\lg^{1.5} n)$ per operation.
}
\noindent We construct the algorithm with the same recursive structure with a pivot in the middle. So last time for picking the pivot, if $s_1, \ldots, s_t$ are the elements being stored, then the set of pivot ``candidates'' as the middle $\frac{t}{\Theta (\lg n)}$ elements. This time, we'll instead consider the middle 
\begin{align*}
    \frac{t}{100\sqrt{\lg n \lg\lg n}}
\end{align*}
elements. As before, the pivot will be the candidate with the minimum hash value. So now,
\begin{align*}
    P(\text{insertion into subproblem changes the pivot}) = O\left(\frac{1}{|\text{candidates}|}\right),
\end{align*}
which is now roughly decreased by $\approx \sqrt{\lg n}$. This would lead to a $\widetilde O(\lg^{1.5} n)$ bound. This is great!

\noindent However the bad news is that this is not a correct algorithm! This is because the density could get as high as 
\begin{align*}
    \left( 1 + \widetilde O\left(\frac{1}{\sqrt{\lg n}}\right)\right)^{\lg n} \gg 1.
\end{align*}
The fix is that if the subproblem's density ever reaches $\geq 3/4$, then swap back to the $O(\lg^2 n)$ algorithm for this subproblem and it's descendants. We will call these subproblems ``corrupted.''

\mlenma{Main Lemma}{Consider insertions into random rank. With probability $1 - \frac{1}{\lg^{10} n}$, the insertion does not go through any corrupted subproblems.}
\noindent How much does insertion cost:
\begin{itemize}
    \item If we void corrupted subproblems, then $\widetilde O(\lg^{1.5}n)$ expected cost.
    \item If we hit a corrupted subproblem, then $\widetilde O(\lg^2 n)$ expected cost, but only occurs with probability $\frac{1}{\lg^{10} n}$.
    \item Also, might have to rebuild subproblem if the insertion makes it corrupt. 
\end{itemize}
\mlenma{}{
    Expected cost from rebuilding is $\widetilde O(\lg^{1.5}n)$. 
}
\noindent We move to the proof of the main lemma. So let $d_1, d_2, \ldots$ be the densities of the subproblems that we encounter. Here's the intuition we have. Consider any subproblem with density $d_i$. Then it's children have densities of roughly
\begin{align*}
    (1 - 1/\sqrt{\lg n}) d_i \quad \text{ and } \quad (1 + 1/\sqrt{\lg n}) d_i.
\end{align*}
So if we condition on an insertion reaching the subproblem with density $d_i$, then it will go to either of the two children with roughly 50\% chance. So the $d_i$'s are really doing a random walk where each step is $\pm \frac{1}{\widetilde \Theta(\sqrt{\lg n})}$ where there are $\lg n$ steps. 

Note that if you have a random walk with step size $s$ and goes for $\ell$ steps, then at the end we expect the random walk to be within roughly $s \sqrt{\ell}$ from its starting point. So we really have 
\begin{align*}
    \sqrt{\lg n} \cdot \frac{1}{100\sqrt{\lg n \lg \lg n}} = \frac{1}{100\sqrt{\lg \lg n}}.
\end{align*}
Morally this means that the probability of hitting $3/4$ density is very very low. So now for an actual proof:
\begin{proof}
    Define $\Delta_i = d_{i+1} - d_i$. Then 
    \begin{align*}
        |\Delta_i| \leq \frac{1}{100 \sqrt{\lg n \lg \lg n}}.
    \end{align*}
    Then we have 
    \begin{align*}
        E[\Delta_i \mid \Delta_1, \ldots, \Delta_{i-1}] =\, ?
    \end{align*}
    So the probaiblity that an insertion goes left would be 
    \begin{align*}
        \frac{1-\Delta}{2}
    \end{align*}
    whereas the probabilty of going right would be
    \begin{align*}
        \frac{1+\Delta}{2}.
    \end{align*}
    So the change in density, $\Delta_i$, would be 
    \begin{align*}
        \Delta_i = -\Delta d_i
    \end{align*}
    in the left case and 
    \begin{align*}
        \Delta_i = \Delta d_i
    \end{align*}
    in the right case. So,
    \begin{align*}
        E[\Delta_i] &= \left(\frac{1-\Delta}{2}\right) \cdot (-\Delta d_i) + \left(\frac{1+\Delta}{2}\right) \cdot (\Delta d_i) \\
        &= \Delta^2 d_i \\
        &\leq \Delta^2 \\
        &\leq \left(\frac{1}{100 \sqrt{\lg n \lg \lg n}}\right)^2 \\
        &= \frac{1}{10000\lg n \lg \lg n}.
    \end{align*}
    So 
    \begin{align*}
        E[\Delta_i \mid \Delta_1, \ldots, \Delta_{i-1}] \leq \frac{1}{10000\lg n \lg \lg n}.
    \end{align*}
    So, we have that 
    \begin{align*}
        P(\text{get to a corrupt subproblem}) &\leq P(\text{$\exists j \in [\lg n]$ such that $\Delta_1 + \cdots + \Delta_j \geq 1/4$}).
    \end{align*}
    We'll focus on $j = \lg n$ and then union bound. So we analyze
    \begin{align*}
        P(\sum \Delta_i \geq 1/4).
    \end{align*}
    Now define $\Delta'_i \coloneq \Delta_i - \frac{1}{10000\lg n}$. So,
    \begin{align*}
        \sum \Delta_i > \frac 14 \implies \sum \Delta_i' > \frac 18.
    \end{align*}
    So then define $\Delta''_i \coloneq \Delta_i' \cdot 100\sqrt{\lg n \lg \lg n}$. Then,
    \begin{align*}
        \sum \Delta_i'' > \frac{100 \sqrt{\lg n\lg \lg n}}{8}.
    \end{align*}
    This is exactly the type of random variable we can apply an adaptive Chernoff bound to as $\Delta_i'' \leq 1$ and $E[\Delta_i'' \mid \text{ the others}] \leq 0$ and there are $r = \lg m$ terms. So,
    \begin{align*}
        P(\sum \Delta_i'' \geq k\sqrt r) \leq e^{-k^2/3}
    \end{align*}
    In our case, $r = \lg n$ and $k \geq 10 \sqrt{\lg n \lg \lg n}$. This gives us that 
    \begin{align*}
        P(\sum \Delta_i'' \geq 1/4) \leq \frac{1}{\lg^{30}n}.
    \end{align*}
\end{proof}
\newpage
\chapter{The Cup Game}
We describe the game:
\begin{itemize}
    \item Start with $n$ cups.
    \item They are all empty initially.
\end{itemize}
\noindent There are two players. In each step of the game, 
\begin{itemize}
    \item The filler distributes up to $1-\eps$ units of water into the cups.
    \item The emptier picks a cup and remove up to $1$ unit of water from the cup.
\end{itemize}
\noindent The emptier's goal is the minize the backlog, which is the fill of the fullest cup. Note that $\eps \in [0, 1/2]$. 
\thm{Dietz, Sleator 1987}{
    Consider the ``greedy'' emptying algorithm, meaning we just empty from the fullest cup. This algorithm achieves backlog $O(\lg n)$.
}
\nt{This result even holds for $\eps = 0$.}
\thm{}{
    If the emptier is deterministic, then filler can force backlog $\Omega(\lg n)$.
}
\nt{This result even holds for $\eps = \frac 12$. }
\begin{proof}
    The filler is gonna put $\frac{1-\eps}{n}$ water in all cups. Then the emptier empties from some cup. So the filler puts $\frac{1-\eps}{n-1}$ in each remaining cup, and we keep doing this. At the end of the construction, the backlog is $(1-\eps)\cdot H_n \leq \frac 12 \ln n$.
\end{proof}
\thm{Bender, Farach-Colton, Kuszmaul 2019}{
    There exists a randomized emptying algorithm that gets backlog $O(\lg \lg n)$ with high probability if $\eps = \Theta(1)$. 
}
\noindent After this theorem, they also proved it for $\eps = \frac{1}{\poly(n)}$.

\noindent \textbf{Smoothed Greedy Algorithm}: At the start, put $r_i \in [0,1]$ water into cup $i$ for each $i \in [n]$ where each $r_i$ is i.i.d. and uniform. Then on each step, empty from the fullest cup. BUT, if the fullest cup has less than 1 unit of water, we skip our turn.

This last caveat is extremely important because we want to preserve the randomness we put in each cup. From now on:
\begin{align*}
    a_i(t) &= \text{amount of water filler adds to cup $i$ on step $t$} \\
    b_i(t) &= \text{amount of water emptier removes from cup $i$ on step $t$} \\
    c_i(t) &= \text{amount of water in cup $i$ at the end of step $t$}.
\end{align*}
\clm{}{
    We claim that $(c_i(t) \mod 1)$ is uniformly random on $[0,1]$.
}
\begin{proof}
    \begin{align*}
        c_i(t) = r_i + \sum_{j=1}^t a_i(t) - \sum_{j=1}^t b_i(t).
    \end{align*}
    Note that $r_i$ is random on $[0,1]$, the second summand is a fixed number chosen by the oblivious adversary, and the third value is an integer.
\end{proof}
\noindent Now we create a potential function:
\begin{align*}
    \phi(t) = \sum_{t=1}^n \lfloor c_i(t) \rfloor.
\end{align*}
\mlenma{}{
    Consider time $t_1$. Let $t_0 \leq t_1$ be most recent time when $\phi(t_0)$ was 0. With high probability, $t_1 - t_0 \leq O(\lg n)$.
}
\noindent During steps $t_0+1, \ldots, t_1$, 
\begin{itemize}
    \item The emptier decreases $\phi$ by 1 per step. So the total decrease by the emptier is $t_1 - t_0$.
\end{itemize}
But how much does the filler increase $\phi$? We consider 
\begin{align*}
    P(\text{filler increases $\lfloor c_i\rfloor$}) = a_i(t).
\end{align*}
This means that the total increase to $\phi$ is a sum of independent Bernoulli random variables with total mean $\sum_{i=1}^n a_i(t) \leq 1 -\eps$. Now we move to analyzing a sequence of $k$ steps. Now we draw a picture:
\begin{itemize}
    \item Draw water in cups initially.
    \item And, draw water added by fillter during the $k$ steps.
\end{itemize}
Let $x_1, \ldots, x_n$ be the amount of water added into cup $i$ during the $k$ steps. Note that cup $i$ crosses:
\begin{itemize}
    \item $\lfloor x_i\rfloor$ thresholds for sure.
    \item One more threshold with probability $\{x_i\}$.
\end{itemize}
This is a sum of $\lfloor x_i\rfloor + 1$ independent Bernoulli random variables with total mean $x_i$. Note that these sums are also independent across cups. Specifically, the total increase to $\phi$ by the filler during the $k$ steps is the sum of Bernoulli random variables with total mean $\sum x_i \leq k(1-\eps)$. 
\newpage
\cor{}{
    For any $k$ steps, 
    \begin{align*}
        P(\text{filler increases $\phi$ by at least $k$ during steps}) &\leq \\
        \leq P(\text{sum of indep}&\text{endent Bernoulli} \text{ random variables with total mean $k(1-\eps)$ is at least $k$}) \\
        &\leq e^{-\Theta(k)}.
    \end{align*}
}
\noindent If $t_1 - t_0 = k$, then the emptier decreases $\phi$ by $k$, but the filler must increase $\phi$ by $k$. But then the probability of this is $e^{-\Omega(k)}$. It follows that for all $k$, 
\begin{align*}
    P(t_1 - t_0 = k) \leq e^{-\Theta(k)}
\end{align*}
So, $[t_1 - t_0]$ is a geometric random variable, so with high probability, we have that 
\begin{align*}
    [t_1 - t_0] \leq O(\lg n).
\end{align*}
Moving on to the proof of the main theorem, consider the amount for which each cup exceeds 2. Consider the heights above 2 during time $t_0, \ldots, t_1 \leftarrow$ now. The number of cups that get to height 2 is at most $t_1 - t_0 \leq O(\lg n)$. This means that these cups are playing a cup game with $n' = O(\lg n)$ cups. From the Dietz/Sleator result, the backlog is $\leq 2 + O(\lg n') = O(\lg \lg n)$ with high probability. $\hfill \qed$
\chapter{Random Hash Functions}
\dfn{}{
    A hash function $h: U_1 \to U_2$ is pairwise independent if for all $x \neq y \in U_1$, $h(x)$ and $h(y)$ are independent random variables. 
}
\noindent Constider a prime $p$. Construct $h: \ZZ_p \to \ZZ_p$. Let $a,b \in \ZZ_p$ be uniformly random. Define $h(x) = ax + b \pmod p$. 
\thm{}{
    This definition of $h$ is pairwise independent and uniformly random.
}
\noindent Let $a_1, \ldots, a_k$ be random in $\ZZ_p$. Then 
\begin{align*}
    h(x) \coloneq a_1 + a_2 x + a_3x^2 + \cdots + a_kx^{k-1} \pmod p
\end{align*}
is $k$-wise independent. However, it takes $\Theta(k)$ time to evaluate. 
\ex{Chained Hashing}{
    $k=2$ works.
    \begin{align*}
        E[\text{time to query $u$}] &= \sum_{v \neq u \text{ in hash table}} P[h(v) = h(u)] \\
        &= (n-1)\cdot \frac 1n \\
        &= O(1).
    \end{align*}
}
\ex{Linear Probing}{
    \begin{itemize}
        \item 5-way independence works.
        \item 4-way independence does not work.
    \end{itemize}
}
\ex{Cuckhoo Hashing}{
    \begin{itemize}
        \item 5-wise doesn't work.
        \item $O(\lg n)$-wise works.
    \end{itemize}
}
\ex{Balls and bins}{
    $n$ balls into $n$ bins. What is the expected maximum load?
    \begin{itemize}
        \item $k=2$ is not enough.
        \item $k=O(1)$ is also not enough.
    \end{itemize}
    It is an open problem to whether $[ax + b \pmod p] \pmod n$ achieves expected load $O(\log n / \log \log n)$. 
}
\thm{}{
    There exists a hash function $h: U \to U$, where $U = \{1, \ldots, \poly(n)\}$, such that 
    \begin{itemize}
        \item The evaluation time is $O(1)$.
        \item The space is $O(n^{0.9})$ machine works.
        \item For any set $S \subseteq U$ such that $|S| = n^{0.8}$, with high probabiilty in $n$, $h$ is random on $S$.
    \end{itemize}
}
\begin{proof}
    Let $c$ be a large constant. Let $A_1, A_2, \ldots, A_c$ be arrays of size $n^{0.9}$, each filled with random numbers from $U$. Let $g_1, \ldots, g_c$ be pairwise independent hash functions from $U \to \{1, \ldots, n^{0.9}\}$. Define
    \begin{align*}
        h(x) = \bigoplus_{i=1}^c A_i[g_i(x)].
    \end{align*}
    Call an element $x \in S$ ``happy'' if there exists $A_i$ such that $g_i(x) \neq g_i(y)$ for all $y \in S \setminus \{x\}$. We observe that if all $x \in S$ are happy, then $h$ is fully random on $S$. 

    So now we observe that with high probability, all $x \in S$ is happy. So fix an element $x \in S$ and see that 
    \begin{align*}
        P(x \text{ is not happy}) &\leq \prod_{i=1}^c P(g_i(x) \in \{g_i(y) \mid y \in S \setminus \{x\}\}) \\
        &\leq \prod_{i=1}^c\sum_{ y\in S \setminus \{x\}} P(g_i(x) = g_i(y)) \\
        &= \prod_{i=1}^c \sum_{y \in S \setminus \{x\}} \frac{1}{n^{0.9}} \\
        &\leq \prod_{i=1}^c \frac{n^{0.8}}{n^{0.9}} \\
        &= \left(\frac{1}{n^{0.1}}\right)^c \\
        &= \frac{1}{n^{c/10}}.
    \end{align*}
    This is as desired.
\end{proof}
\ex{Balls and Bins}{
    Dietzfelbinger and Rink's Splitting Trick:
    \begin{enumerate}
        \item Use a hash function $h_1$ to map balls to $n^{0.2}$ collections $C_1, C_2, \ldots, C_{n^{0.2}}$ such that each $C_i$ has size $(1 \pm o(1))\cdot n^{0.8}$.
        \item Use a hash function $h_2$ to map each collection to $n^{0.8}$ bins. 
    \end{enumerate}
    So in total we see that there are $n^{0.2} n^{0.8} = n$ bins. Now we use the hash function from the previous theorem to conclude that with high probability, $h_2$ is fully random on $C_1$ and on $C_2$ and so on. So, the max load in each of these collections is $O(\lg n / \lg \lg n)$ with high probability, meaning overall the max load is $O(\lg n / \lg \lg n)$. 
}
\nt{
    For step 1, we can also use the hash function from the previous theorem. For analysis, break into sets $S_1, \ldots S_{n^{0.2}}$ of size $n^{0.8}$. So with high probability, $h$ is random on each $S_i$. So we can use a Chernoff bound to argue that with high probability,
    \begin{align*}
        |S_i \cap C_j| = (1 \pm o(1)) \cdot n^{0.6}.
    \end{align*}
    So it follows that 
    \begin{align*}
        |C_j| = \sum_{i=1}^{n^{0.2}} |S_i \cap C_j| = (1 \pm o(1)) \cdot n^{0.8}.
    \end{align*}
}
\thm{Siegel 1989}{
    There exists a hash function $h$ that 
    \begin{itemize}
        \item $O(1)$ time.
        \item $n^{0.9}$ words.
        \item $n^{0.8}$ independent with high probability.
    \end{itemize}
}
\thm{Ostlin + Pagh 2001 $\to$ Pagh + Pagh 2004}{
    Can do 
    \begin{itemize}
        \item $O(1)$ time.
        \item $O(n)$ words.
        \item On any set $S$ of size $n$, we are random with high probability.
    \end{itemize}
}
\chapter{Retrieval Data Structures}
\noindent Our input will be some set $S \subseteq U$, where $U$ is of $\poly(n)$ size and $|S| = n$. We are also given a function $f: S \to \{0, 1\}$. The goal is to output a data structure $D$ supporting queries of the form: on an element $x \in U$, 
\begin{align*}
    D(x) = \begin{cases}
        f(x) & x \in S \\
        * \leftarrow \text{ anything} & \text{otherwise}
    \end{cases}.
\end{align*}
Our goal today is to make $D$ as small as possible. 
\begin{enumerate}
    \item We don't care about time.
    \item We will assume access to infinite type of random bits.
\end{enumerate}
\thm{}{
    There exists a solution using expected space $n + O(1)$ bits. 
}
\begin{proof}
    Let $h_1, h_2, \ldots$ be an infinite sequence of random hash functions from $U \to \{0,1\}$. Let $h_i$ be the first such hash function such that 
    \begin{align*}
        h_i(x) = f(x)
    \end{align*}
    for all $x \in S$. Define $D$ to store the number $i$. 

    \noindent Observation: Each $h_j$ has $1/2^n$ probability of working. So,
    \begin{align*}
        E[i] = 2^n \implies E[\lg i] \leq n
    \end{align*}
    by Jensen's inequality. So, $E[|D|] = n + o(n)$. 
\end{proof}
Now we have the value-dynamic retrieval problem. Suppose I want to add an operation:
\begin{itemize}
    \item $\mathrm{Toggle}(x)$, where $x \in S$, such that $f(x) \leftarrow \lnot f(x)$.
\end{itemize}
\thm{}{
    There xists a value dynamic solution using $n + n\lg e + o(n)$ expected bits. 
}
\begin{proof}
    Store
    \begin{itemize}
        \item Bijection $\phi: S \to \{1, \ldots n\}$.
        \item $n$ bits storing $f(x)$ is position $\phi(x)$ for each $x \in S$.
    \end{itemize}
    For $\phi$, let $\phi_1, \phi_2, \ldots$ be random functions from $U \to \{1, \ldots n\}$. Let $\phi$ be the first $\phi_i$ that works. Each $\phi_j$ works with probability $\frac{n!}{n^n}$. So,
    \begin{align*}
        E[i] \approx e^n \implies E[\lg i] \approx \lg e^n = n\lg e.
    \end{align*}
\end{proof}
\thm{}{
    The value-dynamic problem requires $n + \Omega(n)$ bits. 
}
We're going to assume there exists a solution using $n + o(n)$ bits. Then we are going to construct and impossibly good compression scheme. 

\noindent Warmup 1: Let $S$ be $n$ elements and $f: S \to \{0, 1\}$. Let $D_1$ store $f$ and let $D_2$ be $D_1$ after we toggle every element in $S$. Define the ``churn set'' as
\begin{align*}
    \Delta = \{u \in U \mid D_1(u) \neq D_2(u) \}.
\end{align*}
We claim that if $E[| \Delta |] \leq \frac{|U|}{1000}$, then we can arrive at a contradiction (an impossibly good compression scheme).

\noindent Our procotol:
\begin{itemize}
    \item Alice will send $D_1$, $D_2$, and $S$ as a subset of $\Delta$. 
\end{itemize}
Then,
\begin{align*}
    E[|M|] &= n + n + o(n) + E\left[\lg \binom{|\Delta|}{n}\right] \\
    &\leq n + n + o(n) + \lg \binom{|U|/1000}{n}  \tag{skip a few steps} \\
    &\leq n + n + o(n) + \lg \binom{|U|}{n} - 9n. \tag{skip some more}
\end{align*}
Bob will use $D_1, D_2$ to get $\Delta$, using that to recover $S$. However, $|S|$ is $\lg \binom{|U|}{n}$ bits. This yields a contradiction. 

\noindent Warmup 2: Let $S_1$ and $S_2$ be disjoint $n/2$ element subsets of $U$. Let $S = S_1 \cup S_2$. Our thought experiment is what if we start with $D_1$ and $D_2$ is the data structure after we just toggle $S_1$. As before, define the churn set as 
\begin{align*}
    \Delta = \{u \in U \mid D_1(u) \neq D_2(u)\}.
\end{align*}
Suppose $E[\Delta] \geq \frac{|U|}{2000}$. We claim once again that we get a contradiction in the same way.

\noindent Our protocol:
\begin{itemize}
    \item Alice will send $D_1$, $S_1$ as a subset of $U$, and $S_2$ as a subset of $U \setminus \Delta$. 
\end{itemize}
Now Bob will use $D_1$ and $S_2$ to build $D_2$. Now he can proceed as before to use $D_1$ and $D_2$ to get $\Delta$, which he then uses that to recover $S_2$ and therefore $S$ and $f$. Therefore, Bob recovers approximately $2 \lg \binom{|U|}{n/2} + n$ bits. So now we have 
\begin{align*}
    E[|M|] &= n + o(n) + \lg \binom{|U|}{n/2} + E \left[ \lg \binom{|U| - |\Delta|}{n/2} \right] \\
    &\leq n + 2 \lg \binom{|U|}{n/2} - \Omega(n).
\end{align*}
As such, we have yet again arrived at a contradiction. 

We move to proving the theorem:
\begin{proof}
We have the following set of data structures:
\begin{align*}
    D_1 \xrightarrow{\text{toggle}(S_1)} D_2 \xrightarrow{\text{toggle}(S_2)} D_3
\end{align*}
Define $\Delta_1$ as the churn set between $D_1$ and $D_2$. Define $\Delta_2$ as the churn set between $D_2$ and $D_3$. And finally $\Delta_3$ as the churn set between $D_1$ and $D_3$. Either 
\begin{itemize}
    \item $E[|\Delta_1|] \geq \frac{|U|}{2000}$,
    \item $E[|\Delta_2|] \geq \frac{|U|}{2000}$,
    \item $E[|\Delta_3|] \leq \frac{|U|}{1000}$.
\end{itemize}
At least one of these has to occur. In the first situation, we win by warmup 2, as with the second situation. However, in the third situation, we win by warmup 1. 
\end{proof}
\end{document}