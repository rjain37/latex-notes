\documentclass{report}

\input{../preamble}
\input{../macros}
\input{../letterfonts}

\DeclareMathOperator{\length}{length}
\DeclareMathOperator{\vol}{vol}
\DeclareMathOperator{\dist}{dist}

\title{\Huge{21-269}\\Vector Analysis}
\author{\huge{Rohan Jain}}
\date{}

\begin{document}

\maketitle
\newpage% or \cleardoublepage
% \pdfbookmark[<level>]{<title>}{<dest>}
\pdfbookmark[section]{\contentsname}{toc}
\tableofcontents

\pagebreak

\chapter{}
\section{The Real Numbers}
\dfn{Partial Order}{Let $X$ be a set with a binary relation $\leq$. $\leq$ is a \emph{partial order} if:
\begin{enumerate}
    \item $x \leq x$ for all $x \in X$ (reflexivity)
    \item $x \leq y$ and $y \leq z$ implies $x \leq z$ for all $x, y, z \in X$ (transitivity)
    \item $x \leq y$ and $y \leq x$ implies $x = y$ for all $x,y \in X$ (antisymmetry)
\end{enumerate} }

\dfn{Partially Ordered Set (poset)}{A set $X$ with a partial order $\leq$ is called a \emph{partially ordered set} or \emph{poset}. It is notated as $(X, \leq)$.}

\dfn{Total Order}{A partial order $\leq$ is a \emph{total order} if for all $x,y \in X$, we have $x \leq y$ or $y \leq x$.}

\ex{poset}{Let $Y$ be a set. Define $X = \{\text{all subsets of } Y\} = \mathcal{P}(Y)$. Let $E, F \in Y$, we say that $E \leq F$ if $E \subseteq F$. Then $(X, \leq)$ is a poset. This is not a total order.}

\dfn{Upper Bound, Bounded Above, Supremum, Maximum}{Let $(X, \leq)$ be a poset. Let $E \subseteq X$.
\begin{enumerate}
    \item $y \in X$ is an \emph{upper bound} of $E$ if $x \leq y$ for all $x \in E$.
    \item $E$ is \emph{bounded above} if it has at least one upper bound.
    \item If $E$ is nonempty and bounded above, then the \emph{supremum}, if it exists, of $E$, denoted $\sup E$, is the least upper bound of $E$.
    \item $E$ has a \emph{maximum} if there is $y \in E$ such that $ x \leq y$ for all $x \in E$.
\end{enumerate}}

Properties worth mentioning:
\begin{enumerate}
    \item If $E$ has a maximum, then $\sup E$ exists and is equal to the maximum.
    \begin{proof}
        Let $y$ be the maximum of $E$. If $z \in X$, is an upper bound of $E$, then $z \geq y$ because $y \in E$. Since $z$ was arbitrary, this is true for any upper bound. Thus, $y$ is the least upper bound of $E$.
    \end{proof}
\end{enumerate}

\ex{}{Let $Y$ be a nonempty set, $(\mathcal{P}(Y), \leq)$ poset.

Fix nonempty $Z \subseteq Y$. \[E = \{ W \subseteq Y : W \subset Z\}\]
Trivially, $Z$ is an upper bound of $E$. Realize that any superset of $Z$ is an upper bound as well. We can postulate that the supremum of $E$ is $Z$. We will now prove it:
\begin{proof}
    Need to show that if $F$ is an upper bound of $E$, then $F \supseteq Z$. If $x \in Z$, then $\{x\} \in E$ by definition of $E$, so $F \supseteq {x}$ for all $x \in Z$. Thus, $F \supseteq Z$.
\end{proof}
Note that there is no maximum of $E$. 
}

\dfn{Lower Bound, Bounded Below, Infimum, Minimum}{Let $(X, \leq)$ be a poset. Let $E \subseteq X$.
\begin{enumerate}
    \item $y \in X$ is a \emph{lower bound} of $E$ if $y \leq x$ for all $x \in E$.
    \item $E$ is \emph{bounded below} if it has at least one lower bound.
    \item If $E$ is nonempty and bounded below, then the \emph{infimum}, if it exists, of $E$, denoted $\inf E$, is the greatest lower bound of $E$.
    \item $E$ has a \emph{minimum} if there is $y \in E$ such that $ y \leq x$ for all $x \in E$.
\end{enumerate}
}
Going back to example 1.1.2, we can see that $E$ is bounded below by $\emptyset$. The infimum of $E$ is $\emptyset$. The minimum of $E$ is also $\emptyset$.
\dfn{Complete}{Let $(X, \leq)$ poset. $X$ is \emph{complete} if every nonempty subset of $X$ that is bounded above has a supremum.}
\ex{$\QQ$}{$(\QQ, \leq)$ is not complete.}
\clm{$\RR$}{There is a complete ordered field $(\RR, +, \cdot, \leq)$. Its elements are called real numbers.}
\section{First Recitation, 1/18}
\exer{Function Example}{Let $X$ be the set of all functions $f : D_f \to Z$ with $D_f \subseteq Y$. We say that $f \leq g$ if $D_f \subseteq D_g$ and $f(x) = g(x)$ for all $x \in D_f$. Is $(X, \leq)$ a poset? Is it complete?
\begin{proof}
    To show that $(X, \leq)$ is complete, we need to show that every nonempty subset of $X$ that is bounded above has a supremum. Let $E \subseteq X$ be nonempty and bounded above. Let $G = \bigcup_{f \in E} D_f$. $G$ is the union of all the domains of the functions in $E$. $G$ is bounded above by the union of the upper bounds of the domains of the functions in $E$. Let $H = \bigcup_{f \in E} f(D_f)$. $H$ is bounded above by the union of the upper bounds of the ranges of the functions in $E$. Let $F : G \to H$ be defined as $F(x) = f(x)$ for all $x \in D_f$. $F$ is the supremum of $E$.
\end{proof}}
\section{Natural Numbers}
\exer{}{Take $(X, +, \cdot,\leq)$ ordered field. Prove:
\begin{enumerate}
    \item If $0 \leq x$, then $-x \leq 0$. 
    \item If $x \leq y$, and $0 \leq z \neq 0$, then $xz \leq yz$.
    \item For all $x \in X$, $0 \leq x^2$.
    \item Prove $0 < 1$. 
\end{enumerate}}
\begin{proof}
    Fields have the following important properties:
    \begin{itemize}
        \item If $a \leq b$, then $a + c \leq b + c$.
        \item If $a, b \geq 0$, then $ab \geq 0$.
    \end{itemize}
    \begin{enumerate}
        \item Take the first property with $a = 0$, $b = x$, and $c = -x$. Then $0 \leq x \implies 0 + (-x) \leq x + (-x) \implies -x \leq 0$.
        \item If $x \leq y$, then $0 \leq y + (-x)$. By the second property, $0 \leq z \cdot(y + (-x)) = zy + (-zx)$. Then $0 \leq zy + (-zx) \implies zx \leq zy$. 
        \item We split into the three trichotomy cases:
        \begin{itemize}
            \item If $x = 0$, then $0 \leq 0^2$.
            \item If $x < 0$ with $x \neq 0$, then $0 \leq -x$. By the second property, $0 \leq (-x)^2 = (-x)(-x) = x^2$.
            \item If $x > 0$, then $0 \leq x$. By the second property, $0 \leq x^2$.
        \end{itemize}
        \item FSOC, assume $0 > 1$ and multiply both sides by 1. Then we get $0 \cdot 1 > 1 \cdot 1 \Rightarrow 0 > (1)^{2}$, which is a contradiction to the third property we proved.
    \end{enumerate}
\end{proof}

\dfn{Inductive}{Take $E \subseteq \RR$. $E$ is \emph{inductive} if $1 \in E$ and $x \in E$ implies $x+1 \in E$.}
\ex{Inductive Sets}{\begin{itemize}
    \item $\RR$ is inductive.
    \item $\{x \in \RR: 0 \leq x\}$
    \begin{proof}
        $1 \in E$ because $1 \geq 0$. If $x \in E$, then $x+1 \geq 0$, so $x+1 \in E$.
    \end{proof}
\end{itemize}}
\dfn{Natural Numbers}{The intersection of all inductive sets is denoted $\NN$. The elements of $\NN$ are called \emph{natural numbers}.}
\noindent Properties of $\NN$:
\begin{itemize}
    \item $\NN \neq \emptyset$. Since $1 \in$ every inductive set, $1 \in \NN$.
    \item $\NN$ is an inductive set.
\end{itemize}
\thm{Induction}{For every $n \in \NN$, let $P(n)$ be a proposition such that:
\begin{enumerate}
    \item $P(1)$ is true. 
    \item If $P(n)$, then $P(n+1)$.
\end{enumerate}Then $P(n)$ is true for every $n \in \NN$ }
\begin{proof}
$E = \{n \in \NN\ :\ P(n)\}$ is inductive by 1. and 2. So, $\NN \subseteq E$, but $E \subseteq \NN$ by definition of $\NN$. Thus, $E = \NN$.
\end{proof}
\thm{Archimedean Property}{Let $a, b \in \RR$ with $a > 0$. Then there is $n \in \NN$ such that $na > b$.}
\begin{proof}
    If $b \leq 0$, then we take $n = 1$. Assume $b >0$. For sake of contradiction, assume there does not exist $n$ such that $na > b$. Then $E = \{na : n \in \NN\}$ is bounded above by $b$. Let $c = \sup E$. $c - a \leq c$, so $c-a$ is not an upper bound of $E$. Thus, there is $n \in \NN$ such that $c-a \leq na$. Then $c \leq (n+1)a$. But $c$ is an upper bound of $E$, so $c \geq (n+1)a$. Thus, $c = (n+1)a$. But $c \in E$, so $c = na$ for some $n \in \NN$. Thus, $na = (n+1)a$, so $n = n+1$, which is a contradiction.
\end{proof}
\dfn{Integers}{$\ZZ := \NN \cup \{0\} \cup \{-n : n \in \NN\}$}
\thm{Integer Part}{For every $x \in \RR$, there is a unique $k \in \ZZ$ such that $k \leq x < k+1$.}
\dfn{Integer Part}{The $k$ that satisfies the above theorem is called the \emph{integer part} of $x$, denoted $\lfloor x \rfloor$.}
\begin{proof}
    Let $E = \{k \in \ZZ : k \leq x\}$. First we show that $E$ is nonempty.
    \begin{itemize}
        \item If $x \geq 0$, then $0 \in E$, so $E$ is nonempty.
        \item If $x < 0$, then $-x > 0$. By the Archimedean property, there is $n \in \NN$ such that $n > -x$. Thus, $-n < x$. So, $-n \in E$, so $E$ is nonempty.
    \end{itemize}
    Now we show that $E$ is bounded from above. Very clearly, $x$ is an upper bound. By supremum property, there is $L = \sup(E)$ and $L \in \RR$. $L-1$ is not an upper bound, which means that there is an element $k \in E$ such that $L-1 < k$. But since $L$ is the supremum, $L \geq k$. Thus, $L-1 < k \leq L$. So, $L < k+1$ so $k + 1 \notin E$. Now, $k \leq x$ since $k \in E$.  Now we show that $k$ is unique. Assume there is $m \in \ZZ$ such that $m \leq x < m+1$. Then $m \in E$, so $m \leq L$. But $L$ is the supremum, so $L \geq m$. Thus, $L = m$. So, $k = m$.
\end{proof}
\dfn{$\QQ$}{If $p \in \ZZ$ with $p \neq 0$, then $\exists p^{-1} \in \RR$. Define $\QQ = \{pq^{-1} : p, q \in \ZZ, p \neq 0\}$.}
\section{Density of Rationals}
\thm{Density of the Rationals}{Let $a, b \in \RR$ with $a < b$. Then there is $r \in \QQ$ such that $a < r < b$.}
\begin{proof}
We have $a < b \implies 0 = a + (-a) < b - a \implies 0 < \frac{1}{b-a}$. By the integer part theorem, there is $q \in \ZZ$ such that $\frac{1}{b-a} < q$. So now, $\frac{1}{q} < b -a \implies a < a + \frac{1}{q} < b$. Multiply both sides by $q > 0$ to get $aq < a + 1 < bq$. By the integer part theorem, there is $p \in \ZZ$ such that $p \leq qa < p + 1$ (i.e. $p = \lfloor qa \rfloor$). Since $qa < p + 1 \leq qa + 1 < qb$. Getting rid of unnecessary stuff, we have $qa < p + 1 < qb$. Thus, $a < \frac{p+1}{q} < b$. Let $r = \frac{p+1}{q}$. Then $r \in \QQ$ and $a < r < b$.
\end{proof}
\dfn{Irrational Numbers}{$\RR \setminus \QQ$ is the set of \emph{irrational numbers}.}
\exer{TODO in Recitation 1/23}{
    \begin{itemize}
        \item Prove that there is no $r \in \QQ$ such that $r^2 = 2$.
        \item Prove that ``$\sqrt{2}$'' exists in $\RR$. (prove that there is at least one irrational number)
        \begin{itemize}
            \item Have to play with the set $E = \{ x \in \RR : x > 0, x^2 < 2\}$.
        \end{itemize}
    \end{itemize}}
\thm{Density of Irrationals}{Let $a, b \in \RR$ with $a < b$. Then there is $x \in \RR \setminus \QQ$ such that $a < x < b$.}
\begin{proof}
    $a < b \implies a\sqrt{2} < b\sqrt{2}$. By the density of rationals, there is $r \in \QQ$ such that $a\sqrt{2} < r < b\sqrt{2}$. Then $a < \frac{r}{\sqrt{2}} < b$. Let $x = \frac{r}{\sqrt{2}}$. If $r = 0$, then $a \sqrt{2} < 0 < b \sqrt{2}$. By previous theorem, we can find $q \in \QQ$ such that $a \sqrt{2} < q < 0 < b \sqrt{2}$. Then $a < \frac{q}{\sqrt{2}} < b$. Let $x = \frac{q}{\sqrt{2}}$. Then $x \in \RR \setminus \QQ$ and $a < x < b$.
\end{proof}
\nt{Take $x \in \RR$, $E = \{ r \in \QQ :  r < x\}$. $x$ is the upper bound of $E$. This set is nonempty because we can take $x - 1 < r < x$. Now we prove that $x = \sup E$.
\begin{proof}
    Assume  $\exists L$ upper bound of $E$ such that $L < x$. Then $L < x \implies$ there exists some $r \in \QQ$ such that $L < r < x$, but $r \in E$, so $L$ is not an upper bound of $E$. Thus, $L$ cannot be an upper bound of $E$ and $x$ is the least upper bound of $E$.
\end{proof}}
Since now we know that $\sqrt{2} = \sup\{r \in \QQ : r < \sqrt{2}\}$, we can also define $3^{\sqrt{2}} = \sup\{3^r : r \in \QQ, r < \sqrt{2}\}$.
\dfn{$x^0$}{Let $0 \neq x  \in \RR$. We define $x^0 = 1$.}
\dfn{$x^n$}{Let $x \in \RR$, $n \in \NN$. We start with $x^1 := x$. Then assume $x^m$ has been defined. Then we say $x^{m+1} := x^m \cdot x$.}
\dfn{$x^{p/m}$}{Let $x \in \RR$, $p \in \ZZ$, $m \in \NN$. We say $x^{p/m} = \sqrt[m]{x^p}$.}
\exer{Properties of Exponenets}{Let $x \in \RR$, $r, q \in \QQ$, and $x,r,q > 0$. Prove the following:
\begin{itemize}
    \item $x^r \cdot x^q = x^{r+q}$
    \item $(x^r)^q = (x^q)^r = x^{rq}$
\end{itemize}}
\begin{proof}

\end{proof}
\dfn{Negative Exponent}{Take $x > 0$, $r = -\frac{p}{m}$ for $p,m \in \NN$. First, we have that $x^{-r} := (x^{-1})^{p/m}$.}
\exer{More Properties of Exponents}{Take $x \in \RR, x > 0, r, q \in \QQ$. Prove the following:
\begin{itemize}
    \item If $r > 0$, prove that $x^r > 1$.
    \item If $r < q$, prove that $x^r < x^q$.
\end{itemize}}
\section{1/23 - Recitation - Proving Irrationality of $\sqrt{2}$}
Existence of $\sqrt{2}$:
\begin{enumerate}
    \item Let $E = \{x \in \RR : x >0, x^2 < 2\}$. Prove that $E$ is non-empty and that $E$ is bounded above.
    \begin{proof}
        We know that $0 < 1$ and from that we get $1^2  = 1 < 2$, which can be checked by subtracting $1$ from both sides. As such $E$ is nonempty.

        Now we show that $E$ is bounded above. We know that $2^2 = 4 > 2 > a^2 \in E$, so $2^2 > a^2 \Rightarrow 2 > a$, so $2$ is an upper bound of $E$.
    \end{proof}
    \item By the completeness of $(\RR, \leq)$, $E$ has a supremum, $L$. Prove that $L > 0$ and that $L^2 = 2$.
    \begin{proof}
        Since $L$ is the least upper bound, it has to be greater than 1 which is in the set $E$. Therefore, $L > 1 > 0 \implies L > 0$. 

        Now we show that $L^2 \geq 2$. For sake of contradiction, assume $L^2 < 2$. Since $L > 0$, this means that $ L \in E$. By the density of rationals, there exists $r \in \QQ$ such that $L < r < \sqrt{2}$. Since $L$ is an upper bound of $E$, $r \notin E$. But $r \in \QQ$, so $r^2 \neq 2$. Thus, $r^2 > 2$. Since $r > 0$, $r^2 > 2 \implies r > \sqrt{2}$. But $r < \sqrt{2}$, so we have a contradiction. Thus, $L^2 \geq 2$.
    \end{proof}
    \item Prove that if $y \in \RR \setminus E$ and $y > 0$, then $y$ is an upper bound of $E$.
    \begin{proof}
        Assume $y \in \RR \setminus E$ and $y > 0$. We need to show that $y$ is an upper bound of $E$. Assume for sake of contradiction that $y$ is not an upper bound of $E$. Then there exists $x \in E$ such that $x > y$. But $x \in E \implies x^2 < 2$. Since $y > 0$, $x^2 < 2 \implies y^2 < 2$. But $y \notin E$, so $y^2 \geq 2$. But this would mean that $y \in E$. Contradiction. Thus, $y$ is an upper bound of $E$.
    \end{proof}
    \item Prove that $L^2 = 2$.
    \begin{proof}
        We know that $L^2 \geq 2$ from part 2. Now we show that $L^2 \leq 2$. Assume for sake of contradiction that $L^2 > 2$. 

        How small does $\epsilon > 0$ need to be such that $(L-\epsilon)^2 >2$ as well. 

        Start with $(L-\epsilon)^2 = L^2 - 2L\epsilon + \epsilon^2$, which is greater than $L^2 - 2L\epsilon$ since $\epsilon > 0$. So now, how small does $\epsilon$ need to be such that $L^2 > 2 \implies L^2 - 2L\epsilon > 2$ too. 
        \begin{align*}
            2L\epsilon &< 2 - L^2 \\
            \epsilon &< \frac{2 - L^2}{2L} \\
        \end{align*}
        Since $L^2 > 2$, this means that an $\epsilon$ can be found. This means that $L$ is not the least upper bound. Contradiction. Thus, $L^2 \leq 2$.
    \end{proof}
\end{enumerate}
\newpage
\section{Exponents}
\dfn{$\sqrt{2}$}{$$\sqrt{2} := \sup\{x \in \RR : x > 0, x^2 < 2\}$$}
\exer{}{For $n \in \NN, n \geq 2$. Fix $x > 0$. 
$$E = \{y \in \RR: y > 0, y^n < x\}.$$Prove that $l = \sup E$ satisfies $l^n = x$.}
\begin{proof}
    We first need to show that $\sup E$ exists. Let $y = x / (1+x)$. Then, $0 \leq y < 1$, so $y^n \leq y < x$. Thus, $y \in E$. So, $E$ is nonempty. $E$ is also bounded from above because $x$ is an upper bound of $E$. Thus, $\sup E$ exists by the completeness of $\RR$. Let $l = \sup E$. We now show that $l^n = x$.

    First we show that $l^n \leq x$. FSOC, assume $l^n > x$. If you choose an $\epsilon > 0$ that is small enough, then $(l-\epsilon)^n > x$ as well. We can't do this because $y > l - \epsilon$ for some $y \in E$ since $l$ is the supremum of $E$. As such, we arrive at a contradiction which  means that $l^n \leq x$.

    To show that $l^n \geq x$, assume FSOC that $l^n < x$. Then we can choose an $\epsilon$ such that $(l + \epsilon)^n < x$, meaning we have an element $(l + \epsilon)$ which is in $E$ but bigger than the supremum, which is a contradiction. 
    
    Thus, $l^n \geq x$. 
\end{proof}
\dfn{$\sqrt[m]{x}$}{$$\sqrt[m]{x} := \sup\{y \in \RR : y > 0, y^m < x\}$$}
\dfn{$x^{p/q}$}{$$x^{p/q} := \left( \sqrt[q]{x} \right) ^p$$}
\dfn{$x^q$}{For $q \in \RR$, $q > 0$, and $x > 1$. $$x^q := \sup\{x^r : r \in \QQ, 0 < r < q\}$$} 
\ex{}{$$\sqrt{2} = \sup\{r \in \QQ : r > 0, r < \sqrt{2}\}$$}
\thm{}{Take $a, b \in \RR$, $a, b>  0$ and $x \in \RR > 1$. Then $x^a \cdot x^b = x^{a+b}$.}
\begin{proof}
    Let $E_i = \{ x^r : r \in \QQ, r > 0, r < i\}$. Consider $E_a$, $E_b, E_{a+b}$. Then let $l_i = \sup(E_i)$. Consider $l_a, l_b, l_{a+b}$. We want to show that $l_a \cdot l_b = l_{a+b}$ by showing that both $l_a \cdot l_b \leq l_{a+b}$ and $l_a \cdot l_b \geq l_{a+b}$.

    Let $r \in \QQ$ with $0 < r < a$. Let $s \in \QQ$ with $0 < s < b$. Then we have that $x^r \cdot x^s = x^{r+s}$ (from the exercise two days ago and since $r,s \in \QQ$.) we know that $0 < r + s < a + b$ and is rational. Thus, $x^{r+s} \in E_{a+b}$. Thus, $x^r \cdot x^s \leq l_{a+b}$.

    We want to divide both sides by $x^s$ while fixing $r$. So, we have that $ x ^r \leq \dfrac{l_{a+b}}{x^s}$, which is true for all $r \in \QQ$, such that $0 < r < a$. Thus, $\dfrac{l_{a+b}}{x^s}$ is an upper bound for $E_a$. Thus, $l_a \leq \dfrac{l_{a+b}}{x^s}$. Thus, $ x^s \leq \dfrac{l_{a+b}}{l_a}$, meaning that $\frac{l_{a+b}}{l_a}$ is an upper bound for $E_b$. Thus, $l_b \leq \dfrac{l_{a+b}}{l_a}$. Thus, $l_a \cdot l_b \leq l_{a+b}$.

    Now we show that $l_a \cdot l_b \geq l_{a+b}$. Let $t \in \QQ$ with $0 < t < a + b$. We need $0 < r \in \QQ < a$ and $0 < s \in \QQ < b$ with $t = r + s$. We start by looking at $t - a < b$. By the density of $\QQ$, find $s \in \QQ$ such that $t-a < s < b$. Take $s > 0$ beacuse $b > 0$. So $t -s < a$. By the density of $\QQ$, find $0 < p \in \QQ$ such that $t-s < p < a$. So $t < s + p$. So, $x^t < x^{s+p} = x^{s}x^{p} \leq l_a l_b$ since $x^s \in E_b$ and $x^p \in E_a$. We know that $l_a l_b$ is an upper bound of $E_{a+b}$, so $l_{a+b} \leq l_a l_b$.

    Therefore $l_a \cdot l_b = l_{a+b}$.
\end{proof}
\dfn{Negative Exponents}{Let $x > 1$, $a < 0$. Then: $$x^a := \left(x^{-a}\right)^{-1}$$}
\dfn{Exponents between 0 and 1}{Let $x \in \RR$ with $0 < x < 1$ and $a > 0$. Then:
$$x^a := \left(\dfrac{1}{x}\right)^{-a}$$}
An important note is that if we have $E \subseteq (0, \infty)$ with a bounded $E$. Then if we define $F = \{ \dfrac{1}{x} : x \in E\}$, then we have the following:
\begin{align*}
    \sup E &= \dfrac{1}{\inf F} \\
    \inf E &= \dfrac{1}{\sup F}
\end{align*}
\section{1/25 - Recitation - Sequences of Set}
\dfn{Sequence of a Set}{
Given a set $X$, a sequence on $X$ is a function $f : \NN \to X$. We denote $f(n)$ as $x_n$. We can also denote the sequence as $\{x_n\}_{n=1}^\infty$.}
\dfn{}{Let $(X, \leq)$ be a poset and $\{x_n\}_{n=1}^\infty$ be a sequence on $X$. Then $E = \{x_n : n \in \NN\}$ is a subset of $X$. We say that $\{x_n\}_{n=1}^\infty$ is bounded from above is the set $E$ is bounded from above. We say that $\{x_n\}_{n=1}^\infty$ is bounded from below is the set $E$ is bounded from below. We say that $\{x_n\}_{n=1}^\infty$ is bounded if it is bounded from above and below.}
\dfn{Limit Superior}{Let $(X, \leq)$ be a poset. Let $\{x_n\}_{n=1}^\infty$ be a sequence on $X$. Suppose $\{x_n\}_n$ is bounded from above. Then, we define the \emph{limit superior} of $x_n$ as $n \to \infty$ as:
$$\limsup_{n\to\infty} x_n = \inf_{n \in \NN}\sup_{k \geq n} x_k$$}
\dfn{Limit Inferior}{Let $(X, \leq)$ be a poset. Let $\{x_n\}_{n=1}^\infty$ be a sequence on $X$. Suppose $\{x_n\}_n$ is bounded from below. Then, we define the \emph{limit inferior} of $x_n$ as $n \to \infty$ as:
$$\liminf_{n\to\infty} x_n = \sup_{n \in \NN}\inf_{k \geq n} x_k$$}
\newpage
\exer{}{\begin{enumerate}
    \item Let $\{x_n\}_{n=1}^\infty$ be a sequence on $\RR$ bounded above. Prove that $L \in \RR$ is the $\limsup$ of $\{x_n\}_{n=1}^\infty$ iff for every $\epsilon>0$, there exists $n_\epsilon \in \NN$ such that:
    \begin{enumerate}
        \item $x_n < L + \epsilon$ for all $n \geq n_\epsilon$.
        \item $L - \epsilon < x_n$ for infinitely many $n$.
    \end{enumerate}
\end{enumerate}}
\begin{proof}
    Let $L \in \RR$ be the $\limsup$ of $\{x_n\}_{n=1}^\infty$. Let $\epsilon > 0$. $L$ being the lim sup means that $L = \inf_{n \in \NN}\sup_{k \geq n} x_k$. Thus, $L \leq \sup_{k \geq n} x_k$ for all $n \in \NN$. Thus, $L - \epsilon < \sup_{k \geq n} x_k$ for all $n \in \NN$. Then $L - \epsilon$ is not an upper bound of $\{x_n\}_{n=1}^\infty$. Thus, there is $n_\epsilon \in \NN$ such that $L - \epsilon < x_{n_\epsilon}$. Thus, $L - \epsilon < x_n$ for infinitely many $n$. Now we show that $x_n < L + \epsilon$ for all $n \geq n_\epsilon$. Assume for sake of contradiction that there is $n \geq n_\epsilon$ such that $x_n \geq L + \epsilon$. Then $L + \epsilon$ is an upper bound of $\{x_n\}_{n=1}^\infty$. But $L$ is the $\limsup$, so $L \geq L + \epsilon$. Contradiction. Thus, $x_n < L + \epsilon$ for all $n \geq n_\epsilon$.

    Now we show the other direction. Assume that for every $\epsilon > 0$, there exists $n_\epsilon \in \NN$ such that $x_n < L + \epsilon$ for all $n \geq n_\epsilon$ and $L - \epsilon < x_n$ for infinitely many $n$. We want to show that $L$ is the $\limsup$ of $\{x_n\}_{n=1}^\infty$. We know that $L$ is an upper bound of $\{x_n\}_{n=1}^\infty$. We need to show that $L$ is the least upper bound. Assume for sake of contradiction that $L$ is not the least upper bound. Then there is $L' < L$ such that $L'$ is an upper bound of $\{x_n\}_{n=1}^\infty$. Let $\epsilon = L - L'$. Then $L' < L - \epsilon$. But $L - \epsilon < x_n$ for infinitely many $n$. But $L' < L - \epsilon$, so $L'$ is not an upper bound of $\{x_n\}_{n=1}^\infty$. Contradiction.
\end{proof}
\section{Vector Spaces}
\ex{Vector Spaces}{\begin{itemize}
    \item Euclidean Space $\subseteq \RR^n$. $x \in \RR^n$ is a vector. $x = (x_1, \dots, x_n)$.
    \item Polynomial Space from $\RR \to \RR$. $x \in \RR[x]$. $x = a_0 + a_1x + \dots + a_nx^n$.
    \item $f : [a, b] \to \RR$ continuous functions.
\end{itemize}}
\dfn{Boundedness of Functions}{Let $E$ be a set and $f : E \to \RR$. \begin{enumerate}
    \item $f$ is bounded from above if the set $f(E) = \{ y \in \RR : y = f(x), x \in E\}$ is bounded from above.
    \item $f$ is bounded from below if the set $f(E) = \{ y \in \RR : y = f(x), x \in E\}$ is bounded from below.
    \item $f$ is bounded if $f(E)$ is bounded. 
\end{enumerate}}
\dfn{Inner Product}{A function $(\cdot, \cdot) : V \times V \to \RR$ is an \emph{inner product} if it satisfies the following properties:
\begin{itemize}
    \item $(x, x) \geq 0$ for all $x \in X$. 
    \item $(x, x) = 0$ iff $x = 0$.
    \item $(x, y) = (y, x)$ for all $x, y \in X$.
    \item $(sx + ty, z) = s(x, z) + t(y, z)$ for all $x, y, z \in X$ and $s, t \in \RR$.
\end{itemize}}
\ex{Examples of Inner Products}{\begin{itemize}
    \item $\RR^n$ with dot products.
    \item $f : [a, b] \to \RR$ with $(f, g) = \int_a^b f(x)g(x)dx$. This is is not an inner product because we can define:
    \begin{align*}
        f = \begin{cases} 1 & x = 0.5 \\ 0 & \text{otherwise} \end{cases} \\
    \end{align*}
    which has an integral of 0. But $f \neq 0$.
    If we add that $f$ is continuous, then it is an inner product.
\end{itemize}}
\dfn{Norm}{Let $V$ be a vector space with an inner product $(\cdot, \cdot)$. Then the \emph{norm} of $x \in X$ is defined as $||\cdot||: X \to [0, \infty)$ such that:
\begin{enumerate}
    \item $||x|| = 0 \iff x = 0$
    \item $||tx|| = |t|||x||$ for all $x \in X$
    \item $||x + y|| \leq ||x|| + ||y||$ for all $x, y \in X$
\end{enumerate}}
\ex{Examples of Norms}{\begin{itemize}
    \item $||x|| = \sqrt{(x, x)}$ for $x \in \RR^n$
    \item $X = \{f : E \to \RR, f \text{ bounded}\}$. $||f|| = \sup_{x \in E} |f(x)|$.
    \begin{itemize}
        \item First property is obviously true.
        \item For the second property, we use the fact that $$\sup(tF) = \begin{cases} t\sup(F) & \text{if } t \geq 0 \\ t\inf(F) & \text{if } t< 0 \end{cases}$$
        \item For the third property, we use the triangle inequality:
        \begin{align*}
            \sup|f + g| &\leq \sup|f| + \sup|g| \\
            |f(x) + g(x)| \leq |f(x)| + |g(x)| \leq \sup|f| + \sup|g|
        \end{align*}
    \end{itemize}
\end{itemize}}
\nt{Space of bounded functions denoted as $\ell^\infty (E) = \{f : E \to \RR : f \text{ bounded}\}$.}
\thm{Cauchy Schwarz Inequality}{Let $X$ be a vector space with an inner product $(\cdot, \cdot)$. Then for all $x, y \in X$, we have that $|(x, y)| \leq \sqrt{(x, x)} \cdot \sqrt{(y, y)}$.}
\begin{proof}
    Let $y \neq 0$. Consider $(x + ty, x+ty) = (x, x + ty) + t(y, x + ty) = (x, x) + t(x, y) + t(y, x) + t^2(y, y)$. We can combine the middle terms to get $t^2(y, y) + 2(x, y) + (x,x)$, which is quadratic in $t$. Take $t = -\dfrac{(x, y)}{(y, y)}$. \begin{align*}
        0 &\leq (x, x) - 2\frac{{(x, x)}^2}{(y, y)} + \frac{(x, y)^2}{(y, y)} \\
        0 &\leq (x, x)(y, y) - 2{(x, y)}^2 + {(x, y)}^2 \\
        0 &\leq (x, x)(y, y) - {(x, y)}^2 \\
        {(x, y)}^{2} &\leq (x, x)(y, y) \\
        |(x, y)| &\leq \sqrt{(x, x)} \cdot \sqrt{(y, y)}
    \end{align*}
\end{proof}
\section{Inner Products, Norms, and Metric Spaces}
\thm{}{Let $X$ be a vector space with an inner product $(\cdot, \cdot)$. Then $||x|| := \sqrt{(x, x)}$ is a norm.}
\begin{proof}
    We check the properties of norms:
    \begin{enumerate}
        \item $||x|| = 0 \iff \sqrt{(x, x)} = 0 \iff (x, x) = 0 \iff x = 0$.
        \item $||tx|| = \sqrt{(tx, tx)} = \sqrt{t^2(x, x)} = |t|\sqrt{(x, x)} = |t|||x||$.
        \item $||x + y||^2 = (x + y, x + y) = (x, x) + 2(x, y) + (y, y) = ||x||^2 + 2(x, y) + ||y||^2 \leq ||x||^2 + 2|(x, y)| + ||y||^2 \leq ||x||^2 + 2||x|| \cdot ||y|| + ||y||^2 = (||x|| + ||y||)^2$.
    \end{enumerate}
\end{proof}
\cor{Parallelogram Identity}{Let $X$ be a vector space with inner product $(\cdot, \cdot)$. Then for all $x, y \in X$, we have that \[||x + y||^2 + ||x - y||^2 = 2||x||^2 + 2||y||^2\]}
\begin{proof}
    \begin{align*}
        ||x + y||^2 + ||x - y||^2 &= (x + y, x + y) + (x - y, x - y) \\
        &= (x, x) + 2(x, y) + (y, y) + (x, x) - 2(x, y) + (y, y) \\
        &= 2(x, x) + 2(y, y) \\
        &= 2||x||^2 + 2||y||^2
    \end{align*}
\end{proof}
\noindent If we subtract them instead, we get \begin{align*}
    \frac{||x + y||^2 - ||x-y||^2}{4} = (x, y) \tag{*}
\end{align*}
So, if $||\cdot||$ is a norm, then if i want to define an inner product, I can use *.
\exer{}{Let $||\cdot||$ be a norm. Then $(x, y) := \frac{1}{4}(||x+y||^2 - ||x-y||^2)$ is an inner product iif the parallelogram identity holds.} 
\noindent Linearity of inner products is the hard part to prove because we have to consider:
\begin{itemize}
    \item $t \in \NN$
    \item $t = \dfrac{1}{2}$
    \item $t \in \QQ$
    \item $t \in \RR$ (density of $\QQ$)
\end{itemize}
\nt{For recitation: 
\begin{enumerate}
    \item $X = \{f : E \to \RR \text{ bounded}\}, ||f|| = \sup_E |f|$, does not satisfy the parallelogram identity.
    \item $x \in \RR^N$, $||x||_1 = |x_1| + |x_2| + \cdots + |x_N|$ does not satisfy the parallelogram identity.
\end{enumerate}}
\dfn{Metric}{Let $X$ be a set. A \emph{metric} on $X$ is a function $d : X \times X \to [0, \infty)$ such that:
\begin{enumerate}
    \item $d(x, y) = 0 \iff x = y$
    \item $d(x, y) = d(y, x)$ for all $x, y \in X$
    \item $d(x, z) \leq d(x, y) + d(y, z)$ for all $x, y, z \in X$
\end{enumerate}}
\dfn{Metric Space}{A set $X$ with a metric $d$ is called a \emph{metric space} and is denoted as $(X, d)$.}
\ex{Metrics}{Let $X$ be a set. Then the following is a metric on $X$:
\[ d(x ,y) = \begin{cases} 0 & x = y \\ 1 & x \neq y \end{cases} \]}
\thm{If $X$ is a vector space with $||\cdot||$ as a norm. Then \[d(x,y) := ||x -y||\] is a metric on $X$.}

\begin{proof}
    We check all the properties of metrics.
    \begin{itemize}
        \item $d(x, y) = 0 = ||x - y|| \Rightarrow 0 = x -y \iff x = y$.
        \item $d(x, y) = ||x - y|| = ||y - x|| = d(y, x)$.
        \item $d(x, y) = ||x - y|| = ||x - z + z - y|| \leq ||x - z|| + ||z - y|| = d(x, z) + d(z, y)$.
    \end{itemize}
\end{proof}
\newpage
\ex{}{Let's define \[d(x,y) = \left| \frac{x}{1+|x| } - \frac{y}{1 + |y|}\right| \] as a metric on $\RR$. However, this is not a norm because $d(tx, ty) \neq td(x, y)$.}

\dfn{Ball}{Let $(X, d)$ be a metric space. Let $x \in X$ and $r > 0$. Then the \emph{ball} of radius $r$ centered at $x$ is defined as $B_r(x) = \{y \in X : d(x, y) < r\}$.}

\ex{}{
\begin{itemize}
    \item Take $X = \RR^2$ with $(x, y) \in \RR$. Then defin $||(x, y)||_\infty = \max(|x|, |y|)$ is a norm. Take $B((0, 0), 1) = \{(x, y) \in \RR^2 : ||(x, y) - (0, 0)||_\infty < 1\}$. This is a square with vertices $(1, 1), (-1, 1), (-1, -1), (1, -1)$.
    \item If we have $||(x, y)||_1 = |x| + |y|$, then $B((0, 0), 1) = \{(x, y) \in \RR^2 : ||(x, y) - (0, 0)||_1 < 1\}$. This is a square with vertices $(1, 0), (0, 1), (-1, 0), (0, -1)$.
\end{itemize}}

\dfn{Interior}{Let $(X, d)$ be a metric space and $E \subseteq X$. $x \in E$ is called an \emph{interior point} of $E$ if there is $B(x, r) \subseteq E$. The set of all interior points of $E$ is called the \emph{interior} of $E$ and is denoted as $E^\circ$.}

\dfn{Open Set}{$E$ is \emph{open} if $E = E^\circ$.}


\section{Open Sets}
\ex{Balls}{$B(x , r)$ is open.
\begin{proof}
    Let $ y \in B(x, r)$ and take $B(y, r - d(x, y))$. Let $z \in B(y, r - d(x, y))$. Then $d(x, z) \leq d(x, y) + d(y, z) < d(x, y) + r - d(x, y) = r$. Thus, $z \in B(x, r)$. Thus, $B(y, r - d(x, y)) \subseteq B(x, r)$. Thus, $B(x, r)$ is open.
\end{proof}}

\ex{$\RR$}{
    \begin{enumerate}
        \item $E = (0, 1) \cap \QQ$ is not open. Because the irrationals are dense, we can always find a rational number in any ball. Thus, $E^\circ = \emptyset$.
        \item $E = (3, 4)$ is open. Let $x \in E$. Take $B(x, \min(x - 3, 4 - x))$. Then $B(x, \min(x - 3, 4 - x)) \subseteq E$. Thus, $E$ is open.
        \item $E = [3, 4)$ is not open. $E^\circ = (3, 4)$.
        \item $E = \{x \in \RR : x^3 - 3x + 4 > 0\}$. This is open and we'll be able to use continuity to prove this easily later.
        \item $l^\infty([0, 1]) = \{ f : [0, 1] \to \RR \text{ bounded}\}$. $||f||_\infty = \sup_{[0, 1]} |f|$. $d(f, g) = ||f - g||_\infty$. $E = \{f \in l^\infty([0, 1]) : f(x) > 0 \ \forall x \in [0, 1]\}$ is open? (finish in recitation)
    \end{enumerate}
}
\noindent Properties of open sets $(X, d)$:
\begin{itemize}
    \item $\emptyset$ is open. $X$ is open.
    \item Infinite intersections of open sets are not necessarily open. For example, we have $\bigcap_{n=1}^\infty (-1/n, 1/n) = \{0\}$, which is not open.
    \item Finite intersections of open sets are open. Consider $U_1, \ldots U_n$. Let $x \in \bigcap_{i=1}^n U_i$. Then $x \in U_i$ for all $i$. Since $U_i$ is open, there exists $r_i > 0$ such that $B(x, r_i) \subseteq U_i$. Let $r = \min(r_1, \dots, r_n)$. Then $B(x, r) \subseteq U_i$ for all $i$. Thus, $B(x, r) \subseteq \bigcap_{i=1}^n U_i$.
    \item Unions of open sets are open because if a point in the union is contained in one of the open sets, then there is a ball in that set that is contained in the union.
\end{itemize}

\dfn{Topological Space}{Let $X$ be a set. A \emph{topology} on $X$ is a collection $\mathcal{T}$ of subsets of $X$ such that:
\begin{enumerate}
    \item $\emptyset, X \in \mathcal{T}$.
    \item If $U_1, \ldots, U_n \in \mathcal{T}$, then $\bigcap_{i=1}^n U_i \in \mathcal{T}$. (finite intersections)
    \item If $U_\alpha \in \mathcal{T}$ for all $\alpha \in A$, then $\bigcup_{\alpha \in A} U_\alpha \in \mathcal{T}$. (arbitrary unions)
\end{enumerate}Elements of $\mathcal{T}$ are called open sets.}
\dfn{Closed}{Let $(X, d)$ be a metric space. We say $C \subseteq X$ is \emph{closed} if $X \setminus C$ is open.}
\noindent Note that $X$ and $\emptyset$ are both open and closed.
\ex{Open and Closed Sets}{\begin{itemize}
    \item $[0, 1)$ is not open or closed.
    \item $[0, 1]$ is closed.
\end{itemize}}
\noindent Properties of closed sets:
\begin{itemize}
    \item $\emptyset$ and $X$ are closed.
    \item Infinite intersections of closed sets are closed. (De Morgan's Law)
    \item Finite unions of closed sets are closed. For example, if we have $\bigcup_{m = 1}^\infty (-\infty, -\dfrac{1}{m}) = (-\infty, 0)$ which is closed.
\end{itemize}
\newpage
\section{2/1 - Rectitation}
Recall:
\begin{enumerate}
    \item Let $\{x_n\}$ be a sequence bounded above in $\RR$. Then $L \in \RR$ is the limit superior of $\{x_n\}$ if for every $\epsilon > 0$, there exists $n_\epsilon \in \NN$ such that:
    \begin{enumerate}
        \item $x_n < L + \epsilon$ for all $n \geq n_\epsilon$.
        \item $ x_n > L - \epsilon$ for infinitely many $n$.
    \end{enumerate}
    \item Let $\{x_n\}$ be a sequence bounded below in $\RR$. Then $L \in \RR$ is the limit inferior of $\{x_n\}$ if for every $\epsilon > 0$, there exists $n_\epsilon \in \NN$ such that:
    \begin{enumerate}
        \item $x_n < L + \epsilon$ for infinitely many $n$.
        \item $x_n > L - \epsilon$ for all $n \geq n_\epsilon$.
    \end{enumerate}
\end{enumerate}
Now consider the following sequence: \[ x_n = (-1)^n \frac{2n}{n+1} \in \RR\]
Prove that $\limsup_{n \to \infty} x_n = 2$.
\begin{proof}
    We need to show that for every $\epsilon > 0$, there exists $n_\epsilon \in \NN$ such that:
    \begin{enumerate}
        \item $x_n < 2 + \epsilon$ for all $n \geq n_\epsilon$.
        \item $2 - \epsilon < x_n$ for infinitely many $n$.
    \end{enumerate}
    Let $\epsilon > 0$. We need to find $n_\epsilon \in \NN$ such that $x_n < 2 + \epsilon$ for all $n \geq n_\epsilon$ and $2 - \epsilon < x_n$ for infinitely many $n$. We can find $n_\epsilon \in \NN$ such that $2 - \epsilon < x_n$ for all $n \geq n_\epsilon$. Then $x_n < 2 + \epsilon$ for all $n \geq n_\epsilon$. Thus, $\limsup_{n \to \infty} x_n = 2$.
\end{proof}

\noindent Now prove that for any $\{x_n\}$ in $\RR$, prove that $\liminf_{n \to \infty} x_n \leq \limsup_{n \to \infty} x_n$.
\begin{proof}
    Comes quickly from properties of limits and that the inf is less than the sup.
\end{proof}

\noindent Now prove that $\liminf_{n \to \infty} -x_n = -\limsup_{n \to \infty} x_n$ and that $\limsup_{n \to \infty} -x_n = -\liminf_{n \to \infty} x_n$.
\begin{proof}
    We start by using the property that $\inf(-E) = -\sup(E)$. Then we use the property that $\sup(-E) = -\inf(E)$.

    So, \begin{align*}
        \liminf_{n \to \infty} -x_n &= \sup_{n \in \NN}\inf_{k \geq n} -x_k \\
        &= \sup_{n \in \NN} -\sup_{k \geq n} x_k \\
        &= -\inf_{n \in \NN}\sup_{k \geq n} x_k \\
        &= -\limsup_{n \to \infty} x_n
    \end{align*}
\end{proof}
\newpage
\section{Closure}
\dfn{Closure}{Let $(X, d)$ be a metric space with $A \subset X$. Then the \emph{closure} of $A$ is defined as $\overline{A}$, the intersection of all sets that contain $E$.}
\dfn{Boundary Point}{Let $(X, d)$ be a metric space with $E \subseteq X$. Then $x \in X$ is a \emph{boundary point} of $E$ if for every $r > 0$, $B(x, r) \cap E \neq \emptyset$ and $B(x, r) \cap (X \setminus E) \neq \emptyset$. The set of all boundary points is denoted as $\partial E$.}
\thm{}{Let $(X, d)$ be a metric space and $E \subseteq X$. Then $\overline{E} = E \cup \partial E$.}
\begin{proof}
    Let $x \in \overline{E}$. FSOC, assume $x \notin E \cup \partial E$. Since $x \notin \partial E$, there exists $r > 0$ such that $B(x, r)$ that doesn't intersect with either $E$ or complement of $E$. But since $x\notin E$, only the second option can occur. So there exists $r$ such that $B(x, r) \cap E = \emptyset$. Because of that and the fact that $B(x, r)$ is open, it follows that $X \setminus B(x, r)$ is closed and contains $E$. By the definition of $\overline{E}$, we have that $\overline{E} \subseteq X \setminus B(x, r)$. But this is a contradiction because $x \in \overline{E}$. 

    Conversely, let $x \in E \cup \partial E$ and assume $x \notin \overline{E}$. Since $\overline{E}$ is closed, $X \setminus \overline{E}$ is open. Using the fact that $x \in E \cup \partial E$, we have that we can find a $B(x, r) \in X \setminus \overline{E}$. But this is a contradiction because $B(x, r)$ is open and contains $E$. Thus, $E \cup \partial E \subseteq \overline{E}$.
\end{proof}
\dfn{Accumulation Point}{Let $(X, d)$ be a metric space with $E \subseteq X$. Then $x \in X$ is an \emph{accumulation point} of $E$ if for every $r > 0$, there exists $y \in E$ such that $y \neq x$ and $d(x, y) < r$.}

\dfn{Interval}{$I \subseteq \RR$ is an \emph{interval} if we have that $z \in I$ for all $x < z < y$.}
\dfn{Rectangle}{$R \subseteq \RR^N$ is a \emph{rectangle} if $R = I_1 \times \cdots \times I_N$ where $I_1, \ldots, I_N$ are intervals in $\RR$.}
\dfn{Sequence}{Let $X$ be a set. A \emph{sequence} is a function $f : \NN \to X$. We denote $f(n)$ as $x_n$.}
\dfn{Convergent Sequence}{Let $(X, d)$ be a metric space. A sequence $\{x_n\}_{n=1}^\infty$ is \emph{convergent} if there exists $x \in X$ such that for every $\epsilon > 0$, there exists $n_\epsilon \in \NN$ such that $d(x, x_n) < \epsilon$ for all $n \geq n_\epsilon$. We write $x_n \to x$ as $n \to \infty$ or $\lim_{n \to \infty} x_n = x$.}
\newpage
\section{Bolzano-Weierstrass}
\thm{Bolzano-Weierstrauss}{If $E \subset \RR^N$ is bounded and contains infinitely many distinct points, then $E$ has an accumulation point}
\begin{proof}
    \mlenma{1}{If $[a_n, b_n] \supseteq [a_{n+1}, b_{n+1}]$ for all $n$, then $\bigcap_{n=1}^\infty [a_n, b_n] \neq \emptyset$.}
    \begin{proof}
        For all $a_n$ and $b_n$, we have:
        \begin{align*}
            a_1 &\leq a_2 \leq \cdots \\
            b_1 &\geq b_2 \geq \cdots
        \end{align*}
        Let \[A := \{a_1, a_2, \ldots \}.\] We have that $a_n \leq b_n \leq b_1$ for all $n$. So $A$ is bounded above, so by the supremum property, there exists $x = \sup A \in \RR$ and $a_n \leq x$ for all $n \in \NN$. We claim that $x \leq b_n$ as well. If not, then there exists $m \in \NN$ such that $b_m < x$. Since $x$ is an upper bound of $A$, we'll have that there's an $n \in \NN$ such that $b_m < a_n \leq x$. Find $k \geq m, n$, then we have $b_m < a_n \leq a_k \leq b_k \leq b_m$, which is a contradiction. This proves the claim. Hence, $x \in [a_n, b_n]$ for all $n$. Thus, $x \in \bigcap_{n=1}^\infty [a_n, b_n]$.
    \end{proof}
    \mlenma{2}{Let $R_n$ be a closed and bounded rectangle. Assume that $R_1 \supseteq R_2 \supseteq \cdots$. Then $\bigcap_{n=1}^\infty R_n \neq \emptyset$.}
    \begin{proof}
        We know that \begin{align*}
            R_n &= [a_{1, n}, b_{1, n}] \times \cdots \times [a_{N, n}, b_{N, n}] \\
            R_{n+1} &= [a_{1, n+1}, b_{1, n+1}] \times \cdots \times [a_{N, n+1}, b_{N, n+1}]
        \end{align*}
        We can apply lemma 1 $N$ times (for each of the components of $R_n$) to find that $x_1, x_2, \ldots, x_N \in \RR$ such that $a_{i, n} \leq x_i \leq b_{i, n}$ for all $1 \leq i \leq N$. Then, if you take $x = (x_1, \ldots, x_N)$, then $x \in R_m$ for all $n$. Thus, $x \in \bigcap_{n=1}^\infty R_n$. 
    \end{proof}
    \mlenma{3}{Let $(X, d)$ be a metric space with $E \subseteq X$. Then $x \in X$ is an accumulation point of $E$ if and only if there exists a sequence $\{x_n\}_{n=1}^\infty$ in $E$ such that $x_n \to x$ as $n \to \infty$.}
    \begin{proof}
        Let $x \in X$ be an accumulation point of $E$. Take $r = \dfrac{1}{n}$. Find $x_n \in B\left(x, \dfrac{1}{n}\right) \cap E$ with $x_n \neq x$. We claim $x_n \to x$. Given $\epsilon >0$, find $n_\epsilon \geq \dfrac{1}{\epsilon}$. Then $d(x, x_n) < \dfrac{1}{n} \leq \dfrac{1}{n_\epsilon}$ for all $n \geq n_\epsilon$. Thus, $x_n \to x$ as $n \to \infty$.

        Let $\{x_n\}_{n=1}^\infty$ be a sequence in $E$ such that $x_n \to x$ as $n \to \infty$. We claim that $x \in \text{acc}(E)$. Let $r >0$ and take $\epsilon = r$. Then there exists $n_\epsilon \in \NN$ such that $d(x, x_n) < \epsilon = r$ for all $n \geq n_\epsilon$. Thus, $x_n \in B(x, r) \cap E$ for all $n \geq n_\epsilon$. Thus, $x \in \text{acc}(E)$.
    \end{proof}
    Now we prove the actual theorem. Let $E \subseteq \RR^N$ be bounded. $E \subseteq B(0, r)$ for some $r$. Let $Q_1$ be the closed cube centered at $0$ with sidelength $2r$. Pick some point $x_1 \in E \subseteq Q_1$. Subdivide $Q_1$ into $2^N$ closed cubes of sidelength $\dfrac{2r}{2}$. Let $Q_2$ be the closed cube containing $x_1$. Pick some point $x_2 \in E \cap Q_2$ with $x_2 \neq x_1$. Inductively, assume $Q_1 \supseteq Q_2 \supseteq \cdots \supseteq Q_n$ have been chosen. Then $Q_{n}$ is a closed cube of sidelength $\dfrac{2r}{2^{n-1}}$ containing $x_n$. Each $Q_n$ contains infinitely many elements of $E$. Assume also that $x_1, x_2, \cdots x_n \in E$ have been chosen with $x_i \in Q_i$ and $x_i \neq x_j$ for $i \neq j$. 

    Now we can subdivide $Q_n$ to get $Q_{n+1}$ and continue this process infinitely.

    By Lemma 2, we know that $\bigcap_{n=1}^\infty Q_n \neq \emptyset$. Let $x \in \bigcap_{n=1}^\infty Q_n$. Now we need to show there exists a sequence $\{x_n\}_{n=1}^\infty$ in $E$ such that $x_n \to x$ as $n \to \infty$ but $x_i \neq x$ for any $i$ because then the rest of the points won't converge to $x$. If $x = x_i$ for some $i$, we can just pick another point.

    So WLOG, assume $x_n \neq x$ for any $n$. So we claim $x_n \to x$ as $n \to \infty$. We know that in $Q_n$, the difference between any two points in this cube is given by:
    \begin{align*}
        ||x_n - x|| = \sqrt{(x_{n, 1} - x_1)^2 + (x_{n, 2} - x_2)^2 + \cdots + (x_{n, N} - x_N)^2} \leq \sqrt{\frac{2r}{2^{n-1}} + \frac{2r}{2^{n-1}} + \cdots + \frac{2r}{2^{n-1}}} = \sqrt{N}\frac{2r}{2^{n-1}}
    \end{align*}
    This value is less than $\epsilon$ for all large $n$, so this concludes the proof.
\end{proof}
\section{2/6 - Recitation - Spaces}
Let $X = \{ f : [0, 1] \to \RR \text{ bounded}\}$. Define $\Vert f \Vert = \sup_{x \in [0, 1]} |f(x)|$. Prove that $(X, \Vert \cdot \Vert)$ does not suffice parallelogram identity. That is, show a counterexample to the parallelogram identity, which is \[ \Vert f + g \Vert^2 + \Vert f - g \Vert^2 = 2\Vert f \Vert^2 + 2\Vert g \Vert^2\]
\begin{proof}
    Counterexample: Let $f(x) = x$ and $g(x) = 1$. 
\end{proof}
Now given a normed space which satisifies the parallelogram identity, can we define an inner product?
\begin{proof}
    Yes. We can define $(f, g) = \frac{1}{4}(\Vert f + g \Vert^2 - \Vert f - g \Vert^2)$. We can prove that this is an inner product.

    Linearity of products because the other properties are easy to prove. We need to show that $(x + y, z) = (x, z) + (y, z)$. I'm so lazy so I won't tbh.

    We now show that $(tx, y) = t(x, y) \forall t \in \ZZ$. We proceed with induction for $t \in \ZZ^+$
    
    Our two base cases are $t = 0, 1$. For $t = 0$, we have that $(0x, y) = (0, y) = 0 = 0(0, y)$. For $t = 1$, we have that $(x, y) = (x, y) = 1(x, y)$.

    Now we assume that $(tx, y) = t(x, y)$ for some $t \in \ZZ^+$. Then we have that $(t+1)x = tx + x$. Then we have that $(t+1)x, y = (tx + x, y) = (tx, y) + (x, y) = t(x, y) + (x, y) = (t+1)(x, y)$. Thus, we have that $(tx, y) = t(x, y)$ for all $t \in \ZZ^+$.

    Now we have to deal with $t \in \ZZ^-$. We have that $(tx, y) = -t(-x, y) = -t(x, y) = t(x, y)$. Thus, we have that $(tx, y) = t(x, y)$ for all $t \in \ZZ$.

    To proceed, we deal with $t \in \QQ$. We have that $t = \frac{m}{n}$ for some $m, n \in \ZZ$. Then we have that $n(tx, y) = (ntx, y) = (mx, y) = m(x, y) = t(mx, y) = t(n(x, y))$. Thus, we have that $n(tx, y) = t(n(x, y))$. Thus, we have that $(tx, y) = t(x, y)$ for all $t \in \QQ$.
\end{proof}


\section{Compactness}
\dfn{Subsequence}{Let $X$ be a set and $f : \NN \to X$ a sequence. Let $g: \NN \to \NN$ be strictly increasing. Then $f \circ g : \NN \to X$ is a \emph{subsequence} of $f$. We denote $m_k$ as $g(k)$, so $f(g(k)) = f(m_k) = x_{m_k}$. So we denote the whole sequence as $\{x_{m_k}\}_{k}$.}


\dfn{Sequentially Compact}{Let $(X, d)$ be a metric space. $K \subseteq X$ is \emph{sequentially compact} if every sequence $\{x_n\}_n$ in $K$ and there exists a subsequence $\{x_{n_k}\}_k$ such that $x_{n_k} \to x$ as $k \to \infty$ for some $x \in K$.}

\ex{$\RR$}{
    \begin{enumerate}
        \item $(0, 1]$ is not sequentially compact. Consider the sequence $x_n = \dfrac{1}{n}$. This sequence has no convergent subsequence that tends to $0$ since $0$ is not in the set. The issue is that it's not closed.
        \item $[0, \infty)$ is not sequentially compact. Consider the sequence $x_n = n$. This sequence has no convergent subsequence that tends to $\infty$ since $\infty$ is not in the set. So, $[0, \infty)$ is not sequentially compact. The issue is that it's not bounded.
    \end{enumerate}
}
\newpage
\thm{}{Let $(X, d)$ be a metric space. If $K \subseteq X$ is sequentially compact, then $K$ is closed and bounded. }
\begin{proof}
    Claim: $K$ is closed. We want $X \setminus K$ to be open. Let $x \in X \setminus K$. We want $B(x, r) \in X \setminus K$ for some $r > 0$. By contradiction, for all $r > 0$, assume $\exists y \in B(x, r) \cap K$. Take $r = \dfrac{1}{m} \Rightarrow y_m \in B(x, \dfrac{1}{m}) \cap K$. $d(y_m, x) < \dfrac{1}{m} \to 0$, so $y_m \to x$. But $x \notin K$ even though $y_m \in K$. This is a contradiction, so $K$ is closed.

    Claim: $K$ is bounded. By contradiction, assume $K$ is not bounded. Let $x_0 \in X$. Then $K \not\subseteq B(x_0, r)$ for any $r > 0$. Take $r = n$. Then $\exists x_n \in K$ such that $d(x_n, x_0) \geq n$. So $\{x_n\}_n \in K$. $K$ is sequentially compact, so there exists a subsequence $\{x_{n_k}\}_k$ such that $x_{n_k} \to x$ as $k \to \infty$ for some $x \in K$. But $n_k \leq d(x_{n_k}, x_0) \leq d(x_{n_k}, x) + d(x, x_0)$. But $d(x_{n_k}, x) \to 0$ as $k \to \infty$, so $n_k \to \infty < d(x_{m_k}, x_0) \leq d(x, x_0)$ which is a fixed number, so we have a contradiction. As such, $K$ is bounded.
\end{proof}
\thm{}{Let $K \subseteq \RR^N$. Then $K$ is sequentially compact if and only if $K$ is closed and bounded.}
\begin{proof}
    We just showed the first direction. So, we need to show that if $K$ is closed and bounded, then $K$ is sequentially compact. 

    So now, assume $K$ is closed and bounded. Let $\{x_n\}_n$ be a sequence in $K$. We want to show that there exists a subsequence $\{x_{n_k}\}_k$ such that $x_{n_k} \to x$ as $k \to \infty$ for some $x \in K$. 

    Consider the set $E = \{ x_n : n \in \NN\} \subseteq \RR_N$. We now case on whether $E$ has infinitely many distinct points or not. 
    
    If $E$ doesn't have infinitely many distinct points, there exists $x  \in K$ such that $x_n = x$ for einfinitely many $n$. Then $x_{n_k} = x$ for all $k$, so $x_{n_k} \to x$ as $k \to \infty$.

    Now we consider the case where Bolzano-Weierstrass applies. By B-W, $E$ has an accumulation point $x \in \RR^N$. So we can find a subsequence $\{x_{n_k}\}_k$ such that $x_{n_k} \to x$ as $k \to \infty$. But $x \in K$ because $K$ is closed. Thus, $K$ is sequentially compact.
\end{proof}
\nt{Let $(X, \Vert \cdot \Vert)$ be a normed space. If every closed and bounded set is sequentially compact, then $X$ has finite dimension.}

\exer{}{Recall $l^\infty([0, 1]) = \{ f : [0, 1] \to \RR \text{ bounded}\}$. Define $\Vert f \Vert_\infty = \sup_{x \in [0, 1]} |f(x)|$. $B(0, 1) = \{g \in l^\infty([0, 1]) : \Vert g \Vert_\infty < 1\}$. Prove that $B(0, 1) = \{g \in l^\infty([0, 1]) : |g(x)| < 1 \ \forall x \in [0, 1]\}$. Also prove that this not sequentially compact.}
\newpage
\section{2/8 - Recitation}
Let $n \in \NN$, $x,y\in \RR$. \begin{enumerate}
    \item Prove that $x^n - y^n = (x-y)(x^{n-1} + x^{n-2}y + \cdots + xy^{n-2} + y^{n-1})$.
    \begin{proof}
        Base case: $n=1$ is trivial.

        Now assume that for any $n \in \NN$, $x^n - y^n = (x-y)(x^{n-1} + x^{n-2}y + \cdots + xy^{n-2} + y^{n-1})$. We want to show that this is true for $n+1$. We have that $x^{n+1} - y^{n+1} = x(x^n - y^n) + y^n(x - y) = x(x-y)(x^{n-1} + x^{n-2}y + \cdots + xy^{n-2} + y^{n-1}) + y^n(x-y)$. Then we get $(x-y)(x^{n} + x^{n-1}y + \cdots + xy^{n-1} + y^{n}) = (x-y)(x^{n} + x^{n-1}y + \cdots + xy^{n-1} + y^{n})$.
    \end{proof}
    \item Prove that when $|x-y| \leq 1$, then $|x^n - y^n| \leq n(1 + |x|)^{n-1}|x-y|$.
    \begin{proof}
       Let $|x-y| \leq 1$. Then we have that $|x^n - y^n| = |(x-y)(x^{n-1} + x^{n-2}y + \cdots + xy^{n-2} + y^{n-1})| \leq |x-y|(|x^{n-1}| + |x^{n-2}y| + \cdots + |xy^{n-2}| + |y^{n-1}|) \leq |x-y|(|x^{n-1}| + |x^{n-2}||y| + \cdots + |x||y^{n-2}| + |y^{n-1}|) \leq |x-y|(|x^{n-1}| + |x^{n-2}||y| + \cdots + |x||y^{n-2}| + |y^{n-1}|) \leq |x-y|(|x^{n-1}| + |x^{n-2}| + \cdots + |x| + 1) \leq n(1 + |x|)^{n-1}|x-y|$.
    \end{proof}
    \item Let $E = \{x \in \RR : x^n > 3\}$ for a fixed $n$. Prove that $E$ is open.
    \begin{proof}
        Let $x \in E$. We want to show that there is an $r>0$ such that $B(x, r) \subseteq E$. Take $r = \dfrac{x^n - 3}{n(1 + |x|)^{n-1}}$ and take $y \in B(x, r)$. Then $|x-y| < r \Rightarrow |x^n| - |y^n| \leq |x^n - y^n| \leq n(1 + |x|)^{n-1}|x-y| < n(1 + |x|)^{n-1}r < x^n - 3$. Then $y^n \geq x^n - n(1 + |x|)^{n-1}r > 3$. Thus, $y \in E$. Thus, $B(x, r) \subseteq E$. Thus, $E$ is open.
    \end{proof}
    \item Consider the space $l^\infty([0, 1]) = \{ f : [0, 1] \to \RR \text{ bounded}\}$. Define $\Vert f \Vert_\infty = \sup_{x \in [0, 1]} |f(x)|$. 
    
    Let $E = \{f \in l^\infty([0, 1]) : f(x) > 0 \ \forall x \in [0, 1]\}$. Prove that $E$ is not open.
    \begin{proof}
        Consider \[f(x) = \begin{cases} x & x \in [0, 1) \\ 1 & x = 1 \end{cases}\] Then let $r>0$ and consider $g(x) = f(x) \cdot \dfrac{r}{2}$. Then $g(x) \in B(f, r)$. But $g(x) \notin E$ because $g(1) = \dfrac{r}{2}$. Thus, $B(f, r) \not\subseteq E$. Thus, $E$ is not open.
    \end{proof}
\end{enumerate}

\section{Limits}
\dfn{Limits}{Let $(X, d_X)$ and $(Y, d_Y)$ be metric spaces, $E \subseteq X$, $f:E \to Y$. Let $x_0 \in \operatorname{acc}E$. 

Take $l \in Y$. $l$ is the \emph{limit} of $f$ as $x \to x_0$. We write $\lim_{x \to x_0} f(x) = l$ if for every $\epsilon > 0$, there exists $\delta > 0$ such that $0 < d_X(x, x_0) < \delta \Rightarrow d_Y(f(x), l) < \epsilon$. We can also write it as $f(x) \to l$ as $x \to x_0$.}
\nt{Even if $x_0 \in E$, you don't take in the definition for the limit.}

\thm{}{Let $(X, d_X)$ and $(Y, d_Y)$ be metric spaces, $E \subseteq X$, $f:E \to Y$, and $x_0 \in \operatorname{acc}E$. If $\lim_{x \to x_0} f(x)$ exists, then it is unique.}
\begin{proof}
    Assume that $\lim_{x \to x_0} f(x) = l$ and $\lim_{x \to x_0} f(x) = m$. Take $\epsilon = \frac{d_Y(l, m)}{2} > 0$. Then there exists $\delta_1 > 0$ such that $0 < d_X(x, x_0) < \delta_1 \Rightarrow d_Y(f(x), l) < \epsilon$. There also exists $\delta_2 > 0$ such that $0 < d_X(x, x_0) < \delta_2 \Rightarrow d_Y(f(x), m) < \epsilon$. Take $\delta = \min(\delta_1, \delta_2)$. Then $0 < d_X(x, x_0) < \delta \Rightarrow d_Y(f(x), l) < \epsilon$ and $d_Y(f(x), m) < \epsilon$. Then $d_Y(l, m) \leq d_Y(l, f(x)) + d_Y(f(x), m) < 2\epsilon = d_Y(l, m)$. This is a contradiction, so $l = m$.
\end{proof}
\ex{$\RR^2$}{Take $(x_0, y_0) \in \RR^2$ and $y_0 \neq 0$. Compute \[\lim_{(x, y) \to (x_0, y_0)} \dfrac{x}{y}\]
We want to show that this is $\dfrac{x_0}{y_0}$. We have the set $E = \{(x, y) \in \RR^2 : y \neq 0 \}$. We also know that $(x_0, y_0) \in \operatorname{acc}E$. What we know if that $(x, y) \to (x_0, y_0)$: $|x-x_0|$ and $|y-y_0|$ are going to be small. Then 
\begin{align*}
    \left| f(x, y) - \frac{x_0}{y_0}\right| &= \left| \frac{x}{y} - \frac{x_0}{y_0} \right| \\
    &= \left| \frac{xy_0 - x_0y_0}{yy_0} \right| \\
    &= \left| \frac{xy_0 - x_0y_0 + x_0y_0 - x_0y}{yy_0} \right| \\
    &= \left| \frac{y_0(x-x_0) + x_0(y_0 - y)}{yy_0} \right| \\
    &\leq \frac{|y_0| |x-x_0| + |x_0| |y_0 - y|}{|y| |y_0|} \\
    &= \frac{|x-x_0|}{|y|} + \frac{|x_0| |y_0 - y|}{|y| |y_0|}
\end{align*} 
Then we have $\delta < \frac{|y_0|}{2}$. If $|y-y_0| < \delta < \frac{y_0}{2}$, then we get $|y| \geq \frac{|y_0|}{2} \Rightarrow \frac{1}{|y|} \leq \frac{2}{|y_0|}$. 
\begin{align*}
    \frac{|x-x_0|}{|y|} + \frac{|x_0| |y_0 - y|}{|y| |y_0|} &\leq \frac{2|x-x_0|}{|y_0|} + \frac{2|x_0| |y_0 - y|}{|y_0|^2}
\end{align*}
Take $\delta = \min\left\{\epsilon, \frac{|y_0|}{2} \right\} > 0$. Then $0 <  \Vert(x, y) - (x_0, y_0) \Vert < \delta$. 
\begin{align*}
    |x-x_0| &= \sqrt{(x-x_0)^2} \leq \sqrt{(x-x_0)^2 + (y-y_0)^2} \\
    |y-y_0| &\leq \delta
\end{align*}
So, 
\begin{align*}
    \left|f(x, y) - \frac{x_0}{y_0} \right| < \epsilon\left(\frac{2}{|y_0} + \frac{2}{|y_0|^2} \right)
\end{align*}
}
Say you can prove that for every $\epsilon > 0$, $\exists \delta > 0 $ such that \begin{align*}
    d(f(x), l ) < \epsilon |\log(\epsilon)| \text{ for all } x \in E \text{ such that } 0 < d(x, x_0) < \delta
\end{align*}
For every $\eta > 0$ (``my epsilon''), since $\lim_{\epsilon \to 0 ^+} \epsilon | \log(\epsilon)| = 0$, $\exists \delta_1 > 0 $ such that $\epsilon | \log(\epsilon) | < \eta $ for all $0 < \epsilon < \delta_1$.

So given $\eta > 0$, take $ 0 < \epsilon < \delta_1$. Find $\eta$ from $d(f(x), l) < \epsilon | \log(\epsilon) | < \eta$ for all $x \in E$ such that $0 < d(x, x_0) < \delta$. This means that
\begin{align*}
    d_Y(f(x), l) < \epsilon | \log(\epsilon) | < \eta
\end{align*}
for all $x \in E$, $0 < d(x, x_0) < \delta$. Thus, $\lim_{x \to x_0} f(x) = l$.
\newpage
\section{Limits Continued}
\dfn{Restriction}{Assume that $\lim_{x\to x_0}f(x) = l$ exists. Let $F \subseteq E$ such that $x_0 \in  \operatorname{acc}F$. The function $f: F \to Y$ is called the \emph{restriction} of $f$ to $F$. It is denoted as $f|_F$.}
\nt{If $\lim_{x\to x_0}f(x) = l$, then $\lim_{x\to x_0}f|_F(x) = l$.}
\noindent So to prove that the limit does not exist, you can conjure up two restrictions of the function and show that the limits are different.
\ex{Limits that don't exist}{
    Consider $\lim_{x\to 0} \sin\left(\frac{1}{x}\right)$. We can find two restrictions of the function and show that the limits are different.
    \begin{itemize}
        \item $\frac{1}{x} = 2\pi n + \frac{\pi}{2}$
        \item $x_n = \frac{1}{2\pi n + \frac{\pi}{2}}$
    \end{itemize}
    So we have:
    \begin{itemize}
        \item $\sin x_n = \sin\left(\frac{\pi}{2} + 2\pi n\right) = 1$
        \item $\sin x_n = \sin\left(\frac{1}{2\pi n}\right) =0$
    \end{itemize}
    Thus, the limit does not exist.
}
\exer{TODO in Recitation}{\begin{itemize}
    \item $\lim_{(x, y) \to (0, 0)} \frac{xy}{x^2 + y^2}$ (no)
    \item $\lim_{(x, y) \to (0, 0)} \frac{x^2y}{x^2 + y^2}$ (yes, 0)
    \item $\lim_{(x, y) \to (0, 0)} \frac{x^{10000000000}y}{y - \sin(x)}$ (no)
\end{itemize}}
\noindent Now we talk about the composition of limits. 
\ex{}{Consider \[g(y) = \begin{cases} 1 & y \neq 0 \\ 2 & y = 0 \end{cases}.\] The limit of $g(y)$ as $y \to 0$ is $1$. Now consider $f(x) = 0$. The limit of $f(x)$ as $x \to x_0$ is $0$. Now consider $g(f(x))$. The limit of $g(f(x))$ as $x \to x_0$ is $2$.}
\thm{Composition of Limits}{Let $(X, d_X)$, $(Y, d_Y)$, and $(Z, d_Z)$ be metric spaces, $E \subseteq X$, $F \subseteq Y$, $f:E \to F$, $g:F \to Z$, and $x_0 \in \operatorname{acc}E$. Assume there exists $\lim_{x \to x_0} f(x) = l \in Y$. Assume $l \in \operatorname{acc}F$ and that there is $\lim_{y \to l} g(y) = L \in Z$. Assume that either $f(x) \neq l$ for all $x \in E$ or $ l\in F$ and $g(l) = L$. Then there is $\lim_{x \to x_0}g(f(x)) = L$.}
\begin{proof}
    Since $\lim_{y \to l} g(y) = L$, there for every $\epsilon >0$, there exists $\delta > 0$ such that $d_Z(g(y), L) < \epsilon$ for all $y \in F$ with $0 < d_Y(y, l) < \delta$. We would like to take $y = f(x)$. Use $\delta$ as ``my epsilon'' for the definition of the limit of $f(x)$. Then to find $\eta > 0$ such that $d_X(f(x), l) < \delta$ for all $x \in E$ with $0 < d_X(x, x_0) < \eta$. Now we split into cases:
    \begin{itemize}
        \item Assume $f(x) \neq l$ for all $x \in E$. Then $0 < d_Y(f(x), l)$ so we can take $y = f(x)$ to get $d_z(g(f(x)), L) < \epsilon$  for all $x \in E$ with $0 < d_X(x, x_0) < \eta$. This means that there exists $\lim_{x \to x_0} g(f(x)) = L$.
        \item Assume $l \in F$ and $g(l) = L$. If $f(x) = l$, then $d_Z(g(f(x)), L) = d_Z(g(l), L) = 0$ for all $x \in E$ with $0 < d_X(x, x_0) < \eta$. If $f(x) \neq l$, then take $y = f(x)$ to get $0 < d_Y(f(x), l)$ so we can take $y = f(x)$ to get $d_z(g(f(x)), L) < \epsilon$  for all $x \in E$ with $0 < d_X(x, x_0) < \eta$. This means that there exists $\lim_{x \to x_0} g(f(x)) = L$.
        This means that there exists $\lim_{x \to x_0} g(f(x)) = L$.
    \end{itemize}
\end{proof}
\cor{Limits of the Sum/Products/Quotients}{Let $(X, d)$ be a metric space and $E \in X$. Then take $f: E \to \RR$ and $g: E \to \RR$. Let $x_0 \in X$ and $x_0 \in \operatorname{acc}E$. Assume $\lim_{x \to x_0} f(x) = l$ and $\lim_{x \to x_0} g(x) = m$. Then we have the following results:
\begin{itemize}
    \item $\lim_{x \to x_0} (f+g)(x) = l + m$.
    \item $\lim_{x \to x_0} (f\cdot g)(x) = l \cdot m$.
    \item $\lim_{x \to x_0} \frac{f}{g}(x) = \frac{l}{m}$.
\end{itemize}}
\begin{proof}
    We can use the composition of limits to prove this. We'll just proceed with the quotient case. Consider $x \to (f(x), g(x))$. Then consider the function that takes $(s, t) \to \frac{s}{t}$ and call it $h$. THen we have $\frac{f(x)}{g(x)} = h(f(x), g(x))$. We then have $\lim_{x \to x_0} (f(x), g(x)) = (l, m)$ and $\lim_{(s, t) \to (l, m)} h(s, t) = \frac{l}{m} = h(l, m)$. So now we can use the composition of limits to get $\lim_{x \to x_0} \frac{f}{g}(x) = \frac{l}{m}$.

    The other two cases are similar. For products, you need to show that the limit as $(x,y) \to (x_0, y_0)$ of $xy$ is $x_0y_0$ and similarly for sum.
\end{proof}

\section{Squeeze Theorem}
\thm{Squeeze Theorem}{Let $(X, d_X)$ be a metric space, $E \subseteq X$, $f:E \to \RR$, $g:E \to \RR$, and $h:E \to \RR$. Let $x_0 \in \operatorname{acc}E$ and have $f \leq g \leq h$. Assume that $\lim_{x \to x_0}f(x) = l = \lim_{x \to x_0} h(x)$. Then $\lim_{x \to x_0} g(x) = l$.}
\begin{proof}
    Assume $\lim_{x \to x_0}f(x) = l = \lim_{x \to x_0} h(x)$. Then for every $\epsilon > 0$, there exists $\delta_1 > 0$ such that $0 < d_X(x, x_0) < \delta_1 \Rightarrow |f(x) - l| < \epsilon$ and $0 < d_X(x, x_0) < \delta_2 \Rightarrow |h(x) - l| < \epsilon$. Take $\delta = \min (\delta_1, \delta_2)$ and $x \in E$ with $ 0 < d(x, x_0) < \delta$. Then $l - \epsilon < f(x) < g(x) < h(x) < l + \epsilon$. Then $|g(x) - l| < \epsilon$ for all $x \in E$ with $0 < d_X(x, x_0) < \delta$. Thus, $\lim_{x \to x_0} g(x) = l$.
\end{proof}
\ex{}{\[\lim_{x \to 0} |x|^a \sin \frac{1}{x} = 0\] for all $a > 0$.

Let $Q> 0$. Then $O \leq | |x|^Q \sin\frac{1}{x} | \leq |x|^Q$. Since both sides tend to $0$ as $x \to 0$, then the middle does as well.}

\dfn{Increasing}{$f: E \to \RR$ is \emph{increasing} if $f(x) \leq f(y)$ for all $x \leq y$. It is strictly increasing if $f(x) < f(y)$ for all $x < y$.}
\dfn{Decreasing}{$f: E \to \RR$ is \emph{decreasing} if $f(x) \geq f(y)$ for all $x \leq y$. It is strictly decreasing if $f(x) > f(y)$ for all $x < y$.}
\dfn{Divergent}{Let $(X, d_x)$ be a metric space with $E \subseteq X$, $x_0 \in \operatorname{acc}E$, and $f: E \to \RR$. We say that $f$ diverges to $+\infty$ as $x \to x_0$ if for every $M > 0\in \RR$, there exists $\delta > 0$ such that $f(x) > M$ for all $x \in E$ with $0 < d_X(x, x_0) < \delta$. We say that $f$ diverges to $-\infty$ as $x \to x_0$ if for every $M < 0 \in \RR$, there exists $\delta > 0$ such that $f(x) < M$ for all $x \in E$ with $0 < d_X(x, x_0) < \delta$.}
\thm{}{Let $E \subseteq \RR$ and $f : E \to \RR$ be increasing. Let $x_0 \in \RR$. Assume $x_0$ is an accumulation point of $E \cap (-\infty, x_0)$. Then there is \[\lim_{{x\to x_0}^-} f(x) = \sup_{E \cap (-\infty, x_0)} f(x) \]
Now if $x_0$ is an accumulation point of $E \cap (x_0, \infty)$, then there is \[\lim_{{x\to x_0}^+} f(x) = \inf_{E \cap (x_0, \infty)} f(x) \]}
\begin{proof} $\\$
    \begin{itemize}
        \item Case 1: Assume $f$ is bounded form above on $E \cap (-\infty, x_0)$. Let $l = \sup_{E \cap (-\infty, x_0)} f(x)$. Then for every $\epsilon > 0$, there exists $x_1 \in E \cap (-\infty, x_0)$ such that $l - \epsilon < f(x_1) \leq l$. Then for every $\epsilon > 0$, there exists $\delta > 0$ such that $l - \epsilon < f(x_1) \leq l$. Take $\delta = x_0 - x_1 > 0$. Let $x \in E$ with $x_0 - \delta < x < x_0$. Since $f$ is increasing, we have $l - \epsilon < f(x_1) \leq f(x) \leq l < l + \epsilon$. Thus, $\lim_{{x\to x_0}^-} f(x) = l$.
        \item Case 2: If $f$ is not bounded from above, then for every $M > 0$, there exists $x_1 \in E \cap (-\infty, x_0)$ such that $f(x_1) > M$. Let $\delta = x_0 - x_1 > 0$. Then for every $x \in E$ with $x_1 = x_0 - \delta < x < x_0$, we have $f(x) \geq f(x_1) > M$. Thus, $\lim_{{x\to x_0}^-} f(x) = +\infty$.
    \end{itemize}
    The other case is similar.
\end{proof}

\dfn{Infinite Sum}{Let $X$ be a set and take $f: X \to [0, \infty]$. The \emph{infinite sum} is defined as: \[ \sum_{x \in X} f(x) = \sup \left\{ \sum_{x \in F} f(x) : F \subseteq X \text{ finite} \right\}.\]}
\mlenma{}{Let $X$ be nonempty with $f : X \to [0, \infty]$. Assume that $\sum_{x \in X} f(x) < \infty$. Then $\{x \in X : f(x) > 0\}$ is countable.}
\begin{proof}
    Take $ n\in \NN$ and define $X_n = \{x \in X : f(x) \geq \frac{1}{n}\}$. Let $E \subseteq X_n$ be finite. Then $\frac{1}{n} |E| < \sum_{x \in E} f(x) \leq M$. Then $|E| < nM$. Thus, $X_n$ is countable. Then $\bigcup_{n \in \NN} X_n  = \{x \in X : f(x) > 0\}$ is countable.
\end{proof}
\section{2/15 -  LIMIT  !!!!!!!!!}

\ex{}{\begin{itemize}
    \item Let $f(x) = \frac{xy}{x^2y^2}$. Find $\lim_{(x, y) \to (0, 0)} f(x, y)$. If we take the restriction $y = mx$, we see that the limit depends on $m$ which is a contradiction.
    \item Let $f(x, y) = \frac{x^2y}{x^2 + y^2}$. Find $\lim_{(x, y) \to (0, 0)} f(x, y)$. We can use polar coordinates to show that this is $0$. 
\end{itemize}}

\section{This Theorem}
\thm{}{Take $I \subseteq \RR$ to be an interval with $f: I \to \RR$ increasing. Then for all but countably many $x_0 \in I$, there is $\lim_{{x\to x_0}^-} f(x) =\lim_{{x\to x_0}^+} f(x) = f(x_0)$. }
\begin{proof}
    Let $I = [a,b]$. For every $x \in (a, b)$, there exists \[\lim_{y \to x^+} =: f_+(x), \quad \lim_{y \to x^-} =: f_-(x).\]
    Let $S(x) = f_+(x) - f_-(x) \geq 0$, which is the jump of $f$ at $x$. Then we have that $\lim_{y \to x} f(y) = f(x) \iff S(x) = 0$. Let $J \in [a, b]$ be any finite subset, and write \[J = \{x_1, \ldots, x_k\}, \quad \text{where} x_1 < 
    \cdots < x_k.\]
    Since $f$ is increasing, we have that 
    \begin{align*}
        f(a) \leq f_-(x_1) \leq f_+(x_1) \leq f_-(x_2) \leq f_+(x_2) \leq \cdots \leq f_-(x_k) \leq f_+(x_k) \leq f(b).
    \end{align*}
    So, 
    \begin{align*}
        \sum_{x \in J} S(x) = \sum_{x \in J} f_+(x) - f_-(x) \leq f(b) - f(a),
    \end{align*}
    which implies that $\sum_{x \in (a, b)} S(x) \leq f(b) - f(a)$. It follows that the amount of discontinuities is countable.
\end{proof}
\chapter{}
\dfn{Series}{Given a normed space $X$ and a sequence $\{x_n\}_n$, of vectors in $X$, we call the $n$th-partial sum the vector $s_n = \sum_{k=1}^n x_k$. The sequence $\{s_n\}_n$ of partial sums is called infinite series or \emph{series} and is denoted $\sum_{n=1}^\infty x_n$.

If there exists $\lim_{n \to \infty} s_n = s \in X$, then we say that the series $\sum_{n=1}^\infty x_n$ \emph{converges} to $s$ and $s$ is called the \emph{sum} of the series. If the limit does not converge, we say the series \emph{oscillates}.}

\section{More Series}
\thm{}{Let $X$ be a normed space. Consdier the series $\sum_{n=1}^\infty x_n$. If the series converges, then $\lim_{n \to \infty} x_n = 0$.}
\begin{proof}
    We know by the hypothesis that $s_n = x_1 + \cdots  + x_n$. Also $s_n \to s$ as $n \to \infty$. As such, we can write $x_n = s_n - s_{n-1}$ where both values on the RHS tend to $s$, meaning that $x_n$ tends to 0 as $n\to \infty$. 
\end{proof}
\nt{The above theorem is often useful to negate. In the exercises, we can also use teh fact that i $\lim_{n \to \infty}$ does not exists or does not equal 0, then $\sum_{n=1}^\infty x_n$ cannot converge. 

Also important to note that this series is very much one directional. For example, consider the following sums:
\begin{align*}
    \sum_{n=1}^\infty \frac{1}{n} &\text{ diverges} \\
    \sum_{n=1}^\infty \frac{1}{n^2} &\text{ converges}
\end{align*}
However, both values here tend to $0$ as $n \to \infty$.}

\ex{Geometric Series}{Consider $\sum_{n=1}^\infty x^n$. We know that $\lim_{n\to \infty} x^n = 0$ iff $|x| < 1$. So if $|x| \geq 1$, then the series does not converge. The theorem above does not help us for the $|x| < 1$ case. So let's compute the partial sum:
\begin{align*}
    s_n &= \sum_{k=1}^n x^k \\
    &= \frac{x^{n+1} - x}{x-1}
\end{align*}
So we have that $\lim_{n \to \infty} s_n = \frac{x}{1-x}$ for $|x| < 1$.}
\ex{}{Consider $X = \ell^\infty(E) = \{f : E \to \RR \text{ bounded}\}$ for $E \subseteq \RR$. The norm here is the supremum norm. Consider the series $\sum_{n=1}^\infty f_n(x)$ of random functions in $X$. We need to check that \[ \sup_{x \in E} |f_n(x)| \to 0 \text{ as } n \to \infty\] for the series to converge. If the limit $\neq 0$, then the series does not converge.}
\ex{Combining the above}{Let our space be $\ell^\infty((-1, 1))$ and consider the series $\sum_{n=1}^\infty f_n(x)$ where $f_n(x) = x^n$. We know that $\sup_{x \in (-1, 1)} |x^n| = 1$ for all $n$. Thus, the series does not converge.}

\thm{}{Consider a series of nonnegative terms $\sum_{n=1}^\infty x_n$ in $\RR$. Either the series converges or diverges to $+\infty$. }
\begin{proof}
    We know that $s_{m+1} \geq s_m$ for all $m$ and that these values are increasing, so $\lim_{m \to \infty} s_n = \sup_{n}s_n \in [0, \infty]$. 
\end{proof}
\thm{Comparison Test}{Let $\sum_{n=1}^\infty x_n$ and $\sum_{n=1}^\infty y_n$ be series of nonnegative terms in $\RR$. Assume that $0 \leq x_n \leq y_n$ for all $n \geq N$ for some $N$. If $\sum_{n=1}^\infty y_n$ converges, then $\sum_{n=1}^\infty x_n$ converges. If $\sum_{n=1}^\infty x_n$ diverges, then $\sum_{n=1}^\infty y_n$ diverges.}
\begin{proof}
    Consider the first case. So let $s_n = \sum x_n$ and $t_n = \sum y_n$. Since $y_n$ converges, $\lim t_n = T$ exists, so $t_n$ is bounded by $T$. Then for all $n \geq N$:
    \begin{align*}
        s_n &:= x_1 + \cdots + x_{N-1} + x_N + \cdots + x_n \\
        &\leq x_1 + \cdots x_{N-1} + y_N + \cdots + y_n \\
        &\leq x_1 + \cdots + x_{N-1} + T
    \end{align*}
    Hence, $\{s_n\}$ is bounded and increasing, so it converges.

    For the second case, we have that $s_n \to \infty$. So since \begin{align*}
        s_n \leq (x_1  +\cdots + x_{N-1}) + t_n
    \end{align*}
    we have that $t_n \to \infty$ as $n \to \infty$. 
\end{proof}

\ex{Examples}{
    \begin{enumerate}
        \item $\sum_{n=1}^\infty \left(\frac{1 + \cos n}{3}\right)^n$. We know that $\lim_{n \to \infty} \left(\frac{1 + \cos n}{3}\right)^n = 0$ so the series isn't divergent.
        
        We will compare it to $0 \leq \left(\frac{1 + \cos n}{3}\right)^n \leq \left(\frac{2}{3}\right)^n$. We know that $\sum_{n=1}^\infty \left(\frac{2}{3}\right)^n$ converges, so the series $\sum_{n=1}^\infty \left(\frac{1 + \cos n}{3}\right)^n$ converges by the comparison test.
        \item $\sum_{n=1}^\infty 1 - \cos \frac{1}{3^n}$. We know that $\lim_{n \to \infty} 1 - \cos \frac{1}{3^n} = 0$ so the series isn't divergent.
        
        We have that $\lim_{t \to 0} \frac{1-\cos t}{t} = 0$. Take $\epsilon = 1$ and find $\delta > 0$ such that $\left| \frac{1 - \cos t}{t} - 0 \right| < 1$ for all $0 <  |t| < \delta$. We know that $-1 < \frac{1 - \cos t}{t} < 1$. Now take $1 - \cos \frac{1}{3^n}  < \frac{1}{3^n}$ for all $n$ such that $ \frac{1}{3^n} < \delta $. 

        So, $1 - \cos \frac{1}{3^n} < \frac{1}{3^n}$ for all $ n > N$. THe RHS converges so by comparison test, the LHS converges.
        \item $\sum_{n=1}^\infty \frac{\sin \frac{1}{n^3}}{\log \left( 1 + \frac{1}{n} \right)} \left( e^{1/m} - 1 \right)$. We know that $\sin \frac{1}{n^3} \sim \frac{1}{n^3}$, $\log\left( 1 + \frac{1}{n} \right) \sim \frac{1}{n}$ and $e^{1/n} - 1 \sim \frac{1}{n}$. So we have that $\frac{\sin \frac{1}{n^3}}{\log \left( 1 + \frac{1}{n} \right)} \left( e^{1/m} - 1 \right) \sim \frac{1}{n^3} \cdot \frac{1}{n} \cdot \frac{1}{n} = \frac{1}{n^3}$. 
        \item Prove by induction that $n! > 2^n$ when $n \geq 4$. This implies that $\frac{1}{n!} \leq \frac{1}{2^n}$ for $n \geq 4$.  Since $\sum_{n=1}^\infty \frac{1}{2^n} < \infty$, comparison test tells us that $\sum_{n=0}^\infty \frac{1}{n!} < \infty$. The sum of the series is called \[e := \sum_{n=0}^\infty \frac{1}{n!}.\]
    \end{enumerate}
}

\thm{Root Test}{Let $x_n \geq 0$. 
\begin{enumerate}
    \item If $\limsup_{n \to \infty} \sqrt[n]{x_n} < 1$, then $\sum_{n=1}^\infty x_n < \infty$. 
    \item If $\limsup_{n\to \infty } \sqrt[n]{x_n} > 1$, then $\sum_{n=1}^\infty x_n = \infty$.
    \item If $\limsup_{n\to \infty } \sqrt[n]{x_n} = 1$, then the test is inconclusive.
\end{enumerate}}
\begin{proof}
    \begin{enumerate}
        \item Let $\ell = \limsup_{n\to \infty} \sqrt[n]{x^n}$. Assume $\ell < 1$. Find $\epsilon > 0$ such that $\ell + \epsilon < 1$. Then there exists $N$ such that $\sqrt[n]{x_n} < \ell + \epsilon$ for all $n \geq N$. Then $\sqrt[n]{x_n} > \ell - \epsilon$ for infinitely many $n$. Taking the first inequality, we have that $x_n < (\ell + \epsilon)^n$ for all $n \geq N$. By the comparison test, we have that $\sum_{n=1}^\infty (\ell + \epsilon)^n$ converges, so $\sum_{n=1}^\infty x_n$ converges.
        \item Assume $\ell > 1$. For $\epsilon > 0$ small, $(l - \epsilon) > 1$. So $x_n \geq (l - \epsilon)^n$ for infinitely many $n$. Since the RHS goes to infinity, a subsequence also goes to $\infty$, so $\lim_{n\to \infty} x_n \neq 0$ so the series cannot converge.
    \end{enumerate}
\end{proof}
\ex{Inconclusive Root Test}{
    Consider the series \[\sum_{n=1}^\infty \frac{1}{n} .\] Then, we have:
    \begin{align*}
        \sqrt[n]{\frac{1}{n}} = \left( \frac{1}{n}\right)^\frac{1}{n} = e^{\log\left(\frac{1}{n}\right)^\frac{1}{n}} = e^{\frac{\log\left(\frac{1}{n}\right)}{n}}
    \end{align*}
    The exponent goes to $0$, so the limit is 1.

    Now consider the series \[\sum_{n=1}^\infty \frac{1}{n^2} .\] Then, we have:
    \begin{align*}
        \sqrt[n]{\frac{1}{n^2}} = \left( \frac{1}{n^2}\right)^\frac{1}{n} = e^{\log\left(\frac{1}{n^2}\right)^\frac{1}{n}} = e^{\frac{\log\left(\frac{1}{n^2}\right)}{n}}
    \end{align*}
    The exponent goes to $0$, so the limit is 1.

    We see that the first series diverges and the second series converges, so the root test is inconclusive when the $\limsup$ is 1. 
}
\ex{More Root Test}{Consider the series \[\sum_{n=1}^\infty \frac{n^2+1}{2^n}.\] We have that $\lim_{n \to \infty} \frac{n^2+ 1}{2^n} = 0$, so the series isn't divergent. Consider the root test now. We have that \[\sqrt[n]{\frac{n^2+1}{2^n}} =  \frac{\sqrt[n]{n^2+1}}{2} =\frac{1}{2}e^{\frac{1}{n}\log(n^2+1)} \to \frac{1}{2}e^0 = \frac{1}{2} < 1.\] So the series converges.}

\exer{}{Let $x_n \geq 0$. Prove that \[\liminf_{n\to \infty} \frac{x_{n+1}}{x_n} \leq \liminf_{n\to \infty} \sqrt[n]{x_n} \leq \limsup_{n\to \infty} \sqrt[n]{x_n} \leq \limsup_{n\to \infty } \frac{x_{n+1}}{x_n}.\]
Find an example where the last inequality is strict:
\[\limsup_{n\to \infty} \sqrt[n]{x_n} < \limsup_{n\to \infty } \frac{x_{n+1}}{x_n}.\]
An example is \[x_n = \begin{cases} 1 & n \text{ odd} \\ 2 & n \text{ even} \end{cases}.\]}
\dfn{Ratio Test}{Let $x_n > 0$. 
\begin{enumerate}
    \item If $\limsup_{n\to \infty} \frac{x_{n+1}}{x_n} < 1$, then the series converges.
    \item If $\liminf_{n \to \infty} \frac{x_{n+1}}{x_n} > 1$, then the series diverges.
\end{enumerate}}
\begin{proof}
\begin{enumerate}
    \item This is a one-line proof. If the limit is less than 1, then the series converges by the root test by the exercise above. That is $, \limsup_{n\to \infty} \frac{x_{n+1}}{x_n} < 1 \implies \limsup_{n\to \infty} \sqrt[n]{x_n} < 1$.
    \item This is also a one-line proof. If the limit is greater than 1, then the series diverges by the root test by the exercise above. That is, $\liminf_{n \to \infty} \frac{x_{n+1}}{x_n} > 1 \implies \limsup_{n\to \infty} \sqrt[n]{x_n} > 1$.
\end{enumerate}    
\end{proof}
\ex{}{Consider the sequence:
\[x_n = \begin{cases} \frac{1}{2^n} & n \text{ odd} \\ \frac{1}{3^n} & n \text{ even} \end{cases}.\]
We have that $\limsup_{n\to \infty} \frac{x_{n+1}}{x_n} = \infty$ and that $\liminf_{n \to \infty} \frac{x_{n+1}}{x_n} = 0$. As such, we cannot apply the ratio test in this case. We try the root test instead.

We have that $\limsup_{n\to \infty} \sqrt[n]{x_n} = \frac{1}{2}$ and that $\liminf_{n\to \infty} \sqrt[n]{x_n} = \frac{1}{3}$. Because of the first one, we know that the series converges.}

\dfn{Integral Test}{Let $f: [1, \infty) \to [0, \infty)$ be a decreasing function in $[N, \infty)$. Then the series $\sum_{n=1}^\infty f(n)$ converges if and only if $\lim_{L \to \infty}\int_1^L f(x) \, dx$ converges.}
\begin{proof}
    We start with the forward direction. Consider $\int_{N}^\ell f(x)\, dx$ for integer $\ell$. We have:
    \begin{align*}
        \int_{N}^\ell f(x)\, dx &= \sum_{n=N}^{\ell-1} \int_n^{n+1} f(x)\, dx \\
    \end{align*}
    If $n \leq x \leq n+1$ and $f$ decreasing, we have that $f(n+1) \leq f(x) \leq f(n)$. For each $n$, we have:
    \begin{align*}
        \sum_{n=N}^{\ell-1} \int_n^{n+1} f(x)\, dx \leq \sum_{n=N}^{\ell-1}f(n)
    \end{align*}
    If $\lim_{\ell \to \infty} \int_{N}^\ell f(x)\, dx$ diverges, then $\sum_{n=N}^\infty f(n)$ diverges. So, 
    \begin{align*}
        \int_N^\ell f(x)\, dx &= \sum_{n=N}^{\ell-1} \int_n^{n+1} f(x)\, dx \\
        &\geq \sum_{n=N}^{\ell-1} f(n+1)
    \end{align*}
    So if $\lim_{\ell \to \infty} \int_{N}^\ell f(x)\, dx$ converges, then $\sum_{n=N}^\infty f(n)$ converges since it is less than or equal to.
\end{proof}

% \section{2/22 - Uniform Convergence}
% \dfn{Uniform Convergence}{Let $E$ be a set and take $\ell^\infty(E) = \{f : E \to \RR \text{ bounded}\}$. $\Vert f \Vert_\infty = \sup_{x \in E} |f(x)| < \infty$. Let $\{f_n\}_n \subset \ell^\infty(E)$, $f \in \ell^\infty(E)$. We say that $f_n \to f$ \emph{uniformly} when $\lim_{n \to \infty} \Vert f_n - f \Vert_\infty  = \lim_{n \to \infty} d(f_n, f)= 0$.}
% \nt{If $x \in E$, then $|f_n(x) - f(x)| \leq \Vert f_n - f \Vert_\infty$. This means that uniform convergence implies pointwise convergence. However, it is important to note that the converse is not true. That is, pointwise convergence does not imply uniform convergence. Here is an example. Let:
% \[f_n(x) = x^n \quad f(x) = \begin{cases} 0 & x \in [0, 1) \\ 1 & x = 1 \end{cases}.\]
% }
% \ex{}{Consider the following sequences:
% \begin{itemize}
%     \item $f_n(x) = n^2(x-1)e^{-n(x-1)}$. Find the largest set $E_\circ \subseteq \RR$ where $f_n$ converges pointwise.
%     \begin{itemize}
%         \item For $x=1$, $f_n(1) = 0\, \forall \, n \in \NN$, so $\lim_{n\to \infty} f_n(1) = 0$.
%         \item For $x > 1$, $f_n(x) = \frac{n^2(x-1)}{e^{n(x-1)}} \to 0$ as $n \to \infty$.
%         \item For $x < 1$, $f_n(x) = \frac{-n^2(x-1)}{e^{n(x-1)}} \to -\infty$ as $n \to \infty$.
%     \end{itemize}
%     So, $E_\circ = [1, \infty)$.
%     \item Sketch the graph of $f_n - f$ in $E_\circ$.
% \end{itemize}}

\section{More Series}
\ex{Integral Test}{Consider:
    \begin{itemize}
        \item $\sum_{n=1}^\infty \frac{1}{n^a}$ for $a > 0$ First we check that $\lim_{n \to infty} \frac{1}{n^a} = 0$. This is indeed true. Now we define $f(x) = \frac{1}{x^a}$ for $x > 0$. This function is decreasing, so we can use the integral test. We have:
        \begin{align*}
            \int_1^\infty \frac{1}{x^a}\, dx &= \lim_{L \to \infty} \int_1^L \frac{1}{x^a}\, dx \\
            &= \lim_{L \to \infty} \left[ \frac{x^{1-a}}{1-a} \right]_1^L \\
            &= \lim_{L \to \infty} \left[ \frac{L^{1-a}}{1-a} - \frac{1}{1-a} \right] \\
            &= \frac{-1}{a-1} \quad \text{if } a > 1
            &= \infty \quad \text{if } a < 1
        \end{align*}
        If $a = 1$, then we have:
        \begin{align*}
            \int_1^\infty \frac{1}{x}\, dx &= \lim_{L \to \infty} \int_1^L \frac{1}{x}\, dx \\
            &= \lim_{L \to \infty} \left[ \log(x) \right]_1^L \\
            &= \lim_{L \to \infty} \left[ \log(L) - \log(1) \right] \\
            &= \infty
        \end{align*}
        So, the series converges if $a > 1$ and diverges if $a \leq 1$.
        \item $\sum_{n=2}^\infty \frac{1}{n^a \log n}$. We have that $\lim_{n \to \infty} \frac{1}{n^a \log n} = 0$. We define $f(x) = \frac{1}{x^a \log x}$ for $x > 2$. This function is decreasing, so we can use the integral test. We have:
        \begin{align*}
            \int_2^\infty \frac{1}{x^a \log x}\, dx &= \lim_{L \to \infty} \int_2^L \frac{1}{x^a \log x}\, dx \\
            &= \lim_{L \to \infty} \left[ \frac{\log(\log(x))}{1-a} \right]_2^L \\
            &= \lim_{L \to \infty} \left[ \frac{\log(\log(L))}{1-a} - \frac{\log(\log(2))}{1-a} \right] \\
            &= \infty \quad \text{if } a > 1
        \end{align*}
    \end{itemize}
}
\dfn{Alternating Series}{Let $\{a_n\}_n$ be a sequence of positive numbers. The series $\sum_{n=1}^\infty (-1)^{n+1}a_n$ is called \emph{Alternating}}
\thm{Leibniz Test}{Consider $\sum_{n=1}^\infty (-1)^n a_n$ where $a_n \geq 0$.
    If $\{a_n\}_n$ is decreasing and $\lim_{n \to \infty} a_n = 0$, then the series converges and $|S - s_n| \leq a_{n+1}$ for all $n$.}
\begin{proof}
    Write
    \begin{align*}
        s_{2n + 1} &= -a_1 + (a_2 - a_3) + (a_4 - a_5) + \cdots + (a_{2n} - a_{2n + 1}) \\
        &= -(a_1 - a_2) - (a_3 - a_4) - \cdots - (a_{2n-1} - a_{2n}) - a_{2n+1} 
    \end{align*}
    Since $a_n$ is decreasing, we have that $a_i - a_{i-1} \geq 0$. And from the first equality, we get that $s_{2n+1} \leq s_{2n+3}$, meaning that $s_{2n + 1}$ is an increasing sequence. But from the second equality, we get that $s_{2n+1} \leq -a_{2n+1} \leq 0$. So, there exists:
    \begin{align*}
        \lim_{n \to \infty} s_{2n+1} = \sup_n s_{2n+1} = S \in (-\infty, 0]
    \end{align*}
    Since $s_{2n+1} = s_{2n} + a_{2n+1}$ and $\lim_{n\to\infty} a_n = 0$, we have that $\lim_{n\to\infty} s_{2n} = S$. So, the series converges.

    Moreover, we have that: 
    \begin{align*}
        s_{2n} = -(a_1 - a_2) - (a_3 - a_4) - \cdots - (a_{2n-1} - a_{2n}),
        \end{align*}
        which implies that $s_{2n} \geq s_{2n+2}$, meaning that $s_{2n}$ is a decreasing sequence. So $\inf_n s_{2n} = S \in (-\infty, 0]$. Therefore, $s_{2n+1} \leq S \leq s_{2n}$. It follows that
        \begin{align*}
            |S - s_{2n}| &= s_{2n} - S \leq s_{2n} - s_{2n+1} = a_{2n+1} \\
            |S - s_{2n+1}| &= s_{2n+1} - S \leq s_{2n+2} - s_{2n+1} = a_{2n+1}
        \end{align*}
        as desired.
\end{proof}
\cor{}{Also if an alternating series converges, then the remainder $R_n = |S - S_n|$ satisfies $0 \leq R_n \leq a_{2n+1}$.}
\begin{proof}
    We have that $S_{2n + 1} \leq S$ and that $S_{2n}$ is decreasing. So $S = \inf_{n \in \NN} S_{2n}$, so $S \leq S_{2n}$. This yields $|S - S_{2n} | = S_{2n} - S \leq S_{2n} - S_{2n + 1} = a_{2n + 1}$. For the other case, we have $|S - S_{2n+1}| + S - S_{2n+1} \leq S_{2n+2} - S_{2n + 1} \leq a_{2n+2}$.
\end{proof}
\ex{}{Consider the sequence \[ \sum_{n=1}^\infty (-1)^n \frac{n \log n}{1 + n^2}. \] We consider $\lim_{n \to \infty} \frac{n \log n}{1 + n^2}$. This is similar to the limit of $\frac{\log n}{n}$, which diverges. So by comparison test, our limit diverges. So we have:
\begin{align*}
    f(x) &= \frac{x \log x}{1 + x^2} \\
    f'(x) &= \frac{\log x + 1}{x^2 + 1} - \frac{2x^2 \log x}{(x^2 + 1)^2}
\end{align*}
We can somehow show that $f'(x) < 0$ for all $x \geq N$ for some $N$}

\newpage 
\thm{}{Let $E, \ell^\infty(E) = \{f : E \to \RR \text{ bounded}\}$. Let $\{f_n\}_n \subset \ell^\infty(E)$ and $f \in \ell^\infty(E)$. 
\begin{enumerate}
    \item If $\sum_{n=1}^\infty \sup_{x \in E}|f_n(x)| < \infty$, then $\sum_{n=1}^\infty f_n(x)$ converges uniformly in $E$.
    \item If $\sum_{n=1}^\infty f_n(x)$ converges uniformly to $f$, then $\lim_{n \to \infty} \sup_{x \in E}|f_n(x)| = 0$. 
\end{enumerate}}
\ex{}{
    \begin{enumerate}
        \item Consider the series $\sum_{n=1}^\infty \frac{e^{nx}}{n}$, for $x \in \RR$.
        
        $\frac{e^{nx}}{n} > 0 \, \forall x \in \RR$.
        \begin{itemize}
            \item For $x = 0$, we have $\sum_{n=1}^\infty \frac{1}{n} = \infty$.
            \item For $x > 0$, we have $\lim_{n\to \infty} \frac{e^{nx}}{n} = \infty$.
            \item For $x  < 0$, $\left(\frac{e^{nx}}{n}^{1/n}\right) = \frac{e^x}{n^{1/n}} \to e^x$ as $ n \to infty$. 
        \end{itemize}
        This shows that there is pointwise convergence when $x  < 0$. So if we want to determine a subset were there is uniform convergence, then we have to consider only $x \in (-\infty, 0)$.

        So consider $E = (-\infty ,-\epsilon)$ for some $\epsilon > 0$. Consider the sequence of functions defined as \[f_n(x) = \frac{e^{nx}}{n}\] for $x \in E$. Then we have:
        \[f_n'(x) = e^{nx} > 0 \implies \sup_{x \in E}|f_n(x)| = \frac{e^{-n\epsilon}}{n}\]
        So, by our theorem, we have that $\sum_{n=1}^\infty \frac{e^{nx}}{n}$ converges uniformly in $E$.
        \item Consider $\sum_{n=1}^\infty \frac{x^{2n}}{\sqrt[3]{n}} \log\left( 1 + \frac{x^2}{\sqrt[3]{n}}\right)$, for $x \in \RR$. 
        
        \begin{itemize}
            \item for $x = 0$, it converges to 0.
            \item for $|x| > 1$, we have that \begin{align*}
                \lim_{n\to \infty} \frac{x^{2n}}{\sqrt[3]{n}} \log\left( 1 + \frac{x^2}{\sqrt[3]{n}}\right) = \lim_{n \to \infty} \frac{x^{2n+2}}{n^{2/3}} \frac{\log\left( 1 + \frac{x^2}{\sqrt[3]{n}}\right)}{x^2/\sqrt[3]{n}} = \infty
            \end{align*}
            \item for $|x| < 1$, $\lim_{n \to \infty} \frac{x^{2n}}{\sqrt[3]{n}} \log\left( 1 + \frac{x^2}{\sqrt[3]{n}}\right) = 0$
        \end{itemize}
        \item Consider $\sum_{n=1}^\infty \frac{x^n}{n}$ for $x \geq 0$.
        \begin{itemize}
            \item for $x = 0$, there is pointwise convergence .
            \item for $x \geq 1$, there is no pointwise convergence.
            \item For $x \in (0, 1)$, we have that $\lim_{n \to \infty} \frac{x^n}{n} = 0$.
        \end{itemize}
    \end{enumerate}
}
\newpage
\thm{}{Take some $x_n \in \RR$ and consider the series $\sum_{n=1}^\infty$. If $\sum_{n\to \infty} |x_n|$ converges, then $\sum_{n=1}^\infty x_n$ converges.}
\nt{The converse isn't true. Consider the alternating version of the harmonic series.}
\dfn{}{Let $t \in \RR$. We define:
\begin{align*}
    t^+ &= \max(t, 0) \\
    t^- &= \max(-t, 0)
\end{align*}
From these, we derive:
\begin{align*}
    |t| = t^+ + t^-  \quad \text{and} \quad t = t^+ - t^-
\end{align*}}
\begin{proof}
    We have $0 \leq x_n^+ \leq |x_n|$. By comparison test, we have that $\sum_{n=1}^\infty x_n^+$ converges. We also have $0 \leq x_n^- \leq |x_n|$. By comparison test, we have that $\sum_{n=1}^\infty x_n^-$ converges. Remember that by the limit of the sum,
    \begin{align*}
        \sum_{n=1}^\infty x_n^+ &= \lim_{\ell \to \infty}\sum_{n=1}^\ell x_n^+ \\
        \sum_{n=1}^\infty x_n^- &= \lim_{\ell \to \infty}\sum_{n=1}^\ell x_n^-
    \end{align*}
    So, we have that $$\sum_{n=1}^\infty x_n = 
    \lim_{\ell \to \infty}\sum_{n=1}^\ell x_n^+ -  x_n^- = \lim_{\ell \to \infty} \sum_{n=1}^\ell x_n.$$
    This implies that $x_n$ converges as desired.
\end{proof}
\thm{}{Let $E$ be a set and $\ell^\infty(E) = \{f : E \to \RR \text{ bounded}\}$. Let $\{f_n\}_n \subset \ell^\infty(E)$ and $f \in \ell^\infty(E)$. Then,
\begin{enumerate}
    \item If $\sum_{n = 1}^\infty \sup_{x \in E}|f_n(x)| < \infty$, then $\sum_{n=1}^\infty f_n(x)$ converges uniformly in $E$.
    \item If $\sum_{n=1}^\infty f_n(x)$ converges uniformly to $f$, then $\lim_{n \to \infty} \sup_{x \in E}|f_n(x)| = 0$.
\end{enumerate}}
\begin{proof}
    Let $a_n = \sup_{x \in E}|f_n(x)|$. We know that the sum of the $a_n$ converges in $\RR$. Fix an $x \in E$. We have that $0 \leq |f_n(x)| \leq a_n$. By the comparison test, we have that $\sum_{n=1}^\infty |f_n(x)|$ converges pointwise. So by the previous theorem, $\sum_{n=1}^\infty f_n(x)$ converges pointwise in $\RR$. This isn't good enough; we want uniform convergence. That is, we want:
    \begin{align*}
        \Vert f - \sum_{n = 1}^\infty f_n \Vert_\infty \to 0
    \end{align*}
    FINSIH THIS LATER
\end{proof}
\section{Continuity}
\dfn{Continuity}{Let $(X, d_x)$ and $(Y, d_Y)$ be metric spaces. Let $E \subseteq X$ and $f: E \to Y$. Let $x_0 \in E$ and assume $x_0 \in \operatorname{acc}E$. We say that $f$ is continuous at $x_0$ if there is $\lim_{x \to x_0} f(x) = f(x_0)$. We say that $f$ is continuous on $E$ if $f$ is continuous at all $x_0 \in E$.

We denote $C(E)$ as the continuous functions on $E$.}
\ex{}{\begin{enumerate}
    \item Consider sequences. That is, $f \NN \to \RR$. This is continous because $\NN \cap \operatorname{acc}\NN = \emptyset$.
    \item If we have $f: [0, 1] \cup \{3\} \to \RR$, we only check continuity at $x_0 \in [0, 1]$. $f$ is continuous at 3.
    \item The sum, product, quotient (denominator nonzero), and composition of two continuous functions is continuous.
\end{enumerate}}
\exer{Continuity}{
    \begin{itemize}
        \item $x^n$ continuous
        \item $\sin(x)$ continuous
        \item $\cos(x)$ continuous
    \end{itemize}
}
\dfn{Relatively Open}{Let $(X, d_X)$ be a metric space and $E \subseteq X$. We say that $F \subseteq E$ is \emph{relatively open} in $E$ if $F = E \cap U$ with $U$ open.}
\thm{}{Let $(X, d_X)$ and $(Y, d_Y)$ be metric spaces. Let $E \subseteq X$ and $f: E \to Y$. Then $f$ is continuous on $E$ if and only if for all open sets $V \subseteq Y$, $f^{-1}(V)$ is relatively open in $E$.}

\ex{}{Let $F = \{(x, y) \in \RR^2 : x + \sin y > 4 \}$.Then $f(x,y) = x + \sin y$ is continuous because $F = f^{-1}((4, \infty))$.}

\begin{proof}
    We start with the forward direction. Assume that $f$ is continuous on $E$. Let $V \subseteq Y$ be open. Consider $f^{-1}(V)$. If $V \neq \emptyset$, let $x_0 \in f^{-1}(V)$. Then $f(x_0) \in V$. Find $B_Y(f(x_0), \epsilon) \subseteq V$. If $x_0 \in \operatorname{acc}E$, then we can find $\delta > 0$ such that if $x \in E$ and $d_X(x, x_0) < \delta$, then $d_Y(f(x), f(x_0)) < \epsilon$. So if $x \in B(x_0, \delta) \cap E$, then $f(x) \in B_Y(f(x_0), \epsilon) \subseteq V$. So $B(x_0, \delta) \cap E \subseteq f^{-1}(V)$.

    If $x_0$ isn't an accumulation point, then there exists $\delta > 0$ such that $B(x_0, \delta) \cap E = \{x_0\}$. So $f^{-1}(V) = \{x_0\}$. 
    
    So we just have $f^{-1}(V) = E \cap \bigcup_{x \in E} B(x, \delta_x)$. So $f^{-1}(V)$ is relatively open in $E$.

    For the backward direction, assume that $f^{-1}(V)$ is relatively open for all $V \subseteq Y$ that are open. We want to show that $f$ is continuous. Let $x_0 \in E \cap \operatorname{acc}E$. We want to show that the limit as $x \to x_0$ of $f(x)$ is $f(x_0)$. Take $V = B_Y(f(x_0), \epsilon)$. Since $f^{-1}(V)$ is relatively open, $f^{-1}(B(f(x_0), \epsilon)) = E \cap U$ for some open $U$. So $x_0 \in U$, so there exists $\delta > 0$ such that $B(x_0, \delta) \cap E \subseteq U$. So if $x \in B(x_0, \delta) \cap E$, then $f(x) \in B_Y(f(x_0), \epsilon)$. So $\lim_{x \to x_0} f(x) = f(x_0)$ because $d(f(x), f(x_0)) < \epsilon$.
\end{proof}
\newcommand{\acc}{\operatorname{acc}}

\thm{}{Let $(X, d_X)$ and $(Y, d_Y)$ be metric spaces. Take $ K \subseteq X$ that is sequentially compact and $f: K \to Y$ that is continuous. Then $f(K)$ is sequentially compact.}
\begin{proof}
    Let $y_n \in f(K)$. Then there exists $x_n \in K$ such that $f(x_n) = y_n$. Since $K$ is sequentially compact, there exists a subsequence $\{x_{n_k}\}_k$ that converges to $x \in K$. Since $f$ is continuous, we have that $f(x_{n_k}) \to f(x)$ if $x \in \operatorname{acc}E$. If $x \notin \acc E$, then it is a constant sequence and we are done. So $f(x) \in f(K)$. 
\end{proof}
\thm{Weierstrass Theorem}{Let $(X, d_X)$ be a metric space and $K \subseteq X$ that is sequentially compact. Let $f: K \to \RR$ be continuous. Then $f$ is bounded and attains its bounds.}
\begin{proof}
    We know that $f(K)$ closed and bounded by the previous theorem. As such, there exists $\sup f(K) = \ell \in \RR$. We want to show that this is the maximum. Consider $\ell - \frac{1}{n}$. This is not an upper bound for $f(K)$, so there exists $x_n \in K$ such that $f(x_n) > \ell - \frac{1}{n}$. Since $K$ is sequentially compact, there exists a subsequence $\{x_{n_k}\}_k$ that converges to $x \in K$. So, 
    \begin{align*}
        \ell - \frac{1}{n_k} < f(x_{n_k}) \leq \ell
    \end{align*}
    As $k \to \infty$, we have that $f(x_{n_k}) \to \ell$. So $f(x) = \ell$. So $f$ attains its maximum.
\end{proof}

\thm{Let $I \subseteq \RR$ be an interval and $f:I \to \RR$ continuous and assume there exist $x_1$ and $x_2$ such that $f(x_1) < 0 < f(x_2)$. Then there exists $x_0 \in I$ such that $f(x_0) = 0$.}

\begin{proof}
    Assume $x_1 < x_2$. So $
    lim_{x \to x_1} = f(x_1) < 0$. Let $\epsilon = -f(x_1)/2$ to find $\delta_1 > 0$ such that $f(x) < 0$ in $[x_1, x_1 = \delta_1]$. 

    We also have that $\lim_{x \to x_2} f(x) = f(x_2) > 0$. Let $\epsilon = f(x_2)/2 > 0$. So we can find $\delta_2 > 0$ such that $f(x) > 0$ in $[x_2 - \delta_2, x_2]$.

    Consider the set $E = \{ x \in [x_1, x_2] : f(x) < 0\}$, which is bounded above by $x_2 - \delta_2$. So $\sup E = \ell \in [x_1 + \delta_2, x_2 - \delta-2]$ exists.
    
    We claim that $f(\ell) = 0$. If $f(\ell) < 0$, then by dfn of continuity, $f(\ell) = \lim_{x \to \ell} f(x)$. Take $\epsilon = -\ell / 2$ and find $\delta_3 > 0$ such that $f(x) < 0$ in $[\ell - \delta_3, \ell + \delta_3]$. So $\ell + \delta_3 \in E$, which is a contradiction because $\ell$ is a maximum. If $f(\ell) > 0$, then take $\epsilon = \ell / 2$ and find $\delta_4 > 0$ such that $f(x) > 0$ in $[\ell - \delta_4, \ell + \delta_4]$. So $\ell - \delta_4 \in E$, which is a contradiction because it is then a better lower bound than $\ell$. So $f(\ell) = 0$ by trichotomy.
\end{proof}
\cor{}{A polynomial of odd degree has at least one zero.}
\begin{proof}
    Consider $p(x)$ that has odd degree. WLOG assume the first coefficient is positive. So we have:
    \begin{align*}
        \lim_{x\to \infty} p(x) = \infty \quad \lim_{x \to -\infty} p(x) = -\infty
    \end{align*}
    By the previous theorem, we have that there exists $x_0 \in \RR$ such that $p(x_0) = 0$.
\end{proof}

\cor{}{Consider the interval $I \subseteq \RR$ with $f: I \to \RR$ continuous. Then $f(I)$ is an interval with endpoints $\inf f(I)$ and $\sup f(I)$.}
\begin{proof}
    Let $y_1, y_2 \in f(I)$ with $y_1 < y_2$ and let $y_1 < t < y_2$. Then we wnant to show that $t \in f(I)$. Consider $g(x) = f(x) - t$. There exists $x_1$ and $x_2$ such that $f(x_1) = y_1$ and $f(x_2) = y_2$. So $g(x_1) < 0 < g(x_2)$. So by the previous theorem, there exists $x_0 \in I$ such that $g(x_0) = 0$. So $f(x_0) = t$.
\end{proof}

\cor{}{Let $I \subseteq \RR$ be an interval and $f : I \to \RR$ that is continuous and injective. Then $f^{-1} : f(I) \to \RR$ is continuous.}
\ex{bad}{Consider $E = [0, 1] \cup (2, 3]$. Then let:
\begin{align*}
    f(x) = \begin{cases} x & x \in [0, 1] \\ x - 1 & x \in (2, 3] \end{cases}
\end{align*}
This does not have a continuous inverse.}
\begin{proof}
    First we show that $f$ is monotone. Assume FSOC that there are $a<b$ such that $f(a) < f(b)$. Then $f$ is strictly increasing. 
\end{proof}
\thm{}{Let $(X, d_X)$ and $(Y, d_Y)$ be metric spaces. Let $K \subseteq X$ be sequentially compact. Let $f: K \to Y$ be continuous and injective. Then $f^{-1} : f(K) \to X$ is continuous.}
\begin{proof}
    Let $y_0 \in f(K)$ and let $y_0 \in \acc f(K)$. We claim that $\lim_{y \to y_0} f^{-1}(y) = f^{-1}(y_0)$. So BWOC, there exists $\epsilon > 0$ such that for every $\delta$, we can find $y \in B_Y(y_0, \delta)$ such that $d_X(f^{-1}(y), f^{-1}(y_0)) \geq \epsilon$. Let $\delta = 1/n$. Then we can find $y_n \in B_Y(y_0, 1/n)$ such that $d_X(f^{-1}(y_n), f^{-1}(y_0)) \geq \epsilon$. $y_n \in f(K)$, so there is $x_n \in K$ such that $f(x_n) = y_n$. Since $K$ is sequentially compact, there exists a subsequence $\{x_{n_k}\}_k$ that converges to $x_0 \in K$. So since $f$ is continuous, we have that $x_{n_k} \to x_0 \implies f(x_{n_k})= y_{n_k}\to f(x_0) = y_0$. $d_X(f^{-1}(y_{n_k}), f^{-1}(y_0)) \geq \epsilon > 0$. But we have that $d_X(x_{n_k}, x_0) \to 0$, contradiction. As such, we have that $\lim_{y \to y_0} f^{-1}(y) = f^{-1}(y_0)$ and so $f^{-1}$ is continuous.
\end{proof}
\newpage
\section{Differentiability}
\dfn{Directional Derivatives}{Let $(X, \Vert \cdot \Vert_X)$ and $(Y, \Vert \cdot \Vert_Y)$ be normed spaces. Let $f: E \to Y$, $E \subseteq X$, $x_0 \in E$, $v \in X$, $\Vert v \Vert_X = 1$. $L = \{ x \in E : x = x_0 + tv \text{ for some } t \in \RR\}$. Assume $x_0 \in \acc L$. 

The directional derivative of $f$ at $x_0$ in the direction of $v$ is:
\[\lim_{t \to 0} \frac{f(x_0 + tv) - f(x_0)}{t},\] whenever the limit exists. It is denoted as $\frac{\partial f}{\partial v}(x_0)$. }

\nt{If $X = \RR^N$ and $\ell_i$ is the $i$th vector in the canonical basis, then $\frac{\partial f}{\partial \ell_i}(x_0)$ is the $i$th partial derivative of $f$ at $x_0$.

Also, if we have $f(x, y, z)$, then $\frac{\partial f}{\partial x}(x_0, y_0, z_0)$ is the same as $\lim_{t \to 0} \frac{f(x_0, y_0 + t, z_0) - f(x_0, y_0, z_0)}{t}$.}

\dfn{One-dimensional Derivative}{Let $X = \RR$ and $v = 1$. Then:
\begin{align*}
    \frac{\partial f}{\partial v}(x_0) = \lim_{t \to 0} \frac{f(x_0 + t) - f(x_0)}{t} = f'(x_0)
\end{align*}
This is the one-dimensional derivative of $f$ at $x_0$.}

If $f'(x_0)$ exists, then $f$ is continuous at $x_0$. This is not true for $N \geq 2$. 

\dfn{Differentiability}{Let $(X, \Vert \cdot \Vert_X)$ and $(Y, \Vert \cdot \Vert_Y)$ be normed spaces. Let $E \subseteq X$ and $f: E \to Y$. We say that $f$ is differentiable at $x_0 \in E \cap \acc E$ if there exists a linear function $L: X \to Y$ and continuous such that $\lim_{x \to x_0} \frac{f(x) - f(x_0) - L(x - x_0)}{\Vert x - x_0 \Vert_X} = 0$. We denote $L$ as $df(x_0)$. $L$ is called the differential of $f$ at $x_0$. }

\thm{(Useful to negate)}{Let $(X, \Vert \cdot \Vert_X)$ and $(Y, \Vert \cdot \Vert_Y)$ be normed spaces. Let $E \subseteq X$ and $f: E \to Y$. Let $x_0 \in E \cap \acc E$. If $f$ is differentiable at $x_0$, then $f$ is continuous at $x_0$.}
\begin{proof}
    Let $L = df(x_0)$. Then we have:
    \begin{align*}
        f(x) - f(x_0) &= f(x) - f(x_0) - L(x-x_0) + L(x-x_0) \\
        &= \frac{f(x) - f(x_0) - L(x-x_0)}{\Vert x - x_0 \Vert_X} \Vert x - x_0 \Vert_X + L(x-x_0) \\
        &= 0 \cdot 0 + L(0) = 0
    \end{align*}
    So $f$ is continuous at $x_0$.
\end{proof}
\thm{(Also useful to negate)}{Let $(X, \Vert \cdot \Vert_X)$ and $(Y, \Vert \cdot \Vert_Y)$ be normed spaces. Let $E \subseteq X$ and $f: E \to Y$. Let $x_0 \in E \cap \acc E$. Assume $f$ is differentiable at $x_0$. Let $v \in X$ with $\Vert v \Vert_X = 1$. Assume $x_0 \in \acc L$, $L = \{ x \in E : x = x_0 + tv \text{ for some } t \in \RR\}$. Then $\frac{\partial f}{\partial v}(x_0) = T(v)$ where $T$ is the differential of $f$ at $x_0$.}
\begin{proof}
    Want to show:
    \begin{align*}
        \lim_{t \to 0} \frac{f(x_0 + tv) - f(x_0)}{t} = T(v).
    \end{align*}
    By the definition of differentiability, we know that:
    \begin{align*}
        \lim_{x \to x_0} \frac{f(x) - f(x_0) - T(x-x_0)}{\Vert x - x_0 \Vert_X} = 0
    \end{align*}
    where $T: X \to Y$ is linear and continuous. Take $x = x_0 + tv$ (restriction). Then we have:
    \begin{align*}
        \frac{f(x) - f(x_0) - T(x-x_0)}{\Vert x - x_0 \Vert_X} &= \frac{f(x_0 + tv) - f(x_0) - T(tv)}{\Vert tv \Vert_X} \\
        &= \frac{f(x_0 + tv) - f(x_0) - tT(v)}{|t| \Vert v \Vert_X} \\
        &= \frac{f(x_0 + tv) - f(x_0) - tT(v)}{|t|}
    \end{align*}
    If we take the limit from the right, we get:
    \begin{align*}
        \lim_{t \to 0^+} \frac{f(x) - f(x_0) - T(x-x_0)}{\Vert x - x_0 \Vert_X} &= 0 \\
        &= \lim_{t \to 0^+} \frac{f(x_0 + tv) - f(x_0) - tT(v)}{|t|} \\
        &= \lim_{t \to 0^+} \frac{f(x_0 + tv) - f(x_0)}{t} - T(v) \\
        &\implies \lim_{t \to 0^+} \frac{f(x_0 + tv) - f(x_0)}{t} = T(v)
    \end{align*}
    If we take the limit from the left, we get:
    \begin{align*}
        \lim_{t \to 0^-} \frac{f(x) - f(x_0) - T(x-x_0)}{\Vert x - x_0 \Vert_X} &= 0 \\
        &= \lim_{t \to 0^-} \frac{f(x_0 + tv) - f(x_0) - tT(v)}{|t|} \\
        &= \lim_{t \to 0^-} \frac{f(x_0 + tv) - f(x_0)}{t} - T(v) \\
        &\implies \lim_{t \to 0^-} \frac{f(x_0 + tv) - f(x_0)}{t} = T(v)
    \end{align*}
\end{proof}

\nt{Let $X = \RR^N$ and $x_0 \in E^\circ$. Then let $f: E \to \RR$. Assume $f$ is differentiable at $x_0$. Then for all $v \in \RR^N$ with $\Vert v \Vert = 1$, we have that $\frac{\partial f}{\partial v}(x_0) = T(v)$. But we have that:
\begin{align*}
    v = (v_1, v_2, \ldots, v_N)
\end{align*}
which means that
\begin{align*}
    v = \sum_{i=1}^N v_i e_i.
\end{align*}
Since $T$ is linear, we have that:
\begin{align*}
    \frac{\partial f}{\partial v}(x_0) = T(v) &= \sum_{i=1}^N v_i T(e_i) \\
    &= \sum_{i=1}^N v_i \frac{\partial f}{\partial x_i}(x_0)
\end{align*}}
\dfn{Gradient}{Let $X = \RR^N$ and $x_0 \in E^\circ$. Let $f: E \to \RR$. Assume $f$ is differentiable at $x_0$. Then the gradient of $f$ at $x_0$ is:
\begin{align*}
    \nabla f(x_0) = \left( \frac{\partial f}{\partial x_1}(x_0), \frac{\partial f}{\partial x_2}(x_0), \ldots, \frac{\partial f}{\partial x_N}(x_0) \right)
\end{align*}}
\nt{So to check that $f$ is differentiable at $x_0$, we need to check:
\begin{align*}
    \lim_{x \to x_0} \frac{f(x) - f(x_0) - \nabla f(x_0) \cdot (x - x_0)}{\Vert x - x_0 \Vert} = 0
\end{align*}}
\thm{(useful to prove differentiability at most points)}{Let $E \subseteq \RR^N$ and $x_0 \in E^\circ$. Let $f: E \to \RR$. Assume 
\begin{enumerate}
    \item $\frac{\partial f}{\partial x_i}$ exists in $B(x_0, r) \subseteq E$ for all $i = 1, 2, \ldots, N$.
    \item $\frac{\partial f}{\partial x_i}$ is continuous at $x_0$ for all $i = 1, 2, \ldots, N$.
\end{enumerate}
Then $f$ is differentiable at $x_0$.}

\mlenma{Rolle}{Let $f: [a, b] \in \RR$ with $a< b$ be continuous in $[a, b]$ and differentiable on $(a, b)$. Also assume $f(a) = f(b)$. Then there exists $c \in (a, b)$ such that $f'(c) = 0$.}
\begin{proof}
    $[a,b]$ is sequentially compact and $f$ is continuous, so there are $\max f(x) = M$ and $\min f(x) = m$. If $M = m$, then $f$ is constant and we are done since the derivative is 0 for all $x$. Assume $M > m$. Since $f(a) = f(b)$, we have that either $M$ or $m$ is in the interior. Say $m = f(c)$ for $c \in (a, b)$. So $f(x) - f(c) \geq 0$ for all $x \in [a, b]$. If $x  -c  > 0$, then we have 
    \begin{align*}
        \frac{f(x) - f(c)}{x - c} \geq 0
    \end{align*}
    So,
    \begin{align*}
        \lim_{x \to c^+} \frac{f(x) - f(c)}{x - c} = f'(c) \geq 0.
    \end{align*}
    If $x - c < 0$, then we have
    \begin{align*}
        \frac{f(x) - f(c)}{x - c} \leq 0
    \end{align*}
    So,
    \begin{align*}
        \lim_{x \to c^-} \frac{f(x) - f(c)}{x - c} = f'(c) \leq 0.
    \end{align*}
    So $f'(c) = 0$.
\end{proof}
\mlenma{MVT}{Let $f: [a, b] \to \RR$ be continuous in $[a, b]$ and differentiable on $(a, b)$. Then there exists $c \in (a, b)$ such that:
\begin{align*}
    f(b) - f(a) = f'(c)(b-a).
\end{align*}}
\begin{proof}
    Want $g$ such that $g(b) = g(a)$. Take $g(x) = f(x) - (x - a)\frac{f(b) - f(a)}{b-a}$. Let's make sure this works:
    \begin{align*}
        g(b) &= f(b) - (b - a)\frac{f(b) - f(a)}{b-a} = f(b) - f(b) + f(a) = f(a) \\
        g(a) &= f(a) - (a - a)\frac{f(b) - f(a)}{b-a} = f(a)
    \end{align*}
    So $g(a) = g(b)$. So by Rolle's theorem, there exists $c \in (a, b)$ such that $g'(c) = 0$. So $g'(c) = f'(c) - \frac{f(b) - f(a)}{b-a} = 0$. So $f'(c) = \frac{f(b) - f(a)}{b-a}$ as desired.
\end{proof}
Now we actually prove Theorem 2.2.12.

\begin{proof}
    Take $x \in B(x_0, r)$ and consider $f(x) - f(x_0)$. We need to define these vectors in terms of their components:
    \begin{align*}
        x = (x_1, x_2, \ldots, x_N) \quad x_0 = (x_{01}, x_{02}, \ldots, x_{0N})
    \end{align*}
    So, we alternate the definition of $f(x) - f(x_0)$ component-wise:
    \begin{align*}
        f(x) - f(x_0) &= f(x_1, x_2, \ldots, x_N) - f(x_{01}, x_2, \ldots, x_N) + f(x_{01}, x_2, \ldots, x_N) - f(x_{01}, x_{02}, x_3, \ldots, x_N) + \ldots \\
    \end{align*}
    Consider $g(x_1) = f(x_1, x_2, \ldots, x_N)$ where the last $N-1$ components are fixed. By MVT,
    \begin{align*}
        g(x_1) - g(x_{01}) = g'(c_1)(x_1 - x_{01})
    \end{align*}
    where $c_1$ is between $x_1$ and $x_{01}$. That is,
    \begin{align*}
        f(x_1, x_2, \ldots, x_N) - f(x_{01}, x_2, \ldots, x_N) = \frac{\partial f}{\partial x_1}(c_1, x_2, \ldots, x_N)(x_1 - x_{01})
    \end{align*}
    So 
    \begin{align*}
        f(x) - f(x_0) &= \frac{\partial f}{\partial x_1} (c_1, x_2, \ldots, x_N)(x_1 - x_{01}) + \frac{\partial f}{\partial x_2} (x_{01}, c_2, x_3, \ldots, x_N)(x_2 - x_{02}) + \ldots \\
        &+ \frac{\partial f}{\partial x_{N-1}} (x_{01}, x_2, \ldots, c_{N-1}, x_n)(x_{N-1} - x_{0N-1}) + f(x_{01}, \ldots, x_{0N-1}, x_N) - f(x_{01}, \ldots, x_{0N})
    \end{align*}
    First let's define $z_m = (x_{01}, \ldots, c_m, \ldots, x_N)$. We can now consider:
    \begin{align*}
        f(x) - f(x_0) - \nabla f(x_0) \cdot (x - x_0) &= f(x) - f(x_0)  - \sum_{i=1}^n \frac{\partial f}{\partial x_i}(x_0)(x_i - x_{0i}) \\
        &= \left(\frac{\partial f}{\partial x_1} (z_1)- \frac{\partial f}{\partial x_1}(x_0)\right)(x_1 - x_{01}) + \ldots \\
        &+ f(x_{01}, \ldots, x_{0N-1}, x_N) - f(x_{01}, \ldots, x_{0N}) 
    \end{align*}
    Now we divide by $\Vert x - x_0 \Vert$ to get:
    \begin{align*}
        &\frac{| f(x) -f(x_0) - \nabla f(x_0) \cdot (x-x_0)|}{\Vert x - x_0 \Vert} \leq \\
        &\left| \frac{\partial f}{\partial x_1} (z_1)- \frac{\partial f}{\partial x_1}(x_0)\right| \frac{|x_1 - x_{01}|}{\Vert x - x_0 \Vert} + \ldots + \\
        &\left| \frac{\partial f}{\partial x_{N-1}} (z_{N-1})- \frac{\partial f}{\partial x_{N-1}}(x_0)\right| \frac{|x_{N-1} - x_{0N-1}|}{\Vert x - x_0 \Vert} + \\
        &\frac{|f(x_{01}, \ldots, x_{0N-1}, x_N) - f(x_0) - \frac{\partial f}{\partial x_N}(x_0)(x_N - x_{0N})|}{\Vert x - x_0 \Vert}
    \end{align*}
    All the terms that aren't the last one end up tending to 0 as $x \to x_0$. Consider the last term and call it $A$. So,
    \begin{align*}
        A &= \left| \frac{ f(x_0, \ldots, x_{0N-1}, x_N) - f(x_0) }{x_N - x_{0N}} - \frac{\partial f}{\partial x_N}(x_0) \right| \\
        &= \left| \frac{f(x_0 + t \ell_n) - f(x_0)}{t} - \frac{\partial f}{\partial x_N}(x_0) \right| \to 0
    \end{align*}
    if we set $t = x_N - x_{0N}$. So $f$ is differentiable at $x_0$.
\end{proof}
\newpage
\section{Differentiation Rules}
\thm{Chain Rule}{Let $(X, \Vert \cdot \Vert_X)$, $(Y, \Vert \cdot \Vert_Y)$, and $(Z, \Vert \cdot \Vert_Z)$ be normed spaces. Let $E \subseteq X$, $F \subseteq Y$, $f: E \to Y$, $g: F \to Z$. Assume $f(E) \subseteq F$ and $f$ is differentiable at $x_0 \in E$ and $g$ is differentiable at $f(x_0)$. Then $g \circ f$ is differentiable at $x_0$ and:
\begin{align*}
    d(g \circ f)(x_0) = dg(f(x_0)) \circ df(x_0).
\end{align*}
Moreover, if $x_0 \in \acc \{ x_0 + tv \in E : t \in \RR\}$, then 
\begin{align*}
    \frac{\partial (g \circ f)}{\partial v}(x_0) =d(g(f(x_0)))\left( \frac{\partial f}{\partial v}(x_0) \right).
\end{align*}}
\mlenma{}{Let $(X, \Vert \cdot \Vert_X)$ and $(Y, \Vert \cdot \Vert_Y)$ be normed spaces. Take $T: X \to Y$ linear and continuous. Then there exists a constant $M > 0$ such that $\Vert T(x) \Vert_Y \leq M \Vert x \Vert_X$ for all $x \in X$.}
\begin{proof}
    Let $\epsilon = 1$, $T$ continuous at $x=0$. By the definition of continuity, we can find $\delta > 0$ such that $\Vert T(x) \Vert_Y < \epsilon = 1$ if $\Vert x \Vert_X < \delta$. Take any $x \in X$ where $x \neq 0$. Then let $x_1 = \frac{\delta}{2} \frac{x}{\Vert x\Vert_X}$. So, $\Vert x_1 \Vert_X = \frac{\delta}{2}$. So $\Vert T(x_1) \Vert_Y < 1$. So,
    \begin{align*}
        \Vert T(x_1) \Vert_Y &= \Vert T\left( \frac{\delta}{2} \frac{x}{\Vert x\Vert_X} \right) \Vert_Y \\
        &= \frac{\delta}{2} \frac{\Vert T(x) \Vert_Y}{\Vert x \Vert_X} < 1 \\
        \implies \Vert T(x) \Vert_Y &< \frac{2}{\delta} \Vert x \Vert_X
    \end{align*}
    So we can take $M = \frac{2}{\delta}$ to complete the proof.
\end{proof}
\mlenma{}{Let $(X, \Vert \cdot \Vert_X)$ and $(Y, \Vert \cdot \Vert_Y)$ be normed spaces. Take $f: E \to Y$ and assume $f$ differentiable at some point $x_0 \in E$. Then there exists a constant $C > 0$ and $\delta >0$ such that $\Vert f(x) - f(x_0) \Vert_Y \leq C \Vert x - x_0 \Vert_X$ for all $x \in B(x_0, \delta)$.}
\begin{proof}
    $f$ is differentiable, so we know that $\lim_{x \to x_0} \frac{f(x) - f(x_0) -T(x - x_0)}{\Vert x - x_0 \Vert_X} = 0$ for some $T: X \to Y$ linear and continuous. Take $\epsilon = 1$. Find $\delta > 0$ such that 
    \begin{align*}
        \Vert f(x) - f(x_0) - T(x-x_0) \Vert_Y < \epsilon \Vert x-x_0 \Vert_X
    \end{align*}
    if $\Vert x - x_0 \Vert_X < \delta$. We have:
    \begin{align*}
        \Vert f(x) - f(x_0) \Vert_Y &\leq \Vert f(x) - f(x_0) - T(x-x_0) \Vert_Y + \Vert T(x-x_0) \Vert_Y \\
        &\leq \epsilon \Vert x-x_0 \Vert_X + M \Vert x-x_0 \Vert_X \\
        &= (M+1)\Vert x - x_0 \Vert_X
    \end{align*}
    as desired.
\end{proof}
\newpage
\noindent Now we actually prove the chain rule.
\begin{proof}
    We know there exists $T : X \to Y$ linear and continuous such that $\lim_{x \to x_0} \frac{f(x) - f(x_0) - T(x - x_0)}{\Vert x - x_0 \Vert_X} = 0$. We also know there exists $L: Y \to Z$ linear and continuous such that $\lim_{y \to f(x_0)} \frac{g(y) - g(f(x_0)) - L(y - f(x_0))}{\Vert y - f(x_0) \Vert_Y} = 0$. We want to show that $\lim_{x \to x_0} \frac{g(f(x)) - g(f(x_0)) - (L \circ T)(x - x_0)}{\Vert x - x_0 \Vert_X} = 0$. We start by analyzing the numerator:
    \begin{align*}
        &\Vert g(f(x)) - g(f(x_0)) - L(T(x - x_0))\Vert_Z \\ 
        &\leq \Vert g(f(x)) - g(f(x_0)) - L(f(x) - f(x_0)) \Vert_Z + \Vert L(f(x) - f(x_0)) -L(T(x - x_0)) \Vert_Z \\
        &= \Vert g(f(x)) - g(f(x_0)) - L(f(x) - f(x_0)) \Vert_Z + \Vert L(f(x) - f(x_0) - T(x - x_0)) \Vert_Z 
    \end{align*}
    We will study these quantities separately. 
    
    We start with the second quantity. By Lemma 1, there exists $M > 0$ such that $\Vert L(f(x) - f(x_0) - T(x - x_0)) \Vert_Z \leq M \Vert f(x) - f(x_0) - T(x - x_0) \Vert_Y$. Given $\epsilon > 0$, there exists $\delta_1 > 0$ such that $\Vert f(x) - f(x_0) - T(x - x_0) \Vert_Y < \epsilon \Vert x-x_0 \Vert_X$ if $0 < \Vert x - x_0 \Vert_X < \delta_1$. So,
    \begin{align*}
        \Vert L(f(x) - f(x_0) - T(x - x_0)) \Vert_Z &\leq M \Vert f(x) - f(x_0) - T(x - x_0) \Vert_Y \\
        \leq M \epsilon \Vert x - x_0 \Vert_X
    \end{align*}
    for all $x \in E$ with $0 < \Vert x - x_0 \Vert_X < \delta_1$.

    Now we study the first quantity. Given the same $\epsilon > 0$, there exists $\delta_2 > 0$ such that $\Vert g(y) - g(f(x_0)) - L(y - f(x_0)) \Vert_Z < \epsilon \Vert y - f(x_0) \Vert_Y$ if $0 < \Vert y - f(x_0) \Vert_Y < \delta_2$. We want to take $y = f(x)$ so we need to check that $0 < \Vert f(x) - f(x_0) \Vert_Y < \delta_2$. By Lemma 2, there exists $C > 0$ and $\delta_3 > 0$ such that $\Vert f(x) - f(x_0) \Vert_Y \leq C \Vert x - x_0 \Vert_X$ for all $x \in E$ such that $0 < \Vert x - x_0 \Vert_X < \delta_3$. So we can take $\delta = \min\{\delta_1, \delta_3, \delta_2/C\}$. So we have that we can take $y = f(x)$ because $\Vert f(x) - f(x_0) \Vert_Y < \delta_2$. So we have that:
    \begin{align*}
        \Vert g(f(x)) - g(f(x_0)) - L(f(x) - f(x_0)) \Vert_Z &< \epsilon \Vert f(x) - f(x_0) \Vert_Y \\
        &\leq \epsilon C \Vert x - x_0 \Vert_X
    \end{align*}
    for all $x \in E$ such that $0 < \Vert x - x_0 \Vert_X < \delta$. So we have that:
    \begin{align*}
        \Vert g(f(x)) - g(f(x_0)) - L(f(x) - f(x_0)) \Vert_Z + \Vert L(f(x) - f(x_0) - T(x - x_0)) \Vert_Z < \epsilon (M + C) \Vert x - x_0 \Vert_X
    \end{align*}
    Dividing by $\Vert x - x_0 \Vert_X$, we get that:
    \begin{align*}
        \frac{\Vert g(f(x)) - g(f(x_0)) - L(f(x) - f(x_0)) \Vert_Z + \Vert L(f(x) - f(x_0) - T(x - x_0)) \Vert_Z}{\Vert x - x_0 \Vert_X} < \epsilon (M + C)
    \end{align*}
    Now we prove the second part of the theorem. $f$ differentiable at $x_0$ implies that $\dfrac{\partial f}{\partial v}(x_0) = T(v)$. $g \circ f$ being differentiable at $x_0$ implies that $\dfrac{\partial (g \circ f)}{\partial v}(x_0) = L(T(v))$. So we have that:
    \begin{align*}
        L(T(v)) &= L\left( \frac{\partial f}{\partial v}(x_0) \right)
    \end{align*}
    as desired.
\end{proof}
\newpage
\thm{Quotient Rule}{Let $(X, \Vert \cdot \Vert)$ be a normed space and $f : E \to \RR^2$ with $f$ differentiable at $x_0 \in E$, with $f_2(x) \neq 0$ for all $x \in E$. Then $\frac{f_1}{f_2}$ is differentiable at $x_0$ and:
\begin{align*}
    \left( \frac{f_1}{f_2} \right)'(x_0) = \frac{f_2(x_0)f_1'(x_0) - f_1(x_0)f_2'(x_0)}{(f_2(x_0))^2}
\end{align*}}
\begin{proof}
    We can use the fact that if $g: \RR^2 \to \RR \ \{0\} \to \RR$ with $g(y_1, y_2) = \frac{y_1}{y_2}$, then:
    \begin{align*}
        \frac{\partial g}{\partial y_1}(y_1, y_2) &= \frac{1}{y_2} \\
        \frac{\partial g}{\partial y_2}(y_1, y_2) &= -\frac{y_1}{y_2^2}
    \end{align*}
    We apply chain rule. 
    \begin{align*}
        \frac{\partial}{\partial v}\left( \frac{f_1}{f_2}\right)(x_0) &= \frac{\partial}{\partial v}\left(g \circ f\right)(x_0) \\
        &= dg(f(x_0))\left( \frac{\partial f}{\partial v}(x_0) \right) \\
        &= \nabla g(f(x_0)) \cdot \frac{\partial f}{\partial v}(x_0) \\
        &= \frac{\partial g}{\partial y_1}(f_1(x_0), f_2(x_0))\frac{\partial f_1}{\partial v}(x_0) + \frac{\partial g}{\partial y_2}(f_1(x_0), f_2(x_0))\frac{\partial f_2}{\partial v}(x_0) \\
        &= \frac{1}{f_2(x_0)}f_1'(x_0) - \frac{f_1(x_0)}{(f_2(x_0))^2}f_2'(x_0) \\
        &= \frac{f_2(x_0)f_1'(x_0) - f_1(x_0)f_2'(x_0)}{(f_2(x_0))^2}
    \end{align*}
    as desired.
\end{proof}
\dfn{Jacobian Matrix}{Let $E \subseteq \RR^N$ and $f: E \to \RR^M$ and $H \subseteq \RR^M$ with $g: H\ to \RR^L$. Assume $f$ is differentiable at $x_0$ and $g$ is differentiable at $f(x_0)$. The Jacobian matrix of $f$ at $x_0 \in E$ is the $M \times N$ matrix:
\begin{align*}
    \begin{bmatrix} \nabla f_1(x_0) \\ \nabla f_2(x_0) \\ \vdots \\ \nabla f_M(x_0) 
    \end{bmatrix}
\end{align*}
and is denoted as $Jf(x_0)$.}
\newpage
\section{Higher Order Derivatives}

\dfn{Higher Order Derivative}{Take $E \subseteq \RR^N$ and $f: E \to \RR$. Assume there is $\dfrac{\partial f}{\partial x_i}: E \to \RR$. Take $j \in [1, N]$. Then,
\begin{align*}
    \frac{\partial^2 f}{\partial x_i \partial x_j}:= \frac{\partial}{\partial x_j}\left( \frac{\partial f}{\partial x_i} \right).
\end{align*}}
\noindent In general, we have that:
\begin{align*}
    \frac{\partial^2 f}{\partial x_i \partial x_j} \neq \frac{\partial^2 f}{\partial x_j \partial x_i}.
\end{align*}
\thm{Symmetry of Higher Order Derivatives}{Let $E \subseteq \RR^N$ and $f: E \to \RR$. Assume there exists $\dfrac{\partial f}{\partial x_i}, \dfrac{\partial f}{\partial x_j}, \dfrac{\partial^2 f}{\partial x_i \partial x_j}$ in $B(x_0, r) \subseteq E$. And assume that $\dfrac{\partial^2 f}{\partial x_j \partial x_i}$ is continuous at $x_0$. Then $\dfrac{\partial^2 f}{\partial x_i \partial x_j} = \dfrac{\partial ^2 f}{\partial x_j \partial x_i}$.}
\mlenma{}{Let $A: ((-r, r) \setminus \{0\})^2 \to \RR$. Assume that there exists:
\begin{enumerate}
    \item $\lim_{(s, t) \to (0, 0)} A(s, t) = \ell \in \RR$. 
    \item For every $s \in (-r, r) \setminus \{0\}$,
    \begin{align*}
        \exists \lim_{t \to 0} A(s, t) \in \RR.
    \end{align*}
\end{enumerate}
Then, there is 
\begin{align*}
    \lim_{s \to 0} \left( \lim_{t \to 0} A(s, t) \right) = \lim_{(s, t) \to (0, 0)} A(s, t) = \ell.
\end{align*}}
\begin{proof}
    For every $\epsilon > 0$, there exists $0< \delta < r$ such that \begin{align*}
        |A(s, t) - \ell| < \epsilon
    \end{align*}
    for all $s, t$ with $0 < \sqrt{s^2 + t^2} < \delta$. So, we have:
    \begin{align*}
        \ell - \epsilon \leq A(s, t) \leq \ell + \epsilon.
    \end{align*}
    Fix $s \in (-\delta, \delta) \setminus \{0\}$. By the second condition, we have that:
    \begin{align*}
        \lim_{t \to 0} A(s, t) = \ell_s
    \end{align*}
    for some $\ell_s$. So take $t \to 0$ in the inequality above to get:
    \begin{align*}
        \ell - \epsilon \leq \ell_s \leq \ell + \epsilon
    \end{align*}
    for the same choice of $\delta$. So, 
    \begin{align*}
        \left| \ell_s - \ell \right| < \epsilon
    \end{align*}
    for all $0 < |s| < \delta$. So $\lim_{s \to 0} \ell_s = \ell$, as desired.
\end{proof}
\newpage 
\noindent Now we turn to the proof of the symmetry of higher order derivatives.
\begin{proof}
    Step 1: Let $N = 2$. Let $\vec{x}_0 = (x_0, y_0)$. Consider the rectangle $R$ determined by the corners $(x_0, y_0), (x_0 + s, y_0), (x_0, y_0 + t), (x_0 + s, y_0 + t)$ and assume $R \subseteq B(\vec{x}_0, r)$. Let 
    \begin{align*}
        A(s, t) = \frac{f(x_0 + s, y_0 + t) - f(x_0 + s, y_0) - f(x_0, y_0 + t) + f(x_0, y_0)}{st}.
    \end{align*}
    Fix $t$, move $s$. Consider the following functions
    \begin{align*}
        g(x) = f(x, y_0 + t) - f(x, y_0)
    \end{align*}
    We attempt to write $A(s, t)$ in terms of $g$:
    \begin{align*}
        A(s, t) &= \frac{f(x_0 + s, y_0 + t) - f(x_0 + s, y_0) - f(x_0, y_0 + t) + f(x_0, y_0)}{st} \\
        &= \frac{g(x_0 + s) - g(x_0)}{st} \\
        &= \frac{g(x_0 + s) - g(x_0)}{s} \cdot \frac{1}{t}
    \end{align*}
    By MVT, there exists $c_s$ between $x_0$ and $x_0 + s$ such that:
    \begin{align*}
        g(x_0 + s) - g(x_0) = sg'(c_s)
    \end{align*}
    So we have that:
    \begin{align*}
        A(s, t) = \frac{g'(c_s)}{t}
    \end{align*}
    Consider $g'(x) = \frac{\partial f}{\partial x}(x, y_0 + t) - \frac{\partial f}{\partial x}(x, y_0)$. So we have that:
    \begin{align*}
        A(s, t) = \frac{ \frac{\partial f}{\partial x}(c_s, y_0 + t) -  \frac{\partial f}{\partial x}c_s, y_0) }{t}
    \end{align*}
    Consider the function:
    \begin{align*}
        h(y) = \frac{\partial f}{\partial x}(c_s, y)
    \end{align*}
    So, 
    \begin{align*}
        A(s, t) = \frac{h(y_0 + t) - h(y_0)}{t} = h'(c_t) = \frac{\partial^2 f}{\partial y \partial x}(c_s, c_t)
    \end{align*}
    where $c_t$ is between $y_0$ and $y_0 + t$. As $(s,t) \to (0, 0)$, we have that $c_s \to x_0$ and $c_t \to y_0$. So we have that:
    \begin{align*}
        \lim_{(s,t) \to (0, 0)} A(s, t) = \lim_{(s,t) \to (0, 0)} \frac{\partial^2 f}{\partial y \partial x}(c_s, c_t) = \frac{\partial^2 f}{\partial y \partial x}(x_0, y_0)
    \end{align*}
    Now we consider
    \begin{align*}
        \lim_{t \to 0} A(s, t) &= \frac{1}{s} \lim_{t \to 0} \frac{f(x_0 + s, y_0 + t) - f(x_0 + s, y_0)}{t}  - \frac{f(x_0, y_0 + t) - f(x_0, y_0)}{t} \\
        &= \frac{1}{s} \left(\frac{\partial f}{\partial y}(x_0 + s, y_0) - \frac{\partial f}{\partial y}(x_0, y_0)\right)
    \end{align*}
    So now the preconditions for the lemma are satisfied. So we have that:
    \begin{align*}
        \lim_{(s, t) \to (0, 0)} A(s, t) = \lim_{s \to 0} \left( \lim_{t \to 0} A(s, t) \right) &= \lim_{s\to 0} \frac{\frac{\partial f}{\partial y}(x_0 + s, y_0) - \frac{\partial f}{\partial y}(x_0, y_0)}{s} \\
        &= \frac{\partial^2 f}{\partial x \partial y}(x_0, y_0)
    \end{align*}
    But we also have that:
    \begin{align*}
        \lim_{(s, t) \to (0, 0)}A(s, t) = \frac{\partial^2 f}{\partial y \partial x}(x_0, y_0)
    \end{align*}
    So we have that $\frac{\partial^2 f}{\partial x \partial y}(x_0, y_0) = \frac{\partial^2 f}{\partial y \partial x}(x_0, y_0)$ as desired.

    Now consider the case where $N \geq 2$. Say $1 < i < j < N$. Then realize our function looks like:
    \begin{align*}
        F(x_i, x_j) := f(x_1, x_2, \cdots, x_i, \cdots, x_j, \cdots, x_N)
    \end{align*}
    which is a function of two variables. So we can apply the previous case to get that:
    \begin{align*}
        \frac{\partial^2 F}{\partial x_i \partial x_j} = \frac{\partial^2 F}{\partial x_j \partial x_i}
    \end{align*}
    as desired.
\end{proof}
\newpage
\section{Taylor Series}
\thm{Taylor's Formula}{Let $f: (a, b) \to \RR$ be $k$ times differentiable for $k \in \NN$ with $x_0 \in (a, b)$. Then:
\begin{align*}
    f(x) = f(x_0) + f'(x_0)(x-x_0) + \cdots + \frac{f^{(k)}(x_0)}{k!}(x-x_0)^k + R_k(x)
\end{align*}
where $R_k(x) = \frac{f^{(k-1)}(c)}{(k+1)!}(x - x_0)^{k+1}$ is the error term for some $c$ between $x$ and $x_0$.

Moreover, if there exists $M>0$ such that $|f^{(k+1)}(x)| \leq M$ for every $x \in(a, b)$, then 
\begin{align*}
    |R_k(x)| \leq \frac{M}{(k+1)!}|x-x_0|^{k+1},
\end{align*}
which tends to zero faster than $(x-x_0)^{k+1}$ as $x \to x_0$. That is, 
\begin{align*}
    \lim_{x \to x_0} \frac{R_k(x)}{(x-x_0)^{k+1}} = 0.
\end{align*}
Or,
\begin{align*}
    R_k(x) = o((x-x_0)^{k+1}).
\end{align*}} 
\ex{$\log(x+1)$}{
    We have that $f(x) = \log(x+1)$, $f'(x) = \dfrac{1}{x+1}$, $f''(x) = -\dfrac{1}{(x+1)^2}$. Consider $x_0 = 0$ and $(a,b) = (-1/2, 1/2)$. Then, we have that the $k=1$ exapansion is:
    \begin{align*}
        f(x) &= f(0)  + f'(0)(x-0) + o(x-0)\\
        &= 0 + 1x + o(x) \\
    \end{align*}
    Dividing both sides by $x$, we get that:
    \begin{align*}
        \frac{\log(1+x)}{x} &= \frac{x + o(x)}{x} \\
        &= 1 + \cancelto{0}{o(1)}
    \end{align*}
}
\noindent Now we prove Taylor's Formula.
\begin{proof}
    Fix $x \in (a, b)$. Consider:
    \begin{align*}
        g(t) = f(x) - \left[f(t) + f'(t)(x-t) + \cdots + \frac{f^{(k)}(t)}{k!}(x-t)^k\right] - R_k(x) \frac{(x-t)^{k+1}}{(x-x_0)^{k+1}}
    \end{align*}
    Let $t \in (x_0, x)$. Then,
    \begin{align*}
        g(x) &= f(x) - f(x) + 0 + \cdots + 0 = 0 \\
        g(x_0) &= R_k(x) - R_k(x) = 0
    \end{align*}
    So, we can apply Rolle's Theorem to get that there exists $c$ between $x$ and $x_0$ such that $g'(c) = 0$. So we have that:
    \begin{align*}
        0 &= \cancelto{0}{f'(c)} - f'(c) - f''(c)(x-c) - \cdots - \frac{f^{(k)}(c)}{k!}k(x-c)^{k-1} - R_k(x) \frac{(k+1)(x-c)^k}{(x-x_0)^{k+1}} \\
        &= -f'(c) - \frac{f^{(k+1)}(c)}{k!}(x-c)^k + f'(c) + R_k(x) \frac{(k+1)(x-c)^k}{(x-x_0)^{k+1}} \\
        &= -\dfrac{f^{(k+1)}(c)}{k!} (x-c)^k + R_k(x) \frac{(k+1)(x-c)^k}{(x-x_0)^{k+1}}
    \end{align*}
    So, 
    \begin{align*}
        \frac{f^{(k+1)}(c)}{k!} &= R_k(x) \frac{k+1}{(x-x_0)^{k+1}} 
        \implies R_k(x) = \frac{f^{(k+1)}(c)}{(k+1)!}(x-x_0)^{k+1}
    \end{align*}
    as desired.
\end{proof}
\cor{}{Let $f:(a, b) \to \RR$ be $k$ times differentiable with $f', \cdots, f^{(k)}$ continuous. Then, for every $x, x_0 \in (a, b)$,
\begin{align*}
    f(x) = f(x_0) + \sum_{n=1}^k \frac{1}{n!}f^{(n)}(x_0)(x-x_0)^n + o((x-x_0)^k).
\end{align*}}
\begin{proof}
    Apply Taylor's Formula with $k$ to get:
    \begin{align*}
        f(x) &= f(x_0) + \sum_{n=1}^{k-1} \frac{1}{n!}f^{(n)}(x_0)(x-x_0)^n + \frac{f^{(k)}(c)}{k!}(x-x_0)^k \\
        &= f(x_0) + \sum_{n=1}^{k} \frac{1}{n!}f^{(n)}(x_0)(x-x_0)^n + [f^{(k)}(c) - f^{(k)}(x_0)]\frac{(x-x_0)^k}{k!}
    \end{align*}
\end{proof}
\dfn{Multi-Index}{
A multi-index is a vector $\alpha \in \NN_0$, $\alpha  = (\alpha_1, \cdots, \alpha_N)$. The length of $\alpha$ is $|\alpha| = \alpha_1 + \cdots + \alpha_N$. We define $\alpha! = \alpha_1! \cdots \alpha_N!$ and $x^\alpha = x_1^{\alpha_1} \cdots x_N^{\alpha_N}$. We define the derivative of $f$ with respect to $\alpha$ as:
\begin{align*}
    \frac{\partial^\alpha}{\partial x^\alpha} = \frac{\partial^{\alpha_1}}{\partial x_1^{\alpha_1}} \cdots \frac{\partial^{\alpha_N}}{\partial x_N^{\alpha_N}}.
\end{align*}
}
\ex{}{Let $N=3$ and $\alpha = (2, 0, 3)$. Then, $(x, y, z)^\alpha = x^2z^3$. So, $\frac{\partial^\alpha}{\partial x^\alpha} = \frac{\partial^2}{\partial x^2} \frac{\partial^3}{\partial z^3}$.}
\dfn{Class}{Let $U \subseteq \RR^N$ open, $k \in \NN$, and $f: U \to \RR$. We say $f$ is of class $C^k$ if all partial derivatives of order less than or equal to $k$ exist and are continuous.}
If $f \in C^k(V)$, $k \geq 2$, you can apply theorem about symmetry of derivatives. 
\ex{}{Let $f \in C^5(k)$ and consider the the domain as $\RR^3$. Then, we have that:
\begin{align*}
    \frac{\partial^2}{\partial y^2} \left(\frac{\partial}{\partial x}\left(\frac{\partial}{\partial z} f \right) \right) = \frac{\partial}{\partial x } \left(\frac{\partial^2}{\partial y^2}\left(\frac{\partial }{\partial z} f \right) \right)
\end{align*}}
\thm{Taylor's Formula}{Let $V \in \RR^N$ open, $k \in \NN$. $f: V \to \R$ of class $C^k$. Let $x_0 \in V$. Then:
\begin{align*}
    f(x) = f(x_0) + \sum_{|\alpha| = 1}^k \frac{\partial^\alpha f}{\partial x^\alpha}(x_0) \frac{(x - x_0)^\alpha}{\alpha!} + o(\Vert x-x_0 \Vert^k)
\end{align*}
for $x$ near $x_0$. 
}
\nt{If $k = 1$, 
\begin{align*}
    f(x) &= f(x_0) + \sum_{|\alpha| = 1} \frac{1}{\alpha !}\frac{\partial^\alpha f(x_0)}{\partial x^\alpha}(x-x_0)^\alpha + o(\Vert x-x_0 \Vert) \\
    &= f(x_0) + \sum_{i = 1}^N \frac{\partial f}{\partial x_i}(x_0)(x_i - x_{0i}) + o(\Vert x-x_0 \Vert) \\
    &= f(x_0) + \nabla f(x_0) \cdot (x-x_0) + o(\Vert x-x_0 \Vert).
\end{align*}
So, 
\begin{align*}
    \frac{f(x) - f(x_0) - \nabla f(x_0)\cdot(x-x_0)}{\Vert x-x_0 \Vert} = \frac{o(\Vert x - x_0 \Vert )}{\Vert x - x_0 \Vert} \to 0.
\end{align*}
This is differentiability at $x_0$.}

\ex{Multinomial Theorem}{Let $x \in \RR^N$, $n \in \NN$. Prove that:
\begin{align*}
    (x_1 + x_2 + \cdots + x_N)^n = \sum_{|\alpha| = n} \frac{x^\alpha n!}{\alpha!}
\end{align*}}
\ex{}{$V \in \RR^N$ open, $f:  V \to \RR$ of class $C^k$. $v \in \RR^N$. Define:
\begin{align*}
    v \cdot \nabla &= v_1 \frac{\partial}{\partial x_1} + \cdots + v_N \frac{\partial}{\partial x_N} \\
    (v \cdot \nabla)^n &= (v \cdot \nabla)(v \cdot \nabla)^{n} \\
    &=\sum_{|\alpha| = n} v^\alpha \frac{n!}{\alpha!} \frac{\partial^\alpha}{\partial x^\alpha}
\end{align*}}

\noindent Now we prove the theorem.
\begin{proof}
    Let $\Vert v \Vert = 1$, and consider $x_0 + tv$ for $t \in \RR$. Define $g(t) = f(x_0 + tv)$. We have that $$g'(t) = \nabla f(x_0 + tv) \cdot v = (v \cdot \nabla)f(x_0 + tv)$$ $$g''(t) = (v \cdot \nabla)^2 f(x_0 + tv)$$
    and so on. So we have:
    \begin{align*}
        g^{(n)}(t) &= (v \cdot \nabla)^n f(x_0 + tv) \\
        &= (v_1 \frac{\partial}{\partial x_1} + \cdots + v_N \frac{\partial}{\partial x_N})^n f(x_0 + tv) \\
        &= \sum_{|\alpha| = n} v^\alpha \frac{n!}{\alpha!} \frac{\partial^\alpha f}{\partial x^\alpha}(x_0 + tv)
    \end{align*}
    Apply Taylor's formula to $g$. We have that:
    \begin{align*}
        g(t) &= g(0) + g'(0)t + \cdots + \frac{g^{(n)}(0)}{n!}t^n + \cdots + \frac{g^{(k)}(0)}{k!}t^k + R_k(t)
    \end{align*}
    where $R_k(t) = \left( \dfrac{g^{(k)}(c)}{k!} - \dfrac{g^{(k)}(0)}{k!} \right)t^k$. So then,
    \begin{align*}
        f(x_0 + tv) &= f(x_0) + (v \cdot \nabla)f(x_0 + tv)t + \cdots + \sum_{|\alpha| = k} \frac{k!}{\alpha!}v^\alpha \frac{\partial^\alpha f}{\partial x^\alpha}(x_0) \frac{1}{k!} t^k + \sum_{|\alpha| = k}\frac{k!}{\alpha!}v^\alpha \left( \frac{\partial^\alpha f}{\partial x^\alpha}(x_0 + cv) - \frac{\partial^\alpha f}{\partial x^\alpha}(x_0) \right) \frac{1}{k!} t^k
    \end{align*}
    The $k!$ cancels out, and we are now ready to choose $t$ and $v$. We choose $v = \dfrac{x - x_0}{\Vert x - x_0 \Vert}$ and $t = \Vert x - x_0 \Vert$. So we have that:
    \begin{align*}
        x_0 + tv = x
    \end{align*}
    So, 
    \begin{align*}
        f(x) = f(x_0) + \cdots + \sum_{|\alpha| = k} \frac{1}{\alpha!} \frac{\partial^\alpha f}{\partial x^\alpha}(x_0) \frac{(x-x_0)^\alpha}{\Vert x - x_0 \Vert^k} \Vert x - x_0 \Vert^k + \sum_{|\alpha| = k} \frac{1}{\alpha!} \left(\frac{\partial^\alpha f}{\partial x^\alpha}(x_0 + cv)  - \frac{\partial^\alpha f(x_0)}{\partial x^\alpha}\right)(x - x_0)^\alpha 
    \end{align*}
    Cancelling out the terms, we get the desired result.
\end{proof}
\newpage
\section{Local Min and Local Max}
\dfn{Local Min and Local Max}{Let $(X, d)$ be a metric space, $E \subseteq X$, and $f: X \to \RR$. 
\begin{enumerate}
    \item $f$ has a \emph{local maximum} at $x_0 \in E$ if $f(x) \leq f(x_0)$ for all $x \in E \cap B(x_0, r)$ for some $r > 0$.
    \item $f$ has a \emph{local minimum} at $x_0 \in E$ if $f(x) \geq f(x_0)$ for all $x \in E \cap B(x_0, r)$ for some $r > 0$.
\end{enumerate}}

\thm{}{Let $(X , \Vert \cdot \Vert)$ be a normed space, $f : E \to \RR$. Assume $f$ has a local maximum at $x_0 \in E^\circ$ and $f$ is differentiable at $x_0$. Then $d f(x_0) = 0$.}

\begin{proof}
    Let $\Vert v \Vert = 1$. Take $g(t) = f(x_0 + tv)$. We know that $g(t) \leq g(0)$ for all $|t| < r$. (Proof of Rolle's Theorem) $\implies g'(0) = 0$. So, $g'(t) = df(x_0 + tv)(v)$, and $g'(0) = df(x_0)(v) = \dfrac{\partial f}{\partial v}(x_0) \implies df(x_0) = 0$. 
\end{proof}
\dfn{Hessian}{Let $E \subseteq \RR^N$ and $f: E \to \RR$. Define $Hf(x_0)$ as the Hessian matrix of $f$ and $x_0$ as:
\begin{align*}
    Hf(x_0) = \begin{pmatrix} \frac{\partial^2 f}{\partial x_1^2}(x_0) & \cdots & \frac{\partial^2}{\partial x_N \partial x_1}(x_0) \\ 
    \frac{\partial^2 f}{\partial x_1 \partial x_2}(x_0) & \cdots & \frac{\partial^2}{\partial x_N \partial x_2} \\ 
    \vdots & \ddots & \vdots \\
    \frac{\partial^2 f}{\partial x_1 \partial x_N}(x_0) & \cdots & \frac{\partial^2 f}{\partial x_N^2}(x_0) \end{pmatrix}
\end{align*}
This is an $N \times N$ matrix.}
\noindent If $f \in C^2$, then $Hf(x_0)$ is symmetric. A $N \times N$ symmetric matrix implies that the eigenvalues are real. 

\exer{}{Take $p(t) = t^N + a_{N-1}t^{N-1} + \cdots + a_1t + a_0$. 
\begin{enumerate}
    \item All the roots are positive $\iff$ the coefficients switch signs.
    \item All the roots are negative $\iff$ all the coefficients are positive.
\end{enumerate}}
\newpage
\thm{}{Let $U \subseteq \RR^N$ open and $f: U \to \RR$ of class $C^2$. Assume $x_0 \in U$ and that $\nabla f(x_0) = 0$. Let $\lambda_1, \lambda_2, \ldots, \lambda_N$ be the eigenvalues of $Hf(x_0)$. Then:
\begin{enumerate}
    \item If $\lambda_1, \lambda_2, \ldots, \lambda_N > 0$, then $f$ has a local minimum at $x_0$.
    \item If $f$ has a local minimum at $x_0$, then $\lambda_1, \lambda_2, \ldots, \lambda_N \geq 0$.
    \item If $\lambda_1, \lambda_2, \ldots, \lambda_N < 0$, then $f$ has a local maximum at $x_0$.
    \item If $f$ has a local maximum at $x_0$, then $\lambda_1, \lambda_2, \ldots, \lambda_N \leq 0$.
\end{enumerate}}
\mlenma{}{If $H = (h_{i, j})_{i, j = 1}^N$ is an $N \times N$ symmetric matrix and all the eigenvalues are positive, then $\sum_{i, j = 1}^N h_{i, j}x_ix_j \geq c \Vert x \Vert^2$ for all $x \in \RR^N$ and some $c > 0$. }
\begin{proof}
    Let $x \in \RR^N$. Treat $x$ as a $1 \times N$ matrix. We can show:
    \begin{align*}
        xHx^T = \sum_{i, j = 1}^N h_{i, j}x_ix_j.
    \end{align*}
    Let $\lambda_1, \lambda_2, \ldots, \lambda_N \in \RR$ be the eigenvalues of $H$. Let $e_1, e_2, \ldots, e_N$ be the corresponding eigenvectors. We can assume that they form an orthonormal basis for $\RR^N$. So $x \in \RR^N$ can be written as a linear combination of the eigenvectors:
    \begin{align*}
        x = \sum_{i = 1}^N c_i e_i.
    \end{align*}
    So,
    \begin{align*}
        H x^T &= \sum_{i=1}^N c_i H e_i^T \\ 
        &= \sum_{i=1}^N c_i \lambda_i e_i^T 
    \end{align*}
    So, we have that:
    \begin{align*}
        xHx^T &= \sum_{j=1}^n c_j e_j \sum_{i=1}^N c_i \lambda_i e_i^T \\
        &= \sum_{i, j = 1}^N c_i c_j \lambda_i e_j^T e_i \\
        &= \sum_{i=1}^N c_i^2 \lambda_i \\
        &\geq c \sum_{i=1}^N c_i^2 \tag{$c = \min \{\lambda_1, \lambda_2, \ldots, \lambda_N\}$} \\
        &= c \Vert x \Vert^2 \tag{by orthonormality}
    \end{align*}
\end{proof}
\noindent We move to the proof of the actual theorem.
\begin{proof}
    We start by proving 1.

    \noindent Assume $\lambda_1, \lambda_2, \ldots, \lambda_N > 0$. We know that $Hf(x_0)$ is symmetric. So, we can apply the lemma to get that:
    \begin{align*}
        y Hf(x_0) y^T =\sum_{i, j=1}^N \frac{\partial^2 f}{\partial x_i \partial x_j}(x_0)y_iy_j \geq c \Vert y \Vert^2
    \end{align*}
    for all $y \in \RR^N$ and some $c > 0$. Apply Taylor's formula to $f$:
    \begin{align*}
        f(x) = f(x_0) + \sum_{|\alpha| = 1}^2 \frac{1}{\alpha !}\frac{\partial^\alpha f}{\partial x^\alpha}(x_0)(x-x_0)^\alpha + o(\Vert x - x_0 \Vert^2)
    \end{align*}
    Since the gradient is zero, we know that:
    \begin{align*}
        \frac{\partial f}{\partial x_i}(x_0) = 0
    \end{align*}
    for all $i$. So, we have that:
    \begin{align*}
        f(x) &= f(x_0) + \sum_{|\alpha| = 2}^2 \frac{1}{\alpha !}\frac{\partial^\alpha f}{\partial x^\alpha}(x_0)(x-x_0)^\alpha + o(\Vert x - x_0 \Vert^2) \\
        &= f(x_0) + \frac{1}{2}\sum_{i , j = 1}^N \frac{\partial^2f}{\partial x_i x_j}(x_0)(x_i - x_{0i})(x_j - x_{0j}) + o(\Vert x - x_0 \Vert^2) \\
        &\geq f(x_0) + \frac{c}{2}\Vert x - x_0 \Vert^2 + o(\Vert x - x_0 \Vert^2) \tag{by result from lemma}\\
        &= f(x_0) + \Vert x - x_0 \Vert^2 \left( \frac{c}{2} + \frac{o(\Vert x - x_0 \Vert^2)}{\Vert x -x_0 \Vert^2} \right) \\
        &\geq f(x_0) + \Vert x - x_0 \Vert^2 \left( \frac{c}{2} - \frac{c}{4}\right) \tag{take $\epsilon = \frac{c}{4}$} \\
        &\implies f(x) > f(x_0)
    \end{align*}
    for $\Vert x - x_0 \Vert < \delta$. So, $f$ has a local minimum at $x_0$.

    \noindent Now we prove 2. Assume $f$ has a local minimum at $x_0$. That is $f(x) \geq f(x_0)$ for all $x \in B(x_0, r) \subseteq U$ for some $r  > 0$. We want to show that $\lambda_1, \lambda_2, \ldots, \lambda_N \geq 0$. So BWOC, assume $\lambda_j < 0$. Take $x = x_0 + te_j$ for $t > 0$ where $e_j$ is $\lambda_j$'s eigenvector. Apply Taylor's formula:
    \begin{align*}
        f(x_0 + te_j) &= f(x_0) + \frac{1}{2}(x_0 + te_j - x_0)Hf(x_0)(x_0 + te_j - x_0)^T + o(t^2) \\
        &= f(x_0) + \frac{1}{2}\lambda_j t^2 \cancelto{1}{\Vert e_j \Vert^2} + o(t^2) \\
        &= f(x_0) t^2 \left( \frac{\lambda_j}{2} + \frac{o(t^2)}{t^2} \right) \\
        &\leq f(x_0) + t^2 \left( \frac{\lambda_j}{2} + \frac{\lambda_j}{4} \right) \tag{$\epsilon = -\frac{\lambda_j}{4}$}\\
        &\implies f(x_0 + te_j) < f(x_0)
    \end{align*}
    for $t$ small enough. So, $f$ does not have a local minimum at $x_0$. So, $\lambda_1, \lambda_2, \ldots, \lambda_N \geq 0$ by contradiction.
\end{proof}
\newpage
\section{Implicit Functions}
Let $x \in \RR^N$ and $y \in \RR^M$ and $E \subseteq \RR^N \times \RR^M$. $f: E \to \RR^M$, $f = (f_1, f_2, \ldots, f_M)$.
\begin{align*}
    \frac{\partial f}{\partial x}(x, y) = \begin{pmatrix} \frac{\partial f_1}{\partial x_1} & \frac{\partial f_1}{\partial x_2} & \cdots & \frac{\partial f_1}{\partial x_M} \\ \vdots & \vdots &\cdots &\vdots \\ \frac{\partial f_M}{\partial x_1} & \cdots & \cdots & \frac{\partial f_M}{\partial x_N}\end{pmatrix},
\end{align*}
which is an $M \times N$ matrix. Similarly for $y$, 
\begin{align*}
    \frac{\partial f}{\partial y}(x, y) = \begin{pmatrix} \frac{\partial f_1}{\partial y_1} & \frac{\partial f_1}{\partial y_2} & \cdots & \frac{\partial f_1}{\partial y_M} \\ \vdots & \vdots & \cdots &\vdots \\ \frac{\partial f_M}{\partial y_1} & \cdots & \cdots & \frac{\partial f_M}{\partial y_M}\end{pmatrix},
\end{align*}
which is an $M \times M$ matrix (can find the determinant). Now we state the theorem.
\thm{Implicit Function Theorem}{ Let $U \subseteq \RR^N \times \RR^M$ be an open set and $f: U \to \RR^M$ of class $C^k$. Let $(a, b) \in U$ such that $f(a, b) = 0$. Assume that $\det \frac{\partial f}{\partial y}(a, b) \neq 0$. Then, $r_1 > 0, r_2 > 0$ and a unique function $g: B(a, r_1) \to B(b, r_2)$ of class $C^k$ such that $g(a) = b$, $f(x, g(x)) = 0$ for all $x \in B(a, r_1)$ and $B(a, r_1) \times B(b, r_2) \subseteq U$.}
\begin{proof}
    We start with $M=1$. This means that $y \in \RR$. So, $f : U \to \RR$. Assume $\frac{\partial f}{\partial y}(a, b) > 0$, $f \in C^k$. So we know that $\frac{\partial f}{\partial y}$ is continuous at the point $(a, b)$. As such, we can find a ball centered at $(a, b)$ such that $\frac{\partial f}{\partial y} > 0$ for all points in the ball. Consider $B(a, r) \times [b - r, b + r] \subseteq$ our ball. Consider $h(y) = f(a, y)$. $h'(y) = \frac{\partial f}{\partial y}(a, y) > 0$, meaning that $h$ is strictly increasing and $h(b) = 0$. So, $h(b + r) = f(a, b-r)  > 0$, and $h(b - r) = f(a, b-r) < 0$. $f$ is also continuous at $(a, b+r)$ and $(a, b-r)$. 
    
    As such, we can find $r_3 > 0$ such that $f > 0$ in a ball $B((a, b+r), r_3)$. Additionally, we can find $r_4 > 0$ such that $f < 0$ in a ball $B((a, b-r), r_4)$. Let $r_5 = \min(r_3, r_4)$. Let $x \in B(a, r_5)$. Let $k(y) = f(x, y)$. Then $k'(y) = \frac{\partial f}{\partial y}(x, y) > 0$. $k(b + r) > 0$ and $k(b-r) < 0$. So, $\exists y$ such that $k(y) = 0$ that is unique since $k$ is strictly increasing. 

    So far, we have proved that for every $x \in B(a, r_5)$, there exists a unique $y \in [r-b, r+b]$ such that $f(x, y) = 0$. Define $g(x) = y \implies f(x, g(x)) = 0$. 

    Step 2: We claim that $g$ is continuous. Let $x_0 \in B(a, r_5)$. Fix small $\epsilon > 0$. Call $p(y) = f(x_0 , y)$ which is strictly increasing. $p(g(x_0)) = f(x_0, g(x_0)) = 0$. So, $p(g(x_0) + \epsilon) > 0$ and $p(g(x_0) - \epsilon) < 0$. $f$ continuous at $(x_0, g(x_0) + \epsilon)$, so $f > 0$ at a ball centered at $x(, g(x_0) + \epsilon)$. Similarly, $f < 0$ at a ball centered at $(x_0, g(x_0) - \epsilon)$. So, $f(x, g(x)) = 0$ for all $x$ in a ball centered at $x_0$. Define $r_6$ for the former and $r_7$ for the latter. Let $r_8 = \min(r_6, r_7)$. Let $x \in B(x_0, r_8)$.

    We know that $f(x, g(x_0) + \epsilon) > 0$ and $f(x, g(x_0) - \epsilon) < 0$. Also, $f(x, g(x)) = 0 \implies g(x_0) - \epsilon < g(x) < g(x_0) + \epsilon$ as desired.

    Step 3: Prove that $g$ is differentiable. So, let $x, x_0 \in B(a, r)$, $y_0, y \in (b-r, b+r)$. Consider the fubnction $q(s) = f(s,(x, y) + (1-s)(x_0, y_0)$) for $s\in[0, 1]$. By the mean value theorem, $q(1) - q(0) = g'(c)(1)$ for some $c \in (0, 1)$. Meaning, \begin{align*}
        f(x, y) - f(x_0, y_0) &= \sum_{i=1}^N \frac{\partial f}{\partial x_i}(c(x, y) + (1-c)(x_0, y_0))(x_i - x_{0i}) + \frac{\partial f}{\partial y}(c(x, y) + (1-c)(x_0, y_0))(y - y_0).
    \end{align*}
    Take $(x_0, y_0) = (x_0, g(x_0))$ and $(x, y) = (x_0 + te_k, g(x_0 + te_k))$. So, we have that: 
    \begin{align*}
        f(x_0 + te_k, g(x_0 + te_k)) - f(x_0, g(x_0)) &= 0 \\
        &= 0 + 0 + \cdots + \frac{\partial f}{\partial x_k}(\overline x, \overline y)(x_{0k} + t  - x_{0k}) + \frac{\partial f}{\partial y}(\overline x , \overline y)(g(x_0 + te_k) - g(x_0)) \\
        &= \frac{\partial f}{\partial x_k}(\overline x, \overline y)t + \frac{\partial f}{\partial y}(\overline x, \overline y)(g(x_0 + te_k) - g(x_0)) \\
        \implies \frac{g(x_0 + te_k) - g(x_0)}{t} &= -\frac{\frac{\partial f}{\partial x_k}(\overline x, \overline y)}{\frac{\partial f}{\partial y}(\overline x, \overline y)}
    \end{align*}
    Now take $t \to 0$ to get:
    \begin{align*}
        \frac{\partial g}{\partial x_k}(x_0) &= -\frac{\frac{\partial f}{\partial x_k}(x_0, g(x_0))}{\frac{\partial f}{\partial y}(x_0, g(x_0))}
    \end{align*}
    for every $x_0 \in B(a, r)$. So, since the partials of $f$ are continuous and $g$ is continuous, the partials of $g$ are continuous and therefore $g$ is differentiable. 
\end{proof}
We now use this result to prove the inverse function theorem.
\thm{Inverse Function Theorem}{Let $U \subseteq \RR^N$ be open and $f: U \to \RR^N$ of class $C^k$. Assume $a \in U$ and that $\det{} Jf(a) \neq 0$. Then there is $r > 0$ such that $f: B(a, r) \to \RR^N$ is one-to-one, $f^{-1}: f(B(a, r)) \to \RR^N$ is of class $C^k$ and $f^{-1}$. 

Also, $f(B(a, r))$ is open.}
\begin{proof}
    Define $h(x, y) = f(x) - y$. Let $b = f(a)$. So, $h(a, b) = 0$. Then, 
    \begin{align*}
        \frac{\partial h}{\partial x}(x, y) &= \frac{\partial f}{\partial x}(x) \\
        \det \frac{\partial h}{\partial x}(x, y) &= \det \frac{\partial f}{\partial x}(x) \neq 0 \tag{by hypothesis}
    \end{align*}
    So, we can apply the implicit function theorem to find a function $g: B(b, r_1) \to B(a, r_2)$, $g(b) = a$, of class $C^k$ such that $h(g(y), y) = 0$. So, $f(g(y)) = y$. 

    Let $V = B(a, r_2) \cap f^{-1}(B(b, r_1))$ (pre-image). So $f(V) = B(b, r_1)$. So $f: V \to B(b, r_1)$ is onto. We want to show that it is injective as well. Assume $\exists x_1, x_2 \in V$ such that $f(x_1) = f(x_2) = y \in B(b, r_1)$. But the implicit function theorem tells us that there is a unique $x \in B(a, r_2)$ such that $h(x, y) = 0$. So, $x_1 = x_2$. So, $g = f^{-1}$ is of class $C^k$. 
\end{proof}
\newpage 
\section{Lagrange Multipliers}
\dfn{Constrained Extrema}{Let $f: E \to \RR$, where $E \subseteq \RR^N$. Let $F \subseteq E$ and $x_0 \in F$. We say that:
\begin{itemize}
    \item $f$ attains a \emph{constrained local minimum} at $x_0$ if there exists $r > 0$ such that $f(x) \geq f(x_0)$ for all $x \in B(x_0, r) \cap F$.
    \item $f$ attains a \emph{constrained local maximum} at $x_0$ if there exists $r > 0$ such that $f(x) \leq f(x_0)$ for all $x \in B(x_0, r) \cap F$.
\end{itemize}
The set $F$ is called our \emph{constraint}.}
\thm{Lagrange Multipliers}{Let $U \subseteq \RR^N$ be an open set, $f : U \to \RR$ a function of class $C^1$ and let $g : U \to \RR^M$ a function of class $C^1$ where $M \leq N$. Then let 
\begin{align*}
    F := \{x \in U : g(x) = 0\}.
\end{align*}
Let $x_0 \in F$ and assume that $f$ attains a constrained local minimum (or maximum) at $x_0$. If the vectors $\nabla g_i(x_0)$ for $i = 1, 2, \ldots, M$ are linearly independent, then there exist $\lambda_1, \lambda_2, \ldots, \lambda_M \in \RR$ such that:
\begin{align*}
    \nabla f(x_0) = \sum_{i=1}^M \lambda_i \nabla g_i(x_0).
\end{align*}

}

\begin{proof}
    Assume $f$ gets a local min at $x_0$. Then, there is $B(x_0, r) \subseteq U$ such that:
    \begin{align*}
        f(x) \geq f(x_0) \tag{$\forall x \in B(x_0, r) \text{ with } g(x) = 0$}.
    \end{align*}
    $\nabla g_i(x_0)$ are linearly independent iff $J_g(x_0)$ has an $M \times M$ submatrix with nonzero determinant. That is, by relabelling, we can say:
    \begin{align*}
        \det \begin{pmatrix} \frac{\partial g_1}{\partial x_{N-M+1}}(x_0) & \cdots & \frac{\partial g_1}{\partial x_N}(x_0) \\ \vdots & \ddots & \vdots \\ \frac{\partial g_M}{\partial x_{N-M+1}}(x_0) & \cdots & \frac{\partial g_M}{\partial x_N}(x_0) \end{pmatrix} \neq 0.
    \end{align*}
    Write $x =(z,y) \in \RR^{N-M} \times \RR^M$ and $x_0 = (a, b)$ and we see that:
    \begin{align*}
        g(a, b) = 0 \quad \text{and} \quad \det \begin{pmatrix} \frac{\partial g_1}{\partial y_1}(a, b) & \cdots & \frac{\partial g_1}{\partial y_M}(a, b) \\ \vdots & \ddots & \vdots \\ \frac{\partial g_M}{\partial y_1}(a, b) & \cdots & \frac{\partial g_M}{\partial y_M}(a, b) \end{pmatrix} \neq 0.
    \end{align*}
    So we are in a spot to apply the IFT to get $h(z)$ such that $g(z, h(z)) = 0$ for $z \in B(a, r_0)$ for some $r_0 > 0$. Also $h$ is $C^1$ and $h(a) = b$. Then let $H: z \mapsto (z, h(z))$. So, $H$ is $C^1$ and $H(a) = x_0$. Then,
    \begin{align*}
        f(H(z)) \geq f(x_0) = f(H(a)) \quad \forall z \in B(a, r_0).
    \end{align*}
    As such $f \circ H$ attains a local minimum at $a$, meaning
    \begin{align*}
        0 = \nabla (f \circ H)(a) = \nabla f(x_0) \cdot \nabla J_H(a).
    \end{align*}
\end{proof}


\section{Lebesgue Measure}
\dfn{Length, Rectangle, Volume}{
    $I \subseteq \RR$ interval. The \emph{length} of $I$ is defined as the $\length(I) = \sup I - \inf I$. A \emph{rectangle} in $\RR^N$ is a set of the form $I_1 \times I_2 \times \cdots \times I_N$ where $I_i$ are intervals in $\RR$. The \emph{volume} of a rectangle is defined as $\vol(I_1 \times I_2 \times \cdots \times I_N) = \length(I_1)\length(I_2)\cdots\length(I_N)$.
}

\dfn{Lebesgue Outer Measure}{Let $E \subseteq \RR^N$. The \emph{Lebesgue outer measure} of $E$ is defined as:
\begin{align*}
    \mathcal{L}^N_o (E) := \inf \left\{ \sum_{n=1}^\infty \vol (R_m) : E \subseteq \bigcup_{n=1}^\infty R_m, R_m \text{ rectangles}\right\}
\end{align*}}
\exer{}{$R$ rectangle, $R$ closed. Let $R_n$ be open rectangles such that $R \subseteq \bigcup_{m=1}^\infty R_m$. Prove that there is $\ell \in \NN$ such that $R \subseteq \bigcup_{m=1}^\ell R_m$.}


\exer{}{$R$ rectangle, $\mathcal L_o^N(\partial R) = 0$.}

\thm{}{$R$ rectangle. Then $\mathcal L_o^N(R) = \vol(R)$.}
\begin{proof}
    Let $R_1 = R$ and $R_2 = R_3 = \cdots = \emptyset$. Then we have that $\mathcal L_o^N(R) \leq \sum_{n=1}^\infty \vol(R_n) = \vol(R)$.

    Now assume $R$ is closed. Let $R_n$ be rectangles $\bigcup_{n=1}^\infty R_n \supseteq R$. Fix $\epsilon > 0$, find $S_n \supseteq R_n$ open with $\vol(S_n) \leq \vol(R_n) + \epsilon 2^{-n}$. 

    Now we can apply the exercise to find some $\ell \in \NN$ such that $R \subseteq \bigcup_{n=1}^\ell S_n$. 
    \begin{align*}
        \vol(R) \leq \sum_{n=1}^\infty \vol(S_n)\leq \sum_{n=1}^\ell \vol(S_n) \leq \sum_{n=1}^\ell \vol(R_n) + \epsilon
    \end{align*}
    That is,
    \begin{align*}
        \vol(R) \leq \sum_{n=1}^\infty \vol(R_n)
    \end{align*}
    as $\epsilon \to 0$. So, 
    \begin{align*}
        \vol(R) \leq \mathcal L_o^N(R)
    \end{align*}
    and we are done.

    If $R$ is not closed, $\vol(\bar R) = \vol (R) = \mathcal L_o^N(\bar R) \leq \mathcal L_o^N(R) + \mathcal L_o^N(\partial R) = \mathcal L_o^N(R)$.
\end{proof}
\noindent Properties of Lebesgue outer measure:
\begin{enumerate}
    \item $E \subseteq F$, then $\mathcal L_o^N(E) \leq \mathcal L_o^N(F)$.
    \begin{proof}
        Let $R_i$ be rectangles such that $F \subseteq \bigcup_{i=1}^\infty R_i$, meaning $E \subseteq \bigcup_{i=1}^\infty R_i$. So,
        \begin{align*}
            \mathcal L_o^N(E) \leq \sum_{i=1}^\infty \vol(R_i)
        \end{align*}
        Taking the infimum over all such rectangles gives the desired result.
    \end{proof}
    \item $E \subseteq \bigcup_{k=1}^\infty E_k$, then $\mathcal L_o^N(E) \leq \sum_{k=1}^\infty \mathcal L_o^N(E_k)$.
    \begin{proof}
        If $\sum_{k=1}^\infty \mathcal L_o^N(E_k) = \infty$, then the inequality is trivial. So assume that the sum is finite. Fix $\epsilon > 0$. $\mathcal L_o^N(E_k)$ find rectangles $R_{n, k}$ such that $E_k \subseteq \bigcup_{n=1}^\infty R_{n, k}$ and $\sum_{n=1}^\infty \vol(R_{n, k}) \leq \mathcal L_o^N(E_k) + \epsilon 2^{-k}$. So,
        \begin{align*}
            E \subseteq \bigcup_{k=1}^\infty E_k \subseteq \bigcup_{k=1}^\infty \bigcup_{n=1}^\infty R_{n, k} = \bigcup_{j = 1}^\infty S_j
        \end{align*}
        So, $\mathcal L_o^N(E) \leq \sum_{j=1}^\infty \vol(S_j) = \sum_{k=1}^\infty \sum_{n=1}^\infty \vol(R_{n, k}) \leq \sum_{k=1}^\infty \mathcal L_o^N(E_k) + \epsilon$.
        Letting $\epsilon \to 0^+$ gives the desired result.
    \end{proof}
    \item $E, F \subseteq \RR^N$ with $\dist(E, F) > 0$, where $\dist$ is the infimum of the collection $\Vert x- y \Vert$ for $x \in E$, $y \in F$. Then $\mathcal L_o^N(E \cup F) = \mathcal L_o^N(E) + \mathcal L_o^N(F)$.
    \begin{proof}
        Let $R_n$ be rectangles, $E \cup F \subseteq \bigcup_{n=1}^\infty R_n$. Can we assume $\diam R_n < \dist(E, F)$ (partition $R_n$ into smaller rectangles and use previous exercise). So,
        \begin{align*}
            \bigcup_{n=1}^\infty R_n = \bigcup_{R_n \cap E \neq \emptyset} R_n \cup \bigcup_{R_n \cap F \neq \emptyset} R_n
        \end{align*}
        So, $\mathcal L_o^N(E) + \mathcal L_o^N(F) \leq \sum_{R_n \cap E \neq \emptyset} \vol(R_n) + \sum_{R_n \cap F \neq \emptyset} \vol(R_n) = \sum_{n=1}^\infty \vol(R_n) = \mathcal L_o^N(E \cup F)$.
    \end{proof}
\end{enumerate}
\newpage
\nt{You can construct $E, F \subseteq \RR^N$, $E \cap F = \emptyset$ such that \begin{align*}
    \mathcal L_o^N(E \cup F) < \mathcal L_o^N(E) + \mathcal L_o^N(F).
\end{align*}
(Relies on axiom of choice)}

\dfn{Lebesgue Measure}{A set $E \subseteq \RR^N$ is \emph{Lebesgue measurable} if for every $\epsilon > 0$, there exists an open set $U \supseteq E$ such that $\mathcal L_o^N(U \setminus E) < \epsilon$.

We define the \emph{Lebesgue measure} of $E$ to be $\mathcal L^N(E) = \mathcal L_o^N(E)$.}
\noindent Properties of Lebesgue measure:
\begin{enumerate}
    \item Open sets are Lebesgue measurable. (If $E$ is open, take $U = E$, so $U \setminus E = \emptyset$.)
    \item If $\mathcal L_o^N (E) = 0$, then $E$ is Lebesgue measurable. 
    \begin{proof}
        Let $\epsilon > 0$. Find $R_n$ rectangles such that $\bigcup R_n \supseteq E$ and $\sum \vol(R_n) < \epsilon$. Find $S_n$ open rectangle with $S_n \supseteq R_n$ and $\vol(S_n) < \vol(R_n) + \epsilon 2^{-n}$. So, let $U = \bigcup S_n \supseteq E$, so $U$ is open. Then,
        \begin{align*}
            \mathcal L_o^N(U \setminus E) \leq \mathcal L_o^N(U) \leq \sum \vol(S_n) \leq \sum \vol(R_n) + \epsilon < \epsilon + \epsilon = 2\epsilon
        \end{align*}
        and we are done.
    \end{proof}
    \item If $E_n$ is Lebesgue measurable, then $\bigcup E_n$ is Lebesgue measureable. 
    \begin{proof}
        Fix $\epsilon > 0$, find $U_n \supseteq E_n$ open, $\mathcal L_o^N(U_n \setminus E_n) < \epsilon 2^{-n}$. So, $U = \bigcup U_n \supseteq \bigcup E_n$. So,
        \begin{align*}
            \mathcal L_o^N(U \setminus \bigcup E_n) \leq \sum \mathcal L_o^N(U_n \setminus E_n) < \epsilon
        \end{align*}
        and we are done.
    \end{proof}
\end{enumerate}
\newpage
\thm{}{Sequentially compact sets are Lebesgue measurable.}
\mlenma{}{$C \subset \RR^N$ closed, $K \subset \RR^N$ sequentially compact such that $C \cap K = \emptyset$. Then $\dist(C, K) > 0$.}
\begin{proof}
    Assume $\dist(C, K) = 0$. Take $\epsilon = \frac 1n$, find $x_n \in C$, $y_n \in K$ such that $\Vert x_n - y_n \Vert < \frac 1n$. Since $K$  is sequentially compact, $\exists y_{n_k} \to y \in K$. So,
    \begin{align*}
        \Vert x_{n_k} - y\Vert \leq \Vert x_{n_k} - y_{n_k} \Vert + \Vert y_{n_k} - y \Vert
    \end{align*}
    Both values on the RHS tend to 0, so $x_{n_k} \to y$, $x_{n_k} \in C$ as well. This implies $y \in C$, but $C \cap K$ is empty, so contradiction.
\end{proof}

\mlenma{}{$U \subseteq \RR^N$ open. Then, $U = \bigcup Q_n$ where $Q_n$ are closed cubes with disjoint interiors. }
\begin{proof}
    Subdivide $\RR^N$ into cubes of sidelength one. If $Q_n \subseteq U$, keep it. If $Q_n \subseteq \RR^n \setminus U$, throw it away. If $Q_n \cap U \neq \emptyset$, $\overline Q_n \cap   (\RR^n \setminus U) \neq \emptyset$, subdivide $Q_n$ into cubes of sidelength $\frac 12$. Keep going. Find a sequence of cubes $Q_n$ (the ones you kept). We have that $\overline Q_n \subseteq U$ and we want $\bigcup \overline Q_n =  U$. 

    Let $x \in U$, then $\dist(x, \partial U) > 0$. $x$ was in one of the cubes of side one $T_0$. Either $\overline T_0 \subseteq U$ or $T_0$ was subdivided. In the first case, we are done. Otherwise $x \in T_1$, one of the subcubes $\frac{\sqrt N}{2^n} < \frac r2$. 
\end{proof}
\noindent We turn to the proof of the theorem.
\begin{proof}
    Take $K$ sequentially compact. Fix $\epsilon > 0$. Find open rectangles $R_n$ such that $\bigcup R_n \supseteq K$ and $\sum \vol R_n  \leq \mathcal L_o^N(K) + \epsilon$. 

    Let $U = \bigcup R_n$.  \begin{align*}
        \mathcal L_o^N(U) \leq \sum \vol R_n \leq \mathcal L_o^N(K) + \epsilon
    \end{align*}
    So, $U \setminus K$ is open, and by a previous lemma, we have:
    \begin{align*}
        U \setminus K = \bigcup Q_n
    \end{align*}
    where $Q_n$ are closed cubes with disjoint interiors. Let $C_\ell = \bigcup_{n=1}^\ell Q_n$. We know that $C_\ell$ is closed  and $C\ell \cap K = \emptyset$. By the other lemma, $\dist(C_\ell, K) > 0$, so 
    \begin{align*}
        \mathcal L_o^N ( C_\ell \cup K) &= \mathcal L_o^N (C_\ell) + \mathcal L_o^N(K) \\
        &= \sum_{n=1}^\ell \vol Q_n + \mathcal L_o^N (K) \\
        \implies \sum_{n=1}^\ell \vol Q_n &\leq \mathcal L_o^N (C_\ell \cup K) - \mathcal L_o^N(K) \\
        &\leq \mathcal L_o^N(U) - \mathcal L_o^N(K) \leq \epsilon
    \end{align*}
    Taking $\ell \to \infty$ completes the proof; 
    \begin{align*}
        \mathcal L_o^N (U \setminus K) \leq \sum_{n =1}^{\ell \to \infty} \vol Q_n \leq \epsilon
    \end{align*}
    as desired.
\end{proof}
\newpage
\thm{}{Closed sets are Lebesgue measureable. }
\begin{proof}
    Let $C \in \RR^N$ be closed. If $C$ is bounded, then we're done. Otherwise, $C = \bigcup_{n=1}^\infty C \cap \overline{B(0, n)}$. Each $C \cap \overline{B(0, n)}$ is bounded, so Lebesgue measurable. So, $C$ is Lebesgue measurable, as the union of Lebesgue measurable sets is Lebesgue measurable.
\end{proof}
\thm{}{If $E \subseteq \RR^N$ is Lebesgue measureable, then $\RR^N \setminus E$ is Lebesgue measureable.}
\begin{proof}
    Let $E$ be Lebesgue measureable. Let $\epsilon = \frac 1n$. Let $U_n$ be open, $U_n \supseteq E$, and $\mathcal L_o^N(U_n \setminus E) \leq \frac 1n$. Then we have $\RR^N \setminus U_n \subseteq \RR^N \setminus E$. Let $C_n := \RR^N \setminus U_n$. Then $C_n$ is closed, so Lebesgue measureable. 

    We have that $F := \bigcup C_n \subseteq \RR^N \setminus E$, where the LHS is LM. We want to show that $\RR^n \setminus E = F \cup S$, where $S$ is a set with measure zero. That is, we claim $\mathcal L_o^N((\RR^n \setminus E) \setminus F) = 0$.

    So,
    \begin{align*}
        x \in (\RR^n \setminus E) \setminus F \subseteq U_n \setminus E.
    \end{align*}
    if $x \notin F$, then $x \notin C_n \implies x \in U_n$. So,
    \begin{align*}
        0 \leq \mathcal L_o^N((\RR^N \setminus E) \setminus F) \leq \mathcal L_o^N(U_n \setminus E) \leq \frac 1n.
    \end{align*}
    So as we send $n \to \infty$, we have that $\mathcal L_o^N((\RR^N \setminus E) \setminus F) = 0$, meaning that $\RR^N \setminus E$ is Lebesgue measureable.
\end{proof}
\thm{}{$E_n$ Lebesgue measureable, $n \in \NN$. Then $\bigcap E_n$ is Lebesgue measureable.}
\begin{proof}
    We know that the union of Lebesgue measureable sets is Lebesgue measureable. So, $\RR^N \setminus \bigcup \RR^N \setminus E_n$ is Lebesgue measureable. By the previous theorem, $\bigcap E_n$ is Lebesgue measureable.
\end{proof}
\newpage
\thm{Important}{Let $E_n$ be Lebesgue measureable such that $E_i \cap E_j = \emptyset$ for $i \neq j$. Then $\mathcal L^N\left(\bigcup E_n\right) = \sum \mathcal L^N(E_n)$.}
\begin{proof}
    \textbf{Step 1:} Assume that the $E_n$ are bounded. Consider $\RR^N \setminus E_n$, which we know is LM. By the definition of LM, given an $\epsilon > 0$, we can find some $U_n$ open that contains $\RR^N \setminus E_n$ and $\mathcal L_o^N(U_n \setminus (\RR^N \setminus E_n)) \leq \frac{\epsilon}{2^n}$. 

    Let $K_n := \RR^n \setminus U_n \subseteq E_n$. Then $K_n$ is compact, so LM. Then for $i \neq j$, $K_j \cap K_i = \emptyset$, meaning $\dist (K_i, K_j) > 0$. So, 
    \begin{align*}
        \mathcal L_o^N\left(\bigcup_{n=1}^\ell K_n\right) = \sum_{n=1}^\ell \mathcal L_o^N(K_n).
    \end{align*}
    Then,
    \begin{align*}
        \mathcal L_o^N\left(\bigcup_{n=1}^\infty E_n\right) \geq \mathcal L_o^N\left(\bigcup_{n=1}^\ell E_n\right) \geq \mathcal L_o^N\left(\bigcup_{n=1}^\ell K_n\right) = \sum_{n=1}^\ell \mathcal L_o^N(K_n) \geq \sum_{n=1}^\ell \mathcal L_o^N(E_n) - \mathcal L_o^N(E_n \setminus K_n) \geq \sum_{n=1}^\ell \mathcal L_o^N(E_n) - \epsilon.
    \end{align*}
    Now let $\ell \to \infty$ to get:
    \begin{align*}
        \mathcal L_o^N\left(\bigcup_{n=1}^\infty E_n\right) \geq \sum_{n = 1}^\infty \mathcal L_o^N ( E_n) - \epsilon.
    \end{align*}
    Send $\epsilon \to 0$ to get the desired result.

    \textbf{Step 2:} Now we consider the case where the $E_n$ are not necessarily bounded. Let 
    \begin{align*}
        F_1 &= B(0, 1) \\
        F_2 &= B(0, 2) \setminus B(0, 1) \\
        &\vdots \\
        F_i &= B(0, i) \setminus B(0, i-1) \\
        &\vdots
    \end{align*}
    Then we have $E_n = E_n \cap \bigcup F_i = \bigcup_i (E_n \cap F_i)$. So,
    \begin{align*}
        \mathcal L_o^N \left(\bigcup_{n=1}^\infty E_n \right) &= \mathcal L_o^N \left( \bigcup_{n=1}^\infty \bigcup_{i=1}^\infty (E_n \cap F_i)\right) \\
        &= \sum_{n=1}^\infty \sum_{i=1}^\infty \mathcal L_o^N (E_n \cap F_i) \\
        &= \sum_{n=1}^\infty \mathcal L_o^N\left( \bigcup_{i=1}^\infty E_n \cap F_i\right) \\
        &= \sum_{n=1}^\infty \mathcal L_o^N(E_n).
    \end{align*}
\end{proof}
\newpage
\cor{}{If we have $E_1 \subseteq E_2 \subseteq \cdots \subseteq E_n$, basically do the same as above with $F_1 = E_1, F_2 = E_2 \setminus E_1, \ldots$ to get:
\begin{align*}
    \mathcal L_o^N \left( \bigcup_{n=1}^\infty E_n\right) = \sup_{n} \mathcal L^N_o(E_n) = \lim_{n \to \infty} \mathcal L_o^N(E_n).
\end{align*}
And, \begin{align*}
    \mathcal L_o^N\left( \bigcup_{n=1}^\infty F_n \right) = \sum_{n=1}^\infty \mathcal L_o^N(F_n) = \lim_{\ell \to \infty} \sum_{n=1}^\ell \mathcal L_o^N(F_n) = \lim_{\ell \to \infty} \mathcal L_o^N\left( \bigcup_{n=1}^\ell F_n\right) = E_\ell.
\end{align*}
Note that $\mathcal L_o^N\left( \bigcup_{n=1}^\infty F_n \right) = \mathcal L_o^N\left( \bigcup_{n=1}^\infty E_n \right)$.
}

\section{Lebesgue Integration}
\dfn{Characteristic Function}{Let $E$ be a set. The characteristic function of the set $E$ is defined as:
\begin{align*}
    \chi_E(x) = \begin{cases}
        1 & x \in E \\
        0 & x \notin E
    \end{cases}.
\end{align*}
}
\dfn{Simple Function}{Let $E \subseteq \RR^N$ be a Lebesgue measureable set. A function $s: E \to \RR$ is called a \emph{simple function} if it is of the form:
\begin{align*}
    s(x) = \sum_{i=1}^n a_i \chi_{E_i}(x)
\end{align*}
where $a_i \in \RR$ and $E_i$ are Lebesgue measureable sets.}
\dfn{Lebesgue Integral (simple)}{Let $s: E \to [0, \infty)$ be a simple function. That is:
\begin{align*}
    s = \sum_{i=1}^n c_i \chi_{E_i},
\end{align*}
with disjoint $E_j$. Then the Lebesgue integral of $s$ over $E$ is defined as:
\begin{align*}
    \int_E s\, dx := \sum_{i=1}^n c_i \mathcal L^N(E_i).
\end{align*}
If $c_i = 0$, $c_i \mathcal L^N(E_i) = 0$.}
\newpage

\noindent Properties of the Lebesgue integral:
\begin{enumerate}
    \item $\int_E cs\, dx = c \int_E s\, dx$.
    \item If $s, t$ simple, then $\int_E (s + t)\, dx = \int_E s\, dx + \int_E t\, dx$.
\end{enumerate}
\dfn{Lebesgue Measureable Function}{Let $E \subseteq \RR^N$ be Lebesgue measureable. A function $f: E \to [0, \infty)$ is called Lebesgue measureable if there exists a sequence of simple functions $s_n$ with $0 \leq s_n \leq f$ such that $s_n \to f$ pointwise in $E$.}
\noindent Properties. Let $E \subseteq \RR^N$ LM, $f, g: E \to [0, \infty)$ LM:
\begin{enumerate}
    \item $f + g$ is LM.
    \item $fg$ is LM.
    \item $\max(f, g)$ is LM.
    \item $\min(f, g)$ is LM.
\end{enumerate}
\dfn{Lebesgue Integral}{Let $f: E \to [0, \infty)$ be a Lebesgue measureable function. Then the Lebesgue integral of $f$ over $E$ is defined as:
\begin{align*}
    \int_E f\, dx = \sup \left\{ \int_E s\, dx : 0 \leq s \leq f, s \text{ simple}\right\}.
\end{align*}}
\nt{$f : E \to [0, \infty)$ LM. WLOG, can assume $0 \leq s_n \leq s_{n+1} \leq f$ for all $n$. By def, $\exists s_n$ simple $0 \leq s_n \leq f$ such that $s_n \to f$ pointwise. So redefine $s_n$ to be increasing.

This still converges to $f$ pointwise. If we have $x \in E$, then:
\begin{align*}
    f(x) > \epsilon < s_n(x) \leq f(x),
\end{align*}
but \begin{align*}
    f(x) -\epsilon < s_n(x) \leq \overline s_n(x) \leq f(x).
\end{align*}
So, $\overline s_n \to f$ pointwise.}
\newpage
\thm{}{Let $f: E \to [0, \infty)$ be a Lebesgue measureable function. Then the set:
\begin{align*}
    E_a = \{ x \in E: f(x) > a \geq 0\}
\end{align*}
is Lebesgue measureable. }
\begin{proof}
    There are simple functions $0 \leq s_n \leq s_{n+1} \leq f$ such that $s_n \to f$ pointwise. Then,
    \begin{align*}
        \{ x \in E : f(x) > a \} \stackrel{?}{=} \bigcup_{n=1}^\infty \{ x \in E : s_n(x) > a\}.
    \end{align*}
    If $f(x) > a$, then $\lim_{n \to \infty} s_n(x) > a$. Take $\epsilon = \frac{f(x) - a}{2}$. So, we have:
    \begin{align*}
        s_n(x) > a
    \end{align*}
    for all $ n \geq \overline n$. Then, if $s_n(x) > a$ for some $n$, this means 
    \begin{align*}
        f(x) \geq s_n(x) > a.
    \end{align*}
    So, the two sets are equal.
\end{proof}
\nt{Some more Lebesgue measureable sets:
\begin{enumerate}
    \item $\{x \in E : f(x) \leq a\}$ is LM because $E \setminus \{x \in E : f(x) \leq a\} = \{x \in E : f(x) > a\}$ and the LHS is LM.
    \item Now $\{x \in E : f(x) \geq a\} = \bigcap_{n=1}^\infty \{x \in E : f(x) > a - \frac 1n\}$ is LM. (exercise)
\end{enumerate}}

\thm{}{Let $E \subseteq \RR^N$ be LM. $f: E \to [0, \infty]$. If \begin{align*}
    E_a = \{ x \in E . f(x) > a\}
\end{align*}
is LM for all $a \geq 0$, then $f$ is LM. }

\cor{}{$E\subseteq \RR^N$ LM, $f: E \to [0, \infty)$ continuous. Then $f$ is LM.}
\begin{proof}
    $E_a = f^{-1}((a, \infty)) = E \cap U$ open. Intersection of two LM sets is LM.
\end{proof}
\begin{proof}
    For $n \in \NN$, let:
    \begin{align*}
        E_n &= \{ x \in E: f(x) \geq 2^n\} \\
        E_{n | k} &= \left\{ x\in E :  \frac{k}{2^n} \leq f(x) < \frac{k+1}{2^n}\right\}. \tag{$0 \leq k \leq 2^{2^n - 1}$}
    \end{align*}
    Then define the simple function:
    \begin{align*}
        s_n(x) &= \begin{cases} 2^n & x \in E_n \\ \frac{k}{2^n} & x \in E_{n | k}\end{cases}.
    \end{align*}
    Claim that $s_n \to f$ as $n \to \infty$. If $f(x) = 0$, then $x \in E_{n|0}$. So, $s_n(x) = 0 \to 0 = f(x)$.

    If $f(x) > 0$, note that $k = \lfloor 2^nf(x) \rfloor$. So, $x \in E_{n|k}$. So, 
    \begin{align*}
        s_n(x) = \frac{\lfloor 2^nf(x) \rfloor}{2^n} \leq f(x) < \frac{\lfloor 2^nf(x) \rfloor + 1}{2^n} = s_n(x) + \frac{1}{2^n}.
    \end{align*}
    This yields:
    \begin{align*}
        0 \leq f(x) - s_n(x) < \frac{1}{2^n}.
    \end{align*}
    As $n\to \infty$, we have that $s_n \to f$ pointwise. So, $f$ is LM.
\end{proof}

\noindent Properties of $f_n: E \to [0, \infty)$ measureable. 
\begin{enumerate}
    \item Assume $\sup_n f_n(x): E \to [0, \infty)$, then $\sup_n f_n$ is LM.
    \begin{proof}
        \begin{align*}
            \{ x \in E : \sup_n f_n(x) > a\} = \bigcup_{n=1}^\infty \left\{ x \in E : f_n(x) > a\right\}.
        \end{align*}
        As the right side is Lebesgue measureable, so is the left side.
    \end{proof}
    \item Assume $\inf_n f_n(x): E \to [0, \infty)$, then $\inf_n f_n$ is LM.
    \begin{proof}
        \begin{align*}
            \{ x \in E : \inf_n f_n(x) \geq a \} = \bigcap_{n=1}^\infty \left\{ x \in E : f_n(x) \geq a\right\}.
        \end{align*}
        This is LM because of stuff we did on Friday (complements idk).
    \end{proof}
    \item $\limsup$ and $\liminf$ are LM because of the first two properties.
\end{enumerate}
\newpage
\thm{Lebesgue Monotone Convegence Theorem}{Let $E \subseteq \RR^N$ be LM, $f_n : E \to [0, \infty)$ LM, $0 \leq f_n \leq f_{n+1}$ for all $n$. Assume $f(x) = \lim_{n \to \infty}f_n(x) \in [0, \infty)$ for all $x \in E$. Then,
\begin{align*}
    \int_E f(x) \, \text dx = \lim_{n \to \infty} \int_E f_n(x) \, \text dx.
\end{align*}}
\noindent Recall that $\int_E f(x) \, \text dx = \sup \left\{ \int_E s(x) \, \text dx : 0 \leq s \leq f, s \text{ simple}\right\}$. It has the following properties for $E \subseteq \RR^N$ LM, $f, g: E \to [0, \infty)$ LM:
\begin{enumerate}
    \item If $f \leq g$, then $\int_E f \, \text dx \leq \int_E g \, \text dx$.
\end{enumerate}
\begin{proof}
    We have $f_n \leq f_{n+1} \leq f$, and by the previous property, we have:
    \begin{align*}
        \int_E f_n \, \text dx \leq \int_E f_{n+1} \, \text dx \leq \int_E f \, \text dx.
    \end{align*}
    Meaning, there exists
    \begin{align*}
        \sup_n \int_E f_n \, \text dx = \lim_{n \to \infty} \int_E f_n \, \text dx \leq \int_E f \, \text dx.
    \end{align*}
    Now let $0 \leq s \leq f$ be simple and fix $0 <\epsilon < 1$. Define \begin{align*}
        E_n = \{ x \in E : f_n(x) \geq (1-\epsilon)s(x)\}.
    \end{align*}
    Then $E_n$ is LM and $E_n \subseteq E_{n+1}$. So we have:
    \begin{align*}
        (1-\epsilon)s(x) \leq f_n(x) \leq f_{n+1}(x).
    \end{align*}
    We want to show
    \begin{align*}
        \bigcup_{n=1}^\infty E_n = E.
    \end{align*}
    Obviously the left is contained in the right. So now let $x \in E$. If $f(x) = 0$, then $f_n(x) = 0$ for all $n$ and $s(x) = 0$, meaning $x \in E_n$ for all $n$. 
    
    If $f(x) > 0$, since 
    \begin{align*}
        \lim_{n\to \infty} f_n(x) = f(x) > (1- \epsilon)s(x)
    \end{align*}
    for all $n$ large, 
    \begin{align*}
        f_n(x) > (1-\epsilon)s(x)
    \end{align*}
    meaning $x \in \E_n$ for all large $n$. 

    Now note 
    \begin{align*}
        f_n &\geq \chi_{E_n}f_n \geq (1-\epsilon)s \chi_{E_n} \\ 
        \int_E f_n \, \text dx &\geq \int_E \chi_{E_n} f_n \, \text dx \geq \int_E (1-\epsilon)s \chi_{E_n} \, \text dx.
    \end{align*}
    Now $s = \sum_{i = 1}^\ell c_i \chi_{F_i}$ for disjoint $F_i$. So we can rewrite the last integral as:
    \begin{align*}
        \int_E (1-\epsilon)\sum_{i = 1}^\ell \chi_{E_n}\chi_{F_i} &= (1-\epsilon)\sum_{i=1}^\ell c_i \mathcal L^N(E_n \cap F_i) \\
        &\to (1-\epsilon)\sum_{i=1}^\ell c_i \mathcal L^N\left(\bigcup_{n=1}^\infty E_n \cap F_i\right) \\
        &= (1-\epsilon)\sum_{i=1}^\ell c_i \mathcal L^N(F_i) \\
        &= (1-\epsilon)\int_E s \, \text dx.
    \end{align*}
    So,
    \begin{align*}
        \lim_{n\to \infty} \int_E f_n \, \text dx \geq (1-\epsilon)\int_E s \, \text dx.
    \end{align*}
    Take $\epsilon \to 0^+$. This means the LHS is an upper bound for the integral of $f$. 
\end{proof}

\cor{}{Let $E \subseteq \RR^N$ be LM and $f, g: E \to [0, \infty]$ LM. Then $\int (f + g) = \int f + \int g$. }
\begin{proof}
    Since $f$ is LM, there is a sequence of $s_n$ simple functions that converge pointwise to $f$. Similarly, there is a sequence of $t_n$ simple functions that converge pointwise to $g$. We can assume that the $s_n$ and $t_n$ are increasing. Then we have that $s_n + t_n \leq s_{n+1} + t_{n+1}$. Now we apply the Lebesgue Monotone Convergence Theorem to get:
    \begin{align*}
        \int (f + g) = \lim_{n \to \infty} \int (s_n + t_n) = \lim_{n \to \infty} \int s_n + \lim_{n \to \infty} \int t_n = \int f + \int g.
    \end{align*}
\end{proof}
\section{Real Calculus (idk)}
\mlenma{Fatou's Lemma}{Let $E \subseteq \RR^N$ be LM, $f_n: E \to [0, \infty)$ LM. Assume $\liminf_{n \to \infty}f(x) =: f(x) < \infty$ for all $x \in E$. Then, $\int_E f \, \text dx \leq \liminf_{n \to \infty} \int_E f_n \, \text dx$.}
\begin{proof}
    $\liminf_{n \to \infty} f_n(x) =  \sup_n \inf_{k \geq n} f_k(x)$. Let $g_n(x) := \inf_{k \geq n} f_k(x)$. Then $g_n \leq g_{n+1}$, so we can apply the Lebesgue Monotone Convergence Theorem to get:
    \begin{align*}
        \int_E \lim_{n \to \infty} g_n \, \text dx = \lim_{n \to \infty} \int_E g_n \, \text dx \leq \liminf_{n \to \infty} \int_E f_n \, \text dx.
    \end{align*}
    But $\int_E \lim_{n \to \infty} g_n \, \text dx = \int_E \sup_n g_n \, \text dx = \int_E \liminf_{n \to \infty} f_n \, \text dx$.
\end{proof}

\thm{}{Let $E \subseteq \RR^N \times \RR^M$ be LM. Let $f: E \to [0, \infty)$ be LM. Then,
\begin{align*}
    \mathcal L^{N+M} (E)= \int_G \mathcal L^M(E_x) \, \text dx = \int_H \mathcal L^N(E^y) \, \text dy.
\end{align*}}
\cor{}{Let $E \subseteq \RR^N$ be LM, $a, b: E \to [0, \infty)$ LM with $a \leq b$. Then,
\begin{align*}
    F = \{ (x, y) \in E \times \RR : a(x) \leq y \leq b(x)\}
\end{align*}
is LM.}
\newpage
\thm{Fubini's Theorem}{Let $E \subseteq \RR^N \times \RR^M$ be LM. Let $f: E \to [0, \infty)$ be LM. Then,
\begin{align*}
    \int_E f(x, y) \, \text dx \, \text dy = \int_{\RR^N} \left( \int_{\RR^M} f(x, y) \, \text dy\right) \, \text dx = \int_{\RR^M} \left( \int_{\RR^N} f(x, y) \, \text dx\right) \, \text dy.
\end{align*}}
\cor{}{Let $E \subseteq \RR^N$, $a, b: E \to [0, \infty)$ LM with $a\leq b$. $F = \{ (x, y) \in E \times \RR : a(x) \leq y \leq b(x)\}$, $f : F \to [0, \infty)$ LM. Then,
\begin{align*}
    \int_F f \, \text dx \text dy = \int_E \left( \int_{a(x)}^{b(x)} f(x, y) \, \text dy\right) \, \text dx.
\end{align*}}
\nt{$F$ is LM because $g(x, y) = y - a(x)$ is LM. So, 
\begin{align*}
    F_1 = \{ (x, y) \in E \times \RR : g(x, y) \geq 0\}
\end{align*}
is measureable. Then $h(x, y) = b(x) - y$ is LM, so $F_2 = \{ (x, y) \in E \times \RR : h(x, y) \geq 0\}$ is LM. Then $F = F_1 \cap F_2$ is LM.}

\dfn{LM Functions (can be negative)}{$E \subseteq \RR^N$ Lebesgue measureble, $f: E \to \RR$ LM. Then $f$ is Lebesgue Measureable if $f^+$ and $f^-$ are LM.}

\dfn{Lebesgue Integral}{The Lebesgue Integral of $f$ over $E$ is 
\begin{align*}
    \int_E f \, \text dx := \int_E f^+ \, \text dx - \int_E f^- \, \text dx,
\end{align*}
provided one of them is finite. 
}

\ex{}{$f(x) = \frac{\sin x}{x}$ on $x > 1$. Check:
\begin{align*}
    \int_1^\infty \left(\frac{\sin x}{x}\right)^+ \, \text dx &= \infty \\
    \int_1^\infty \left(\frac{\sin x}{x}\right)^- \, \text dx &= \infty.
\end{align*}
So, the integral does not exist. }

\dfn{Lebesgue Integrable}{$f$ is Lebesgue Integral over $E$ if $\int_E f \, \text dx < \infty$.}

\thm{Lebesgue Dominated Convergence Theorem}{Let $E \subseteq \RR^N$ be LM, $f_n: E \to \RR$ LM, $f: E \to \RR$ LM. Assume $f_n \to f$ pointwise, $|f_n| \leq g$ for all $n$ and some Lebesgue Integrable $g: E \to [0, \infty)$. Then,
\begin{align*}
    \lim_{n \to \infty} \int_E f_n \, \text dx = \int_E f \, \text dx.
\end{align*}}
\begin{proof}
    Step 1: Claim: $\int_E |f| \, \text dx < \infty$. We have that $|f_n| \leq g$ for all $n$, so 
    \begin{align*}
        \int_E |f_n| \, \text dx \leq \int_E g \, \text dx < \infty.
    \end{align*}
    We apply Fatou's Lemma to get:
    \begin{align*}
        \int_E |f| \, \text dx \leq \liminf_{n \to \infty} \int_E |f_n| \, \text dx < \infty.
    \end{align*}
    Step 2: $g + f_n \geq 0$ and $g - f_n \geq 0$. By Fatou's, we have
    \begin{align*}
        \liminf_{n \to \infty} \int_E ( g + f_n) \text dx \geq \int_E \liminf_{n \to \infty} ( g + f_n) \, \text dx = \int_E (g + f) \, \text dx = \int_E g \, \text dx + \int_E f \, \text dx \\
        \int_E g \, \text dx + \liminf_{n \to \infty} \int_E f_n \, \text dx \geq \int_E g \, \text dx + \int_E f \, \text dx.
    \end{align*}
    So, 
    \begin{align*}
        \liminf_{n \to \infty} \int_E f_n \, \text dx \geq \int_E f \, \text dx.
    \end{align*}
    We apply Fatou's to get:
    \begin{align*}
        \liminf_{n \to \infty} \int_E (g - f_n) \, \text dx \geq \int_E \liminf_{n \to \infty} ( g - f_n) \, \text dx = \int_E (g - f) \, \text dx = \int_E g \, \text dx - \int_E f \, \text dx.
    \end{align*}
    Then,
    \begin{align*}
        \int_E g \, \text dx - \limsup_{n \to \infty} \int_E f_n \, \text dx \geq \int_E g \, \text dx - \int_E f \, \text dx \\
        \limsup_{n \to \infty} \int_E f_n \, \text dx \leq \int_E f \, \text dx.
    \end{align*}
    So we have proved that:
    \begin{align*}
        \int_E f \, \text dx \leq \liminf_{n \to \infty} \int_E f_n \, \text dx \leq \limsup_{n \to \infty} \int_E f_n \, \text dx \leq \int_E f \, \text dx.
    \end{align*}
    SO,
    \begin{align*}
        \lim_{n \to \infty} \int_E f_n \, \text dx = \int_E f \, \text dx.
    \end{align*}
\end{proof}
\newpage
\thm{Fubini's Theorem (again)}{Let $E \subseteq \RR^N \times \RR^M$ be LM, $f: E \to \RR$ Lebesgue Integrable. Then,
\begin{align*}
    \int_E f(x, y) \, \text dx \, \text dy = \int_{G} \left( \int_{E_x} f(x, y) \, \text dy\right) \, \text dx = \int_{H} \left( \int_{E_y} f(x, y) \, \text dx\right) \, \text dy
\end{align*}
where
\begin{align*}
    G &= \{ x \in \RR^N : E_x \neq \emptyset\} \\
    H &= \{ y \in \RR^M : E_y \neq \emptyset\}.
\end{align*}}

\ex{}{Compute:
\begin{align*}
    \int_E \frac{y^3 e^{-xy^4}}{x^4} \, d(x, y),
\end{align*}
where \begin{align*}
    E = \{ (x, y) \in \RR^2 : x\geq 1, |y| < \frac{1}{x^{3/4}}\}.
\end{align*}
We know $E$ is LM because is the the intersection of the two LM sets:
\begin{align*}
    E_1 &= \{ x \geq 1\} \tag{closed} \\
    E_2 &= \left\{ x > 0, |y| < \frac{1}{x^{3/4}}\right\} \tag{open}.
\end{align*}
Now \begin{align*}
    f(x, y) = \frac{y^3 e^{-xy^4}}{x^4}. 
\end{align*}
This is Lebesgue measureable because $f^+$ is just when $y$ is positive, which is obviously LM, and the same for $f^-$. So now we need to show that $\int_E |f| \, d(x, y) < \infty$. We have:
\begin{align*}
    \int_E \frac{|y|^3e^{-xy^4}}{x^4}\, d(x, y) &= \int_1^\infty \frac{1}{x^4}\int_{-\frac{1}{x^{3/4}}}^{\frac{1}{x^{3/4}}} |y^3|e^{-xy^4} \, dy \, dx \\
    &= \int_1^\infty \frac{2}{x^{3/4}} \int_0^{\frac{1}{x^{3/4}}} y^3 e^{-xy^4} \, dy \, dx 
\end{align*}
SPENDING THE LAST 10 MINUTES OF 21-269 VECTOR ANALYSIS DOING A DOUBLE INTEGARL AHAHAHAJHAHA

}

FUN CLASS DEFINITELY RECOMMEND! $\square$





\end{document}