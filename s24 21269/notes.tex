\documentclass{report}

\input{../preamble}
\input{../macros}
\input{../letterfonts}

\title{\Huge{21-269}\\Vector Analysis}
\author{\huge{Rohan Jain}}
\date{}

\begin{document}

\maketitle
\newpage% or \cleardoublepage
% \pdfbookmark[<level>]{<title>}{<dest>}
\pdfbookmark[section]{\contentsname}{toc}
\tableofcontents

\pagebreak

\chapter{}
\section{The Real Numbers}
\dfn{Partial Order}{Let $X$ be a set with a binary relation $\leq$. $\leq$ is a \emph{partial order} if:
\begin{enumerate}
    \item $x \leq x$ for all $x \in X$ (reflexivity)
    \item $x \leq y$ and $y \leq z$ implies $x \leq z$ for all $x, y, z \in X$ (transitivity)
    \item $x \leq y$ and $y \leq x$ implies $x = y$ for all $x,y \in X$ (antisymmetry)
\end{enumerate} }

\dfn{Partially Ordered Set (poset)}{A set $X$ with a partial order $\leq$ is called a \emph{partially ordered set} or \emph{poset}. It is notated as $(X, \leq)$.}

\dfn{Total Order}{A partial order $\leq$ is a \emph{total order} if for all $x,y \in X$, we have $x \leq y$ or $y \leq x$.}

\ex{poset}{Let $Y$ be a set. Define $X = \{\text{all subsets of } Y\} = \mathcal{P}(Y)$. Let $E, F \in Y$, we say that $E \leq F$ if $E \subseteq F$. Then $(X, \leq)$ is a poset. This is not a total order.}

\dfn{Upper Bound, Bounded Above, Supremum, Maximum}{Let $(X, \leq)$ be a poset. Let $E \subseteq X$.
\begin{enumerate}
    \item $y \in X$ is an \emph{upper bound} of $E$ if $x \leq y$ for all $x \in E$.
    \item $E$ is \emph{bounded above} if it has at least one upper bound.
    \item If $E$ is nonempty and bounded above, then the \emph{supremum}, if it exists, of $E$, denoted $\sup E$, is the least upper bound of $E$.
    \item $E$ has a \emph{maximum} if there is $y \in E$ such that $ x \leq y$ for all $x \in E$.
\end{enumerate}}

Properties worth mentioning:
\begin{enumerate}
    \item If $E$ has a maximum, then $\sup E$ exists and is equal to the maximum.
    \begin{proof}
        Let $y$ be the maximum of $E$. If $z \in X$, is an upper bound of $E$, then $z \geq y$ because $y \in E$. Since $z$ was arbitrary, this is true for any upper bound. Thus, $y$ is the least upper bound of $E$.
    \end{proof}
\end{enumerate}

\ex{}{Let $Y$ be a nonempty set, $(\mathcal{P}(Y), \leq)$ poset.

Fix nonempty $Z \subseteq Y$. \[E = \{ W \subseteq Y : W \subset Z\}\]
Trivially, $Z$ is an upper bound of $E$. Realize that any superset of $Z$ is an upper bound as well. We can postulate that the supremum of $E$ is $Z$. We will now prove it:
\begin{proof}
    Need to show that if $F$ is an upper bound of $E$, then $F \supseteq Z$. If $x \in Z$, then $\{x\} \in E$ by definition of $E$, so $F \supseteq {x}$ for all $x \in Z$. Thus, $F \supseteq Z$.
\end{proof}
Note that there is no maximum of $E$. 
}

\dfn{Lower Bound, Bounded Below, Infimum, Minimum}{Let $(X, \leq)$ be a poset. Let $E \subseteq X$.
\begin{enumerate}
    \item $y \in X$ is a \emph{lower bound} of $E$ if $y \leq x$ for all $x \in E$.
    \item $E$ is \emph{bounded below} if it has at least one lower bound.
    \item If $E$ is nonempty and bounded below, then the \emph{infimum}, if it exists, of $E$, denoted $\inf E$, is the greatest lower bound of $E$.
    \item $E$ has a \emph{minimum} if there is $y \in E$ such that $ y \leq x$ for all $x \in E$.
\end{enumerate}
}
Going back to example 1.1.2, we can see that $E$ is bounded below by $\emptyset$. The infimum of $E$ is $\emptyset$. The minimum of $E$ is also $\emptyset$.
\dfn{Complete}{Let $(X, \leq)$ poset. $X$ is \emph{complete} if every nonempty subset of $X$ that is bounded above has a supremum.}
\ex{$\QQ$}{$(\QQ, \leq)$ is not complete.}
\clm{$\RR$}{There is a complete ordered field $(\RR, +, \cdot, \leq)$. Its elements are called real numbers.}
\section{First Recitation, 1/18}
\exer{Function Example}{Let $X$ be the set of all functions $f : D_f \to Z$ with $D_f \subseteq Y$. We say that $f \leq g$ if $D_f \subseteq D_g$ and $f(x) = g(x)$ for all $x \in D_f$. Is $(X, \leq)$ a poset? Is it complete?
\begin{proof}
    To show that $(X, \leq)$ is complete, we need to show that every nonempty subset of $X$ that is bounded above has a supremum. Let $E \subseteq X$ be nonempty and bounded above. Let $G = \bigcup_{f \in E} D_f$. $G$ is the union of all the domains of the functions in $E$. $G$ is bounded above by the union of the upper bounds of the domains of the functions in $E$. Let $H = \bigcup_{f \in E} f(D_f)$. $H$ is bounded above by the union of the upper bounds of the ranges of the functions in $E$. Let $F : G \to H$ be defined as $F(x) = f(x)$ for all $x \in D_f$. $F$ is the supremum of $E$.
\end{proof}}
\section{Natural Numbers}
\exer{}{Take $(X, +, \cdot,\leq)$ ordered field. Prove:
\begin{enumerate}
    \item If $0 \leq x$, then $-x \leq 0$. 
    \item If $x \leq y$, and $0 \leq z \neq 0$, then $xz \leq yz$.
    \item For all $x \in X$, $0 \leq x^2$.
    \item Prove $0 < 1$. 
\end{enumerate}}
\begin{proof}
    Fields have the following important properties:
    \begin{itemize}
        \item If $a \leq b$, then $a + c \leq b + c$.
        \item If $a, b \geq 0$, then $ab \geq 0$.
    \end{itemize}
    \begin{enumerate}
        \item Take the first property with $a = 0$, $b = x$, and $c = -x$. Then $0 \leq x \implies 0 + (-x) \leq x + (-x) \implies -x \leq 0$.
        \item If $x \leq y$, then $0 \leq y + (-x)$. By the second property, $0 \leq z \cdot(y + (-x)) = zy + (-zx)$. Then $0 \leq zy + (-zx) \implies zx \leq zy$. 
        \item We split into the three trichotomy cases:
        \begin{itemize}
            \item If $x = 0$, then $0 \leq 0^2$.
            \item If $x < 0$ with $x \neq 0$, then $0 \leq -x$. By the second property, $0 \leq (-x)^2 = (-x)(-x) = x^2$.
            \item If $x > 0$, then $0 \leq x$. By the second property, $0 \leq x^2$.
        \end{itemize}
        \item FSOC, assume $0 > 1$ and multiply both sides by 1. Then we get $0 \cdot 1 > 1 \cdot 1 \Rightarrow 0 > (1)^{2}$, which is a contradiction to the third property we proved.
    \end{enumerate}
\end{proof}

\dfn{Inductive}{Take $E \subseteq \RR$. $E$ is \emph{inductive} if $1 \in E$ and $x \in E$ implies $x+1 \in E$.}
\ex{Inductive Sets}{\begin{itemize}
    \item $\RR$ is inductive.
    \item $\{x \in \RR: 0 \leq x\}$
    \begin{proof}
        $1 \in E$ because $1 \geq 0$. If $x \in E$, then $x+1 \geq 0$, so $x+1 \in E$.
    \end{proof}
\end{itemize}}
\dfn{Natural Numbers}{The intersection of all inductive sets is denoted $\NN$. The elements of $\NN$ are called \emph{natural numbers}.}
\noindent Properties of $\NN$:
\begin{itemize}
    \item $\NN \neq \emptyset$. Since $1 \in$ every inductive set, $1 \in \NN$.
    \item $\NN$ is an inductive set.
\end{itemize}
\thm{Induction}{For every $n \in \NN$, let $P(n)$ be a proposition such that:
\begin{enumerate}
    \item $P(1)$ is true. 
    \item If $P(n)$, then $P(n+1)$.
\end{enumerate}Then $P(n)$ is true for every $n \in \NN$ }
\begin{proof}
$E = \{n \in \NN\ :\ P(n)\}$ is inductive by 1. and 2. So, $\NN \subseteq E$, but $E \subseteq \NN$ by definition of $\NN$. Thus, $E = \NN$.
\end{proof}
\thm{Archimedean Property}{Let $a, b \in \RR$ with $a > 0$. Then there is $n \in \NN$ such that $na > b$.}
\begin{proof}
    If $b \leq 0$, then we take $n = 1$. Assume $b >0$. For sake of contradiction, assume there does not exist $n$ such that $na > b$. Then $E = \{na : n \in \NN\}$ is bounded above by $b$. Let $c = \sup E$. $c - a \leq c$, so $c-a$ is not an upper bound of $E$. Thus, there is $n \in \NN$ such that $c-a \leq na$. Then $c \leq (n+1)a$. But $c$ is an upper bound of $E$, so $c \geq (n+1)a$. Thus, $c = (n+1)a$. But $c \in E$, so $c = na$ for some $n \in \NN$. Thus, $na = (n+1)a$, so $n = n+1$, which is a contradiction.
\end{proof}
\dfn{Integers}{$\ZZ := \NN \cup \{0\} \cup \{-n : n \in \NN\}$}
\thm{Integer Part}{For every $x \in \RR$, there is a unique $k \in \ZZ$ such that $k \leq x < k+1$.}
\dfn{Integer Part}{The $k$ that satisfies the above theorem is called the \emph{integer part} of $x$, denoted $\lfloor x \rfloor$.}
\begin{proof}
    Let $E = \{k \in \ZZ : k \leq x\}$. First we show that $E$ is nonempty.
    \begin{itemize}
        \item If $x \geq 0$, then $0 \in E$, so $E$ is nonempty.
        \item If $x < 0$, then $-x > 0$. By the Archimedean property, there is $n \in \NN$ such that $n > -x$. Thus, $-n < x$. So, $-n \in E$, so $E$ is nonempty.
    \end{itemize}
    Now we show that $E$ is bounded from above. Very clearly, $x$ is an upper bound. By supremum property, there is $L = \sup(E)$ and $L \in \RR$. $L-1$ is not an upper bound, which means that there is an element $k \in E$ such that $L-1 < k$. But since $L$ is the supremum, $L \geq k$. Thus, $L-1 < k \leq L$. So, $L < k+1$ so $k + 1 \notin E$. Now, $k \leq x$ since $k \in E$.  Now we show that $k$ is unique. Assume there is $m \in \ZZ$ such that $m \leq x < m+1$. Then $m \in E$, so $m \leq L$. But $L$ is the supremum, so $L \geq m$. Thus, $L = m$. So, $k = m$.
\end{proof}
\dfn{$\QQ$}{If $p \in \ZZ$ with $p \neq 0$, then $\exists p^{-1} \in \RR$. Define $\QQ = \{pq^{-1} : p, q \in \ZZ, p \neq 0\}$.}
\section{Density of Rationals}
\thm{Density of the Rationals}{Let $a, b \in \RR$ with $a < b$. Then there is $r \in \QQ$ such that $a < r < b$.}
\begin{proof}
We have $a < b \implies 0 = a + (-a) < b - a \implies 0 < \frac{1}{b-a}$. By the integer part theorem, there is $q \in \ZZ$ such that $\frac{1}{b-a} < q$. So now, $\frac{1}{q} < b -a \implies a < a + \frac{1}{q} < b$. Multiply both sides by $q > 0$ to get $aq < a + 1 < bq$. By the integer part theorem, there is $p \in \ZZ$ such that $p \leq qa < p + 1$ (i.e. $p = \lfloor qa \rfloor$). Since $qa < p + 1 \leq qa + 1 < qb$. Getting rid of unnecessary stuff, we have $qa < p + 1 < qb$. Thus, $a < \frac{p+1}{q} < b$. Let $r = \frac{p+1}{q}$. Then $r \in \QQ$ and $a < r < b$.
\end{proof}
\dfn{Irrational Numbers}{$\RR \setminus \QQ$ is the set of \emph{irrational numbers}.}
\exer{TODO in Recitation 1/23}{
    \begin{itemize}
        \item Prove that there is no $r \in \QQ$ such that $r^2 = 2$.
        \item Prove that ``$\sqrt{2}$'' exists in $\RR$. (prove that there is at least one irrational number)
        \begin{itemize}
            \item Have to play with the set $E = \{ x \in \RR : x > 0, x^2 < 2\}$.
        \end{itemize}
    \end{itemize}}
\thm{Density of Irrationals}{Let $a, b \in \RR$ with $a < b$. Then there is $x \in \RR \setminus \QQ$ such that $a < x < b$.}
\begin{proof}
    $a < b \implies a\sqrt{2} < b\sqrt{2}$. By the density of rationals, there is $r \in \QQ$ such that $a\sqrt{2} < r < b\sqrt{2}$. Then $a < \frac{r}{\sqrt{2}} < b$. Let $x = \frac{r}{\sqrt{2}}$. If $r = 0$, then $a \sqrt{2} < 0 < b \sqrt{2}$. By previous theorem, we can find $q \in \QQ$ such that $a \sqrt{2} < q < 0 < b \sqrt{2}$. Then $a < \frac{q}{\sqrt{2}} < b$. Let $x = \frac{q}{\sqrt{2}}$. Then $x \in \RR \setminus \QQ$ and $a < x < b$.
\end{proof}
\nt{Take $x \in \RR$, $E = \{ r \in \QQ :  r < x\}$. $x$ is the upper bound of $E$. This set is nonempty because we can take $x - 1 < r < x$. Now we prove that $x = \sup E$.
\begin{proof}
    Assume  $\exists L$ upper bound of $E$ such that $L < x$. Then $L < x \implies$ there exists some $r \in \QQ$ such that $L < r < x$, but $r \in E$, so $L$ is not an upper bound of $E$. Thus, $L$ cannot be an upper bound of $E$ and $x$ is the least upper bound of $E$.
\end{proof}}
Since now we know that $\sqrt{2} = \sup\{r \in \QQ : r < \sqrt{2}\}$, we can also define $3^{\sqrt{2}} = \sup\{3^r : r \in \QQ, r < \sqrt{2}\}$.
\dfn{$x^0$}{Let $0 \neq x  \in \RR$. We define $x^0 = 1$.}
\dfn{$x^n$}{Let $x \in \RR$, $n \in \NN$. We start with $x^1 := x$. Then assume $x^m$ has been defined. Then we say $x^{m+1} := x^m \cdot x$.}
\dfn{$x^{p/m}$}{Let $x \in \RR$, $p \in \ZZ$, $m \in \NN$. We say $x^{p/m} = \sqrt[m]{x^p}$.}
\exer{Properties of Exponenets}{Let $x \in \RR$, $r, q \in \QQ$, and $x,r,q > 0$. Prove the following:
\begin{itemize}
    \item $x^r \cdot x^q = x^{r+q}$
    \item $(x^r)^q = (x^q)^r = x^{rq}$
\end{itemize}}
\begin{proof}

\end{proof}
\dfn{Negative Exponent}{Take $x > 0$, $r = -\frac{p}{m}$ for $p,m \in \NN$. First, we have that $x^{-r} := (x^{-1})^{p/m}$.}
\exer{More Properties of Exponents}{Take $x \in \RR, x > 0, r, q \in \QQ$. Prove the following:
\begin{itemize}
    \item If $r > 0$, prove that $x^r > 1$.
    \item If $r < q$, prove that $x^r < x^q$.
\end{itemize}}
\section{1/23 - Recitation - Proving Irrationality of $\sqrt{2}$}
Existence of $\sqrt{2}$:
\begin{enumerate}
    \item Let $E = \{x \in \RR : x >0, x^2 < 2\}$. Prove that $E$ is non-empty and that $E$ is bounded above.
    \begin{proof}
        We know that $0 < 1$ and from that we get $1^2  = 1 < 2$, which can be checked by subtracting $1$ from both sides. As such $E$ is nonempty.

        Now we show that $E$ is bounded above. We know that $2^2 = 4 > 2 > a^2 \in E$, so $2^2 > a^2 \Rightarrow 2 > a$, so $2$ is an upper bound of $E$.
    \end{proof}
    \item By the completeness of $(\RR, \leq)$, $E$ has a supremum, $L$. Prove that $L > 0$ and that $L^2 = 2$.
    \begin{proof}
        Since $L$ is the least upper bound, it has to be greater than 1 which is in the set $E$. Therefore, $L > 1 > 0 \implies L > 0$. 

        Now we show that $L^2 \geq 2$. For sake of contradiction, assume $L^2 < 2$. Since $L > 0$, this means that $ L \in E$. By the density of rationals, there exists $r \in \QQ$ such that $L < r < \sqrt{2}$. Since $L$ is an upper bound of $E$, $r \notin E$. But $r \in \QQ$, so $r^2 \neq 2$. Thus, $r^2 > 2$. Since $r > 0$, $r^2 > 2 \implies r > \sqrt{2}$. But $r < \sqrt{2}$, so we have a contradiction. Thus, $L^2 \geq 2$.
    \end{proof}
    \item Prove that if $y \in \RR \setminus E$ and $y > 0$, then $y$ is an upper bound of $E$.
    \begin{proof}
        Assume $y \in \RR \setminus E$ and $y > 0$. We need to show that $y$ is an upper bound of $E$. Assume for sake of contradiction that $y$ is not an upper bound of $E$. Then there exists $x \in E$ such that $x > y$. But $x \in E \implies x^2 < 2$. Since $y > 0$, $x^2 < 2 \implies y^2 < 2$. But $y \notin E$, so $y^2 \geq 2$. But this would mean that $y \in E$. Contradiction. Thus, $y$ is an upper bound of $E$.
    \end{proof}
    \item Prove that $L^2 = 2$.
    \begin{proof}
        We know that $L^2 \geq 2$ from part 2. Now we show that $L^2 \leq 2$. Assume for sake of contradiction that $L^2 > 2$. 

        How small does $\epsilon > 0$ need to be such that $(L-\epsilon)^2 >2$ as well. 

        Start with $(L-\epsilon)^2 = L^2 - 2L\epsilon + \epsilon^2$, which is greater than $L^2 - 2L\epsilon$ since $\epsilon > 0$. So now, how small does $\epsilon$ need to be such that $L^2 > 2 \implies L^2 - 2L\epsilon > 2$ too. 
        \begin{align*}
            2L\epsilon &< 2 - L^2 \\
            \epsilon &< \frac{2 - L^2}{2L} \\
        \end{align*}
        Since $L^2 > 2$, this means that an $\epsilon$ can be found. This means that $L$ is not the least upper bound. Contradiction. Thus, $L^2 \leq 2$.
    \end{proof}
\end{enumerate}
\newpage
\section{Exponents}
\dfn{$\sqrt{2}$}{$$\sqrt{2} := \sup\{x \in \RR : x > 0, x^2 < 2\}$$}
\exer{}{For $n \in \NN, n \geq 2$. Fix $x > 0$. 
$$E = \{y \in \RR: y > 0, y^n < x\}.$$Prove that $l = \sup E$ satisfies $l^n = x$.}
\begin{proof}
    We first need to show that $\sup E$ exists. Let $y = x / (1+x)$. Then, $0 \leq y < 1$, so $y^n \leq y < x$. Thus, $y \in E$. So, $E$ is nonempty. $E$ is also bounded from above because $x$ is an upper bound of $E$. Thus, $\sup E$ exists by the completeness of $\RR$. Let $l = \sup E$. We now show that $l^n = x$.

    First we show that $l^n \leq x$. FSOC, assume $l^n > x$. If you choose an $\epsilon > 0$ that is small enough, then $(l-\epsilon)^n > x$ as well. We can't do this because $y > l - \epsilon$ for some $y \in E$ since $l$ is the supremum of $E$. As such, we arrive at a contradiction which  means that $l^n \leq x$.

    To show that $l^n \geq x$, assume FSOC that $l^n < x$. Then we can choose an $\epsilon$ such that $(l + \epsilon)^n < x$, meaning we have an element $(l + \epsilon)$ which is in $E$ but bigger than the supremum, which is a contradiction. 
    
    Thus, $l^n \geq x$. 
\end{proof}
\dfn{$\sqrt[m]{x}$}{$$\sqrt[m]{x} := \sup\{y \in \RR : y > 0, y^m < x\}$$}
\dfn{$x^{p/q}$}{$$x^{p/q} := \left( \sqrt[q]{x} \right) ^p$$}
\dfn{$x^q$}{For $q \in \RR$, $q > 0$, and $x > 1$. $$x^q := \sup\{x^r : r \in \QQ, 0 < r < q\}$$} 
\ex{}{$$\sqrt{2} = \sup\{r \in \QQ : r > 0, r < \sqrt{2}\}$$}
\thm{}{Take $a, b \in \RR$, $a, b>  0$ and $x \in \RR > 1$. Then $x^a \cdot x^b = x^{a+b}$.}
\begin{proof}
    Let $E_i = \{ x^r : r \in \QQ, r > 0, r < i\}$. Consider $E_a$, $E_b, E_{a+b}$. Then let $l_i = \sup(E_i)$. Consider $l_a, l_b, l_{a+b}$. We want to show that $l_a \cdot l_b = l_{a+b}$ by showing that both $l_a \cdot l_b \leq l_{a+b}$ and $l_a \cdot l_b \geq l_{a+b}$.

    Let $r \in \QQ$ with $0 < r < a$. Let $s \in \QQ$ with $0 < s < b$. Then we have that $x^r \cdot x^s = x^{r+s}$ (from the exercise two days ago and since $r,s \in \QQ$.) we know that $0 < r + s < a + b$ and is rational. Thus, $x^{r+s} \in E_{a+b}$. Thus, $x^r \cdot x^s \leq l_{a+b}$.

    We want to divide both sides by $x^s$ while fixing $r$. So, we have that $ x ^r \leq \dfrac{l_{a+b}}{x^s}$, which is true for all $r \in \QQ$, such that $0 < r < a$. Thus, $\dfrac{l_{a+b}}{x^s}$ is an upper bound for $E_a$. Thus, $l_a \leq \dfrac{l_{a+b}}{x^s}$. Thus, $ x^s \leq \dfrac{l_{a+b}}{l_a}$, meaning that $\frac{l_{a+b}}{l_a}$ is an upper bound for $E_b$. Thus, $l_b \leq \dfrac{l_{a+b}}{l_a}$. Thus, $l_a \cdot l_b \leq l_{a+b}$.

    Now we show that $l_a \cdot l_b \geq l_{a+b}$. Let $t \in \QQ$ with $0 < t < a + b$. We need $0 < r \in \QQ < a$ and $0 < s \in \QQ < b$ with $t = r + s$. We start by looking at $t - a < b$. By the density of $\QQ$, find $s \in \QQ$ such that $t-a < s < b$. Take $s > 0$ beacuse $b > 0$. So $t -s < a$. By the density of $\QQ$, find $0 < p \in \QQ$ such that $t-s < p < a$. So $t < s + p$. So, $x^t < x^{s+p} = x^{s}x^{p} \leq l_a l_b$ since $x^s \in E_b$ and $x^p \in E_a$. We know that $l_a l_b$ is an upper bound of $E_{a+b}$, so $l_{a+b} \leq l_a l_b$.

    Therefore $l_a \cdot l_b = l_{a+b}$.
\end{proof}
\dfn{Negative Exponents}{Let $x > 1$, $a < 0$. Then: $$x^a := \left(x^{-a}\right)^{-1}$$}
\dfn{Exponents between 0 and 1}{Let $x \in \RR$ with $0 < x < 1$ and $a > 0$. Then:
$$x^a := \left(\dfrac{1}{x}\right)^{-a}$$}
An important note is that if we have $E \subseteq (0, \infty)$ with a bounded $E$. Then if we define $F = \{ \dfrac{1}{x} : x \in E\}$, then we have the following:
\begin{align*}
    \sup E &= \dfrac{1}{\inf F} \\
    \inf E &= \dfrac{1}{\sup F}
\end{align*}
\section{1/25 - Recitation - Sequences of Set}
\dfn{Sequence of a Set}{
Given a set $X$, a sequence on $X$ is a function $f : \NN \to X$. We denote $f(n)$ as $x_n$. We can also denote the sequence as $\{x_n\}_{n=1}^\infty$.}
\dfn{}{Let $(X, \leq)$ be a poset and $\{x_n\}_{n=1}^\infty$ be a sequence on $X$. Then $E = \{x_n : n \in \NN\}$ is a subset of $X$. We say that $\{x_n\}_{n=1}^\infty$ is bounded from above is the set $E$ is bounded from above. We say that $\{x_n\}_{n=1}^\infty$ is bounded from below is the set $E$ is bounded from below. We say that $\{x_n\}_{n=1}^\infty$ is bounded if it is bounded from above and below.}
\dfn{Limit Superior}{Let $(X, \leq)$ be a poset. Let $\{x_n\}_{n=1}^\infty$ be a sequence on $X$. Suppose $\{x_n\}_n$ is bounded from above. Then, we define the \emph{limit superior} of $x_n$ as $n \to \infty$ as:
$$\limsup_{n\to\infty} x_n = \inf_{n \in \NN}\sup_{k \geq n} x_k$$}
\dfn{Limit Inferior}{Let $(X, \leq)$ be a poset. Let $\{x_n\}_{n=1}^\infty$ be a sequence on $X$. Suppose $\{x_n\}_n$ is bounded from below. Then, we define the \emph{limit inferior} of $x_n$ as $n \to \infty$ as:
$$\liminf_{n\to\infty} x_n = \sup_{n \in \NN}\inf_{k \geq n} x_k$$}
\newpage
\exer{}{\begin{enumerate}
    \item Let $\{x_n\}_{n=1}^\infty$ be a sequence on $\RR$ bounded above. Prove that $L \in \RR$ is the $\limsup$ of $\{x_n\}_{n=1}^\infty$ iff for every $\epsilon>0$, there exists $n_\epsilon \in \NN$ such that:
    \begin{enumerate}
        \item $x_n < L + \epsilon$ for all $n \geq n_\epsilon$.
        \item $L - \epsilon < x_n$ for infinitely many $n$.
    \end{enumerate}
\end{enumerate}}
\begin{proof}
    Let $L \in \RR$ be the $\limsup$ of $\{x_n\}_{n=1}^\infty$. Let $\epsilon > 0$. $L$ being the lim sup means that $L = \inf_{n \in \NN}\sup_{k \geq n} x_k$. Thus, $L \leq \sup_{k \geq n} x_k$ for all $n \in \NN$. Thus, $L - \epsilon < \sup_{k \geq n} x_k$ for all $n \in \NN$. Then $L - \epsilon$ is not an upper bound of $\{x_n\}_{n=1}^\infty$. Thus, there is $n_\epsilon \in \NN$ such that $L - \epsilon < x_{n_\epsilon}$. Thus, $L - \epsilon < x_n$ for infinitely many $n$. Now we show that $x_n < L + \epsilon$ for all $n \geq n_\epsilon$. Assume for sake of contradiction that there is $n \geq n_\epsilon$ such that $x_n \geq L + \epsilon$. Then $L + \epsilon$ is an upper bound of $\{x_n\}_{n=1}^\infty$. But $L$ is the $\limsup$, so $L \geq L + \epsilon$. Contradiction. Thus, $x_n < L + \epsilon$ for all $n \geq n_\epsilon$.

    Now we show the other direction. Assume that for every $\epsilon > 0$, there exists $n_\epsilon \in \NN$ such that $x_n < L + \epsilon$ for all $n \geq n_\epsilon$ and $L - \epsilon < x_n$ for infinitely many $n$. We want to show that $L$ is the $\limsup$ of $\{x_n\}_{n=1}^\infty$. We know that $L$ is an upper bound of $\{x_n\}_{n=1}^\infty$. We need to show that $L$ is the least upper bound. Assume for sake of contradiction that $L$ is not the least upper bound. Then there is $L' < L$ such that $L'$ is an upper bound of $\{x_n\}_{n=1}^\infty$. Let $\epsilon = L - L'$. Then $L' < L - \epsilon$. But $L - \epsilon < x_n$ for infinitely many $n$. But $L' < L - \epsilon$, so $L'$ is not an upper bound of $\{x_n\}_{n=1}^\infty$. Contradiction.
\end{proof}
\section{Vector Spaces}
\ex{Vector Spaces}{\begin{itemize}
    \item Euclidean Space $\subseteq \RR^n$. $x \in \RR^n$ is a vector. $x = (x_1, \dots, x_n)$.
    \item Polynomial Space from $\RR \to \RR$. $x \in \RR[x]$. $x = a_0 + a_1x + \dots + a_nx^n$.
    \item $f : [a, b] \to \RR$ continuous functions.
\end{itemize}}
\dfn{Boundedness of Functions}{Let $E$ be a set and $f : E \to \RR$. \begin{enumerate}
    \item $f$ is bounded from above if the set $f(E) = \{ y \in \RR : y = f(x), x \in E\}$ is bounded from above.
    \item $f$ is bounded from below if the set $f(E) = \{ y \in \RR : y = f(x), x \in E\}$ is bounded from below.
    \item $f$ is bounded if $f(E)$ is bounded. 
\end{enumerate}}
\dfn{Inner Product}{A function $(\cdot, \cdot) : V \times V \to \RR$ is an \emph{inner product} if it satisfies the following properties:
\begin{itemize}
    \item $(x, x) \geq 0$ for all $x \in X$. 
    \item $(x, x) = 0$ iff $x = 0$.
    \item $(x, y) = (y, x)$ for all $x, y \in X$.
    \item $(sx + ty, z) = s(x, z) + t(y, z)$ for all $x, y, z \in X$ and $s, t \in \RR$.
\end{itemize}}
\ex{Examples of Inner Products}{\begin{itemize}
    \item $\RR^n$ with dot products.
    \item $f : [a, b] \to \RR$ with $(f, g) = \int_a^b f(x)g(x)dx$. This is is not an inner product because we can define:
    \begin{align*}
        f = \begin{cases} 1 & x = 0.5 \\ 0 & \text{otherwise} \end{cases} \\
    \end{align*}
    which has an integral of 0. But $f \neq 0$.
    If we add that $f$ is continuous, then it is an inner product.
\end{itemize}}
\dfn{Norm}{Let $V$ be a vector space with an inner product $(\cdot, \cdot)$. Then the \emph{norm} of $x \in X$ is defined as $||\cdot||: X \to [0, \infty)$ such that:
\begin{enumerate}
    \item $||x|| = 0 \iff x = 0$
    \item $||tx|| = |t|||x||$ for all $x \in X$
    \item $||x + y|| \leq ||x|| + ||y||$ for all $x, y \in X$
\end{enumerate}}
\ex{Examples of Norms}{\begin{itemize}
    \item $||x|| = \sqrt{(x, x)}$ for $x \in \RR^n$
    \item $X = \{f : E \to \RR, f \text{ bounded}\}$. $||f|| = \sup_{x \in E} |f(x)|$.
    \begin{itemize}
        \item First property is obviously true.
        \item For the second property, we use the fact that $$\sup(tF) = \begin{cases} t\sup(F) & \text{if } t \geq 0 \\ t\inf(F) & \text{if } t< 0 \end{cases}$$
        \item For the third property, we use the triangle inequality:
        \begin{align*}
            \sup|f + g| &\leq \sup|f| + \sup|g| \\
            |f(x) + g(x)| \leq |f(x)| + |g(x)| \leq \sup|f| + \sup|g|
        \end{align*}
    \end{itemize}
\end{itemize}}
\nt{Space of bounded functions denoted as $\ell^\infty (E) = \{f : E \to \RR : f \text{ bounded}\}$.}
\thm{Cauchy Schwarz Inequality}{Let $X$ be a vector space with an inner product $(\cdot, \cdot)$. Then for all $x, y \in X$, we have that $|(x, y)| \leq \sqrt{(x, x)} \cdot \sqrt{(y, y)}$.}
\begin{proof}
    Let $y \neq 0$. Consider $(x + ty, x+ty) = (x, x + ty) + t(y, x + ty) = (x, x) + t(x, y) + t(y, x) + t^2(y, y)$. We can combine the middle terms to get $t^2(y, y) + 2(x, y) + (x,x)$, which is quadratic in $t$. Take $t = -\dfrac{(x, y)}{(y, y)}$. \begin{align*}
        0 &\leq (x, x) - 2\frac{{(x, x)}^2}{(y, y)} + \frac{(x, y)^2}{(y, y)} \\
        0 &\leq (x, x)(y, y) - 2{(x, y)}^2 + {(x, y)}^2 \\
        0 &\leq (x, x)(y, y) - {(x, y)}^2 \\
        {(x, y)}^{2} &\leq (x, x)(y, y) \\
        |(x, y)| &\leq \sqrt{(x, x)} \cdot \sqrt{(y, y)}
    \end{align*}
\end{proof}
\section{Inner Products, Norms, and Metric Spaces}
\thm{}{Let $X$ be a vector space with an inner product $(\cdot, \cdot)$. Then $||x|| := \sqrt{(x, x)}$ is a norm.}
\begin{proof}
    We check the properties of norms:
    \begin{enumerate}
        \item $||x|| = 0 \iff \sqrt{(x, x)} = 0 \iff (x, x) = 0 \iff x = 0$.
        \item $||tx|| = \sqrt{(tx, tx)} = \sqrt{t^2(x, x)} = |t|\sqrt{(x, x)} = |t|||x||$.
        \item $||x + y||^2 = (x + y, x + y) = (x, x) + 2(x, y) + (y, y) = ||x||^2 + 2(x, y) + ||y||^2 \leq ||x||^2 + 2|(x, y)| + ||y||^2 \leq ||x||^2 + 2||x|| \cdot ||y|| + ||y||^2 = (||x|| + ||y||)^2$.
    \end{enumerate}
\end{proof}
\cor{Parallelogram Identity}{Let $X$ be a vector space with inner product $(\cdot, \cdot)$. Then for all $x, y \in X$, we have that \[||x + y||^2 + ||x - y||^2 = 2||x||^2 + 2||y||^2\]}
\begin{proof}
    \begin{align*}
        ||x + y||^2 + ||x - y||^2 &= (x + y, x + y) + (x - y, x - y) \\
        &= (x, x) + 2(x, y) + (y, y) + (x, x) - 2(x, y) + (y, y) \\
        &= 2(x, x) + 2(y, y) \\
        &= 2||x||^2 + 2||y||^2
    \end{align*}
\end{proof}
\noindent If we subtract them instead, we get \begin{align*}
    \frac{||x + y||^2 - ||x-y||^2}{4} = (x, y) \tag{*}
\end{align*}
So, if $||\cdot||$ is a norm, then if i want to define an inner product, I can use *.
\exer{}{Let $||\cdot||$ be a norm. Then $(x, y) := \frac{1}{4}(||x+y||^2 - ||x-y||^2)$ is an inner product iif the parallelogram identity holds.} 
\noindent Linearity of inner products is the hard part to prove because we have to consider:
\begin{itemize}
    \item $t \in \NN$
    \item $t = \dfrac{1}{2}$
    \item $t \in \QQ$
    \item $t \in \RR$ (density of $\QQ$)
\end{itemize}
\nt{For recitation: 
\begin{enumerate}
    \item $X = \{f : E \to \RR \text{ bounded}\}, ||f|| = \sup_E |f|$, does not satisfy the parallelogram identity.
    \item $x \in \RR^N$, $||x||_1 = |x_1| + |x_2| + \cdots + |x_N|$ does not satisfy the parallelogram identity.
\end{enumerate}}
\dfn{Metric}{Let $X$ be a set. A \emph{metric} on $X$ is a function $d : X \times X \to [0, \infty)$ such that:
\begin{enumerate}
    \item $d(x, y) = 0 \iff x = y$
    \item $d(x, y) = d(y, x)$ for all $x, y \in X$
    \item $d(x, z) \leq d(x, y) + d(y, z)$ for all $x, y, z \in X$
\end{enumerate}}
\dfn{Metric Space}{A set $X$ with a metric $d$ is called a \emph{metric space} and is denoted as $(X, d)$.}
\ex{Metrics}{Let $X$ be a set. Then the following is a metric on $X$:
\[ d(x ,y) = \begin{cases} 0 & x = y \\ 1 & x \neq y \end{cases} \]}
\thm{If $X$ is a vector space with $||\cdot||$ as a norm. Then \[d(x,y) := ||x -y||\] is a metric on $X$.}

\begin{proof}
    We check all the properties of metrics.
    \begin{itemize}
        \item $d(x, y) = 0 = ||x - y|| \Rightarrow 0 = x -y \iff x = y$.
        \item $d(x, y) = ||x - y|| = ||y - x|| = d(y, x)$.
        \item $d(x, y) = ||x - y|| = ||x - z + z - y|| \leq ||x - z|| + ||z - y|| = d(x, z) + d(z, y)$.
    \end{itemize}
\end{proof}
\newpage
\ex{}{Let's define \[d(x,y) = \left| \frac{x}{1+|x| } - \frac{y}{1 + |y|}\right| \] as a metric on $\RR$. However, this is not a norm because $d(tx, ty) \neq td(x, y)$.}

\dfn{Ball}{Let $(X, d)$ be a metric space. Let $x \in X$ and $r > 0$. Then the \emph{ball} of radius $r$ centered at $x$ is defined as $B_r(x) = \{y \in X : d(x, y) < r\}$.}

\ex{}{
\begin{itemize}
    \item Take $X = \RR^2$ with $(x, y) \in \RR$. Then defin $||(x, y)||_\infty = \max(|x|, |y|)$ is a norm. Take $B((0, 0), 1) = \{(x, y) \in \RR^2 : ||(x, y) - (0, 0)||_\infty < 1\}$. This is a square with vertices $(1, 1), (-1, 1), (-1, -1), (1, -1)$.
    \item If we have $||(x, y)||_1 = |x| + |y|$, then $B((0, 0), 1) = \{(x, y) \in \RR^2 : ||(x, y) - (0, 0)||_1 < 1\}$. This is a square with vertices $(1, 0), (0, 1), (-1, 0), (0, -1)$.
\end{itemize}}

\dfn{Interior}{Let $(X, d)$ be a metric space and $E \subseteq X$. $x \in E$ is called an \emph{interior point} of $E$ if there is $B(x, r) \subseteq E$. The set of all interior points of $E$ is called the \emph{interior} of $E$ and is denoted as $E^\circ$.}

\dfn{Open Set}{$E$ is \emph{open} if $E = E^\circ$.}


\section{Open Sets}
\ex{Balls}{$B(x , r)$ is open.
\begin{proof}
    Let $ y \in B(x, r)$ and take $B(y, r - d(x, y))$. Let $z \in B(y, r - d(x, y))$. Then $d(x, z) \leq d(x, y) + d(y, z) < d(x, y) + r - d(x, y) = r$. Thus, $z \in B(x, r)$. Thus, $B(y, r - d(x, y)) \subseteq B(x, r)$. Thus, $B(x, r)$ is open.
\end{proof}}

\ex{$\RR$}{
    \begin{enumerate}
        \item $E = (0, 1) \cap \QQ$ is not open. Because the irrationals are dense, we can always find a rational number in any ball. Thus, $E^\circ = \emptyset$.
        \item $E = (3, 4)$ is open. Let $x \in E$. Take $B(x, \min(x - 3, 4 - x))$. Then $B(x, \min(x - 3, 4 - x)) \subseteq E$. Thus, $E$ is open.
        \item $E = [3, 4)$ is not open. $E^\circ = (3, 4)$.
        \item $E = \{x \in \RR : x^3 - 3x + 4 > 0\}$. This is open and we'll be able to use continuity to prove this easily later.
        \item $l^\infty([0, 1]) = \{ f : [0, 1] \to \RR \text{ bounded}\}$. $||f||_\infty = \sup_{[0, 1]} |f|$. $d(f, g) = ||f - g||_\infty$. $E = \{f \in l^\infty([0, 1]) : f(x) > 0 \ \forall x \in [0, 1]\}$ is open? (finish in recitation)
    \end{enumerate}
}
\noindent Properties of open sets $(X, d)$:
\begin{itemize}
    \item $\emptyset$ is open. $X$ is open.
    \item Infinite intersections of open sets are not necessarily open. For example, we have $\bigcap_{n=1}^\infty (-1/n, 1/n) = \{0\}$, which is not open.
    \item Finite intersections of open sets are open. Consider $U_1, \ldots U_n$. Let $x \in \bigcap_{i=1}^n U_i$. Then $x \in U_i$ for all $i$. Since $U_i$ is open, there exists $r_i > 0$ such that $B(x, r_i) \subseteq U_i$. Let $r = \min(r_1, \dots, r_n)$. Then $B(x, r) \subseteq U_i$ for all $i$. Thus, $B(x, r) \subseteq \bigcap_{i=1}^n U_i$.
    \item Unions of open sets are open because if a point in the union is contained in one of the open sets, then there is a ball in that set that is contained in the union.
\end{itemize}

\dfn{Topological Space}{Let $X$ be a set. A \emph{topology} on $X$ is a collection $\mathcal{T}$ of subsets of $X$ such that:
\begin{enumerate}
    \item $\emptyset, X \in \mathcal{T}$.
    \item If $U_1, \ldots, U_n \in \mathcal{T}$, then $\bigcap_{i=1}^n U_i \in \mathcal{T}$. (finite intersections)
    \item If $U_\alpha \in \mathcal{T}$ for all $\alpha \in A$, then $\bigcup_{\alpha \in A} U_\alpha \in \mathcal{T}$. (arbitrary unions)
\end{enumerate}Elements of $\mathcal{T}$ are called open sets.}
\dfn{Closed}{Let $(X, d)$ be a metric space. We say $C \subseteq X$ is \emph{closed} if $X \setminus C$ is open.}
\noindent Note that $X$ and $\emptyset$ are both open and closed.
\ex{Open and Closed Sets}{\begin{itemize}
    \item $[0, 1)$ is not open or closed.
    \item $[0, 1]$ is closed.
\end{itemize}}
\noindent Properties of closed sets:
\begin{itemize}
    \item $\emptyset$ and $X$ are closed.
    \item Infinite intersections of closed sets are closed. (De Morgan's Law)
    \item Finite unions of closed sets are closed. For example, if we have $\bigcup_{m = 1}^\infty (-\infty, -\dfrac{1}{m}) = (-\infty, 0)$ which is closed.
\end{itemize}
\newpage
\section{2/1 - Rectitation}
Recall:
\begin{enumerate}
    \item Let $\{x_n\}$ be a sequence bounded above in $\RR$. Then $L \in \RR$ is the limit superior of $\{x_n\}$ if for every $\epsilon > 0$, there exists $n_\epsilon \in \NN$ such that:
    \begin{enumerate}
        \item $x_n < L + \epsilon$ for all $n \geq n_\epsilon$.
        \item $ x_n > L - \epsilon$ for infinitely many $n$.
    \end{enumerate}
    \item Let $\{x_n\}$ be a sequence bounded below in $\RR$. Then $L \in \RR$ is the limit inferior of $\{x_n\}$ if for every $\epsilon > 0$, there exists $n_\epsilon \in \NN$ such that:
    \begin{enumerate}
        \item $x_n < L + \epsilon$ for infinitely many $n$.
        \item $x_n > L - \epsilon$ for all $n \geq n_\epsilon$.
    \end{enumerate}
\end{enumerate}
Now consider the following sequence: \[ x_n = (-1)^n \frac{2n}{n+1} \in \RR\]
Prove that $\limsup_{n \to \infty} x_n = 2$.
\begin{proof}
    We need to show that for every $\epsilon > 0$, there exists $n_\epsilon \in \NN$ such that:
    \begin{enumerate}
        \item $x_n < 2 + \epsilon$ for all $n \geq n_\epsilon$.
        \item $2 - \epsilon < x_n$ for infinitely many $n$.
    \end{enumerate}
    Let $\epsilon > 0$. We need to find $n_\epsilon \in \NN$ such that $x_n < 2 + \epsilon$ for all $n \geq n_\epsilon$ and $2 - \epsilon < x_n$ for infinitely many $n$. We can find $n_\epsilon \in \NN$ such that $2 - \epsilon < x_n$ for all $n \geq n_\epsilon$. Then $x_n < 2 + \epsilon$ for all $n \geq n_\epsilon$. Thus, $\limsup_{n \to \infty} x_n = 2$.
\end{proof}

\noindent Now prove that for any $\{x_n\}$ in $\RR$, prove that $\liminf_{n \to \infty} x_n \leq \limsup_{n \to \infty} x_n$.
\begin{proof}
    Comes quickly from properties of limits and that the inf is less than the sup.
\end{proof}

\noindent Now prove that $\liminf_{n \to \infty} -x_n = -\limsup_{n \to \infty} x_n$ and that $\limsup_{n \to \infty} -x_n = -\liminf_{n \to \infty} x_n$.
\begin{proof}
    We start by using the property that $\inf(-E) = -\sup(E)$. Then we use the property that $\sup(-E) = -\inf(E)$.

    So, \begin{align*}
        \liminf_{n \to \infty} -x_n &= \sup_{n \in \NN}\inf_{k \geq n} -x_k \\
        &= \sup_{n \in \NN} -\sup_{k \geq n} x_k \\
        &= -\inf_{n \in \NN}\sup_{k \geq n} x_k \\
        &= -\limsup_{n \to \infty} x_n
    \end{align*}
\end{proof}
\newpage
\section{Closure}
\dfn{Closure}{Let $(X, d)$ be a metric space with $A \subset X$. Then the \emph{closure} of $A$ is defined as $\overline{A}$, the intersection of all sets that contain $E$.}
\dfn{Boundary Point}{Let $(X, d)$ be a metric space with $E \subseteq X$. Then $x \in X$ is a \emph{boundary point} of $E$ if for every $r > 0$, $B(x, r) \cap E \neq \emptyset$ and $B(x, r) \cap (X \setminus E) \neq \emptyset$. The set of all boundary points is denoted as $\partial E$.}
\thm{}{Let $(X, d)$ be a metric space and $E \subseteq X$. Then $\overline{E} = E \cup \partial E$.}
\begin{proof}
    Let $x \in \overline{E}$. FSOC, assume $x \notin E \cup \partial E$. Since $x \notin \partial E$, there exists $r > 0$ such that $B(x, r)$ that doesn't intersect with either $E$ or complement of $E$. But since $x\notin E$, only the second option can occur. So there exists $r$ such that $B(x, r) \cap E = \emptyset$. Because of that and the fact that $B(x, r)$ is open, it follows that $X \setminus B(x, r)$ is closed and contains $E$. By the definition of $\overline{E}$, we have that $\overline{E} \subseteq X \setminus B(x, r)$. But this is a contradiction because $x \in \overline{E}$. 

    Conversely, let $x \in E \cup \partial E$ and assume $x \notin \overline{E}$. Since $\overline{E}$ is closed, $X \setminus \overline{E}$ is open. Using the fact that $x \in E \cup \partial E$, we have that we can find a $B(x, r) \in X \setminus \overline{E}$. But this is a contradiction because $B(x, r)$ is open and contains $E$. Thus, $E \cup \partial E \subseteq \overline{E}$.
\end{proof}
\dfn{Accumulation Point}{Let $(X, d)$ be a metric space with $E \subseteq X$. Then $x \in X$ is an \emph{accumulation point} of $E$ if for every $r > 0$, there exists $y \in E$ such that $y \neq x$ and $d(x, y) < r$.}

\dfn{Interval}{$I \subseteq \RR$ is an \emph{interval} if we have that $z \in I$ for all $x < z < y$.}
\dfn{Rectangle}{$R \subseteq \RR^N$ is a \emph{rectangle} if $R = I_1 \times \cdots \times I_N$ where $I_1, \ldots, I_N$ are intervals in $\RR$.}
\dfn{Sequence}{Let $X$ be a set. A \emph{sequence} is a function $f : \NN \to X$. We denote $f(n)$ as $x_n$.}
\dfn{Convergent Sequence}{Let $(X, d)$ be a metric space. A sequence $\{x_n\}_{n=1}^\infty$ is \emph{convergent} if there exists $x \in X$ such that for every $\epsilon > 0$, there exists $n_\epsilon \in \NN$ such that $d(x, x_n) < \epsilon$ for all $n \geq n_\epsilon$. We write $x_n \to x$ as $n \to \infty$ or $\lim_{n \to \infty} x_n = x$.}
\newpage
\section{Bolzano-Weierstrass}
\thm{Bolzano-Weierstrauss}{If $E \subset \RR^N$ is bounded and contains infinitely many distinct points, then $E$ has an accumulation point}
\begin{proof}
    \mlenma{1}{If $[a_n, b_n] \supseteq [a_{n+1}, b_{n+1}]$ for all $n$, then $\bigcap_{n=1}^\infty [a_n, b_n] \neq \emptyset$.}
    \begin{proof}
        For all $a_n$ and $b_n$, we have:
        \begin{align*}
            a_1 &\leq a_2 \leq \cdots \\
            b_1 &\geq b_2 \geq \cdots
        \end{align*}
        Let \[A := \{a_1, a_2, \ldots \}.\] We have that $a_n \leq b_n \leq b_1$ for all $n$. So $A$ is bounded above, so by the supremum property, there exists $x = \sup A \in \RR$ and $a_n \leq x$ for all $n \in \NN$. We claim that $x \leq b_n$ as well. If not, then there exists $m \in \NN$ such that $b_m < x$. Since $x$ is an upper bound of $A$, we'll have that there's an $n \in \NN$ such that $b_m < a_n \leq x$. Find $k \geq m, n$, then we have $b_m < a_n \leq a_k \leq b_k \leq b_m$, which is a contradiction. This proves the claim. Hence, $x \in [a_n, b_n]$ for all $n$. Thus, $x \in \bigcap_{n=1}^\infty [a_n, b_n]$.
    \end{proof}
    \mlenma{2}{Let $R_n$ be a closed and bounded rectangle. Assume that $R_1 \supseteq R_2 \supseteq \cdots$. Then $\bigcap_{n=1}^\infty R_n \neq \emptyset$.}
    \begin{proof}
        We know that \begin{align*}
            R_n &= [a_{1, n}, b_{1, n}] \times \cdots \times [a_{N, n}, b_{N, n}] \\
            R_{n+1} &= [a_{1, n+1}, b_{1, n+1}] \times \cdots \times [a_{N, n+1}, b_{N, n+1}]
        \end{align*}
        We can apply lemma 1 $N$ times (for each of the components of $R_n$) to find that $x_1, x_2, \ldots, x_N \in \RR$ such that $a_{i, n} \leq x_i \leq b_{i, n}$ for all $1 \leq i \leq N$. Then, if you take $x = (x_1, \ldots, x_N)$, then $x \in R_m$ for all $n$. Thus, $x \in \bigcap_{n=1}^\infty R_n$. 
    \end{proof}
    \mlenma{3}{Let $(X, d)$ be a metric space with $E \subseteq X$. Then $x \in X$ is an accumulation point of $E$ if and only if there exists a sequence $\{x_n\}_{n=1}^\infty$ in $E$ such that $x_n \to x$ as $n \to \infty$.}
    \begin{proof}
        Let $x \in X$ be an accumulation point of $E$. Take $r = \dfrac{1}{n}$. Find $x_n \in B\left(x, \dfrac{1}{n}\right) \cap E$ with $x_n \neq x$. We claim $x_n \to x$. Given $\epsilon >0$, find $n_\epsilon \geq \dfrac{1}{\epsilon}$. Then $d(x, x_n) < \dfrac{1}{n} \leq \dfrac{1}{n_\epsilon}$ for all $n \geq n_\epsilon$. Thus, $x_n \to x$ as $n \to \infty$.

        Let $\{x_n\}_{n=1}^\infty$ be a sequence in $E$ such that $x_n \to x$ as $n \to \infty$. We claim that $x \in \text{acc}(E)$. Let $r >0$ and take $\epsilon = r$. Then there exists $n_\epsilon \in \NN$ such that $d(x, x_n) < \epsilon = r$ for all $n \geq n_\epsilon$. Thus, $x_n \in B(x, r) \cap E$ for all $n \geq n_\epsilon$. Thus, $x \in \text{acc}(E)$.
    \end{proof}
    Now we prove the actual theorem. Let $E \subseteq \RR^N$ be bounded. $E \subseteq B(0, r)$ for some $r$. Let $Q_1$ be the closed cube centered at $0$ with sidelength $2r$. Pick some point $x_1 \in E \subseteq Q_1$. Subdivide $Q_1$ into $2^N$ closed cubes of sidelength $\dfrac{2r}{2}$. Let $Q_2$ be the closed cube containing $x_1$. Pick some point $x_2 \in E \cap Q_2$ with $x_2 \neq x_1$. Inductively, assume $Q_1 \supseteq Q_2 \supseteq \cdots \supseteq Q_n$ have been chosen. Then $Q_{n}$ is a closed cube of sidelength $\dfrac{2r}{2^{n-1}}$ containing $x_n$. Each $Q_n$ contains infinitely many elements of $E$. Assume also that $x_1, x_2, \cdots x_n \in E$ have been chosen with $x_i \in Q_i$ and $x_i \neq x_j$ for $i \neq j$. 

    Now we can subdivide $Q_n$ to get $Q_{n+1}$ and continue this process infinitely.

    By Lemma 2, we know that $\bigcap_{n=1}^\infty Q_n \neq \emptyset$. Let $x \in \bigcap_{n=1}^\infty Q_n$. Now we need to show there exists a sequence $\{x_n\}_{n=1}^\infty$ in $E$ such that $x_n \to x$ as $n \to \infty$ but $x_i \neq x$ for any $i$ because then the rest of the points won't converge to $x$. If $x = x_i$ for some $i$, we can just pick another point.

    So WLOG, assume $x_n \neq x$ for any $n$. So we claim $x_n \to x$ as $n \to \infty$. We know that in $Q_n$, the difference between any two points in this cube is given by:
    \begin{align*}
        ||x_n - x|| = \sqrt{(x_{n, 1} - x_1)^2 + (x_{n, 2} - x_2)^2 + \cdots + (x_{n, N} - x_N)^2} \leq \sqrt{\frac{2r}{2^{n-1}} + \frac{2r}{2^{n-1}} + \cdots + \frac{2r}{2^{n-1}}} = \sqrt{N}\frac{2r}{2^{n-1}}
    \end{align*}
    This value is less than $\epsilon$ for all large $n$, so this concludes the proof.
\end{proof}
\section{2/6 - Recitation - Spaces}
Let $X = \{ f : [0, 1] \to \RR \text{ bounded}\}$. Define $\Vert f \Vert = \sup_{x \in [0, 1]} |f(x)|$. Prove that $(X, \Vert \cdot \Vert)$ does not suffice parallelogram identity. That is, show a counterexample to the parallelogram identity, which is \[ \Vert f + g \Vert^2 + \Vert f - g \Vert^2 = 2\Vert f \Vert^2 + 2\Vert g \Vert^2\]
\begin{proof}
    Counterexample: Let $f(x) = x$ and $g(x) = 1$. 
\end{proof}
Now given a normed space which satisifies the parallelogram identity, can we define an inner product?
\begin{proof}
    Yes. We can define $(f, g) = \frac{1}{4}(\Vert f + g \Vert^2 - \Vert f - g \Vert^2)$. We can prove that this is an inner product.

    Linearity of products because the other properties are easy to prove. We need to show that $(x + y, z) = (x, z) + (y, z)$. I'm so lazy so I won't tbh.

    We now show that $(tx, y) = t(x, y) \forall t \in \ZZ$. We proceed with induction for $t \in \ZZ^+$
    
    Our two base cases are $t = 0, 1$. For $t = 0$, we have that $(0x, y) = (0, y) = 0 = 0(0, y)$. For $t = 1$, we have that $(x, y) = (x, y) = 1(x, y)$.

    Now we assume that $(tx, y) = t(x, y)$ for some $t \in \ZZ^+$. Then we have that $(t+1)x = tx + x$. Then we have that $(t+1)x, y = (tx + x, y) = (tx, y) + (x, y) = t(x, y) + (x, y) = (t+1)(x, y)$. Thus, we have that $(tx, y) = t(x, y)$ for all $t \in \ZZ^+$.

    Now we have to deal with $t \in \ZZ^-$. We have that $(tx, y) = -t(-x, y) = -t(x, y) = t(x, y)$. Thus, we have that $(tx, y) = t(x, y)$ for all $t \in \ZZ$.

    To proceed, we deal with $t \in \QQ$. We have that $t = \frac{m}{n}$ for some $m, n \in \ZZ$. Then we have that $n(tx, y) = (ntx, y) = (mx, y) = m(x, y) = t(mx, y) = t(n(x, y))$. Thus, we have that $n(tx, y) = t(n(x, y))$. Thus, we have that $(tx, y) = t(x, y)$ for all $t \in \QQ$.
\end{proof}


\section{Compactness}
\dfn{Subsequence}{Let $X$ be a set and $f : \NN \to X$ a sequence. Let $g: \NN \to \NN$ be strictly increasing. Then $f \circ g : \NN \to X$ is a \emph{subsequence} of $f$. We denote $m_k$ as $g(k)$, so $f(g(k)) = f(m_k) = x_{m_k}$. So we denote the whole sequence as $\{x_{m_k}\}_{k}$.}


\dfn{Sequentially Compact}{Let $(X, d)$ be a metric space. $K \subseteq X$ is \emph{sequentially compact} if every sequence $\{x_n\}_n$ in $K$ and there exists a subsequence $\{x_{n_k}\}_k$ such that $x_{n_k} \to x$ as $k \to \infty$ for some $x \in K$.}

\ex{$\RR$}{
    \begin{enumerate}
        \item $(0, 1]$ is not sequentially compact. Consider the sequence $x_n = \dfrac{1}{n}$. This sequence has no convergent subsequence that tends to $0$ since $0$ is not in the set. The issue is that it's not closed.
        \item $[0, \infty)$ is not sequentially compact. Consider the sequence $x_n = n$. This sequence has no convergent subsequence that tends to $\infty$ since $\infty$ is not in the set. So, $[0, \infty)$ is not sequentially compact. The issue is that it's not bounded.
    \end{enumerate}
}
\newpage
\thm{}{Let $(X, d)$ be a metric space. If $K \subseteq X$ is sequentially compact, then $K$ is closed and bounded. }
\begin{proof}
    Claim: $K$ is closed. We want $X \setminus K$ to be open. Let $x \in X \setminus K$. We want $B(x, r) \in X \setminus K$ for some $r > 0$. By contradiction, for all $r > 0$, assume $\exists y \in B(x, r) \cap K$. Take $r = \dfrac{1}{m} \Rightarrow y_m \in B(x, \dfrac{1}{m}) \cap K$. $d(y_m, x) < \dfrac{1}{m} \to 0$, so $y_m \to x$. But $x \notin K$ even though $y_m \in K$. This is a contradiction, so $K$ is closed.

    Claim: $K$ is bounded. By contradiction, assume $K$ is not bounded. Let $x_0 \in X$. Then $K \not\subseteq B(x_0, r)$ for any $r > 0$. Take $r = n$. Then $\exists x_n \in K$ such that $d(x_n, x_0) \geq n$. So $\{x_n\}_n \in K$. $K$ is sequentially compact, so there exists a subsequence $\{x_{n_k}\}_k$ such that $x_{n_k} \to x$ as $k \to \infty$ for some $x \in K$. But $n_k \leq d(x_{n_k}, x_0) \leq d(x_{n_k}, x) + d(x, x_0)$. But $d(x_{n_k}, x) \to 0$ as $k \to \infty$, so $n_k \to \infty < d(x_{m_k}, x_0) \leq d(x, x_0)$ which is a fixed number, so we have a contradiction. As such, $K$ is bounded.
\end{proof}
\thm{}{Let $K \subseteq \RR^N$. Then $K$ is sequentially compact if and only if $K$ is closed and bounded.}
\begin{proof}
    We just showed the first direction. So, we need to show that if $K$ is closed and bounded, then $K$ is sequentially compact. 

    So now, assume $K$ is closed and bounded. Let $\{x_n\}_n$ be a sequence in $K$. We want to show that there exists a subsequence $\{x_{n_k}\}_k$ such that $x_{n_k} \to x$ as $k \to \infty$ for some $x \in K$. 

    Consider the set $E = \{ x_n : n \in \NN\} \subseteq \RR_N$. We now case on whether $E$ has infinitely many distinct points or not. 
    
    If $E$ doesn't have infinitely many distinct points, there exists $x  \in K$ such that $x_n = x$ for einfinitely many $n$. Then $x_{n_k} = x$ for all $k$, so $x_{n_k} \to x$ as $k \to \infty$.

    Now we consider the case where Bolzano-Weierstrass applies. By B-W, $E$ has an accumulation point $x \in \RR^N$. So we can find a subsequence $\{x_{n_k}\}_k$ such that $x_{n_k} \to x$ as $k \to \infty$. But $x \in K$ because $K$ is closed. Thus, $K$ is sequentially compact.
\end{proof}
\nt{Let $(X, \Vert \cdot \Vert)$ be a normed space. If every closed and bounded set is sequentially compact, then $X$ has finite dimension.}

\exer{}{Recall $l^\infty([0, 1]) = \{ f : [0, 1] \to \RR \text{ bounded}\}$. Define $\Vert f \Vert_\infty = \sup_{x \in [0, 1]} |f(x)|$. $B(0, 1) = \{g \in l^\infty([0, 1]) : \Vert g \Vert_\infty < 1\}$. Prove that $B(0, 1) = \{g \in l^\infty([0, 1]) : |g(x)| < 1 \ \forall x \in [0, 1]\}$. Also prove that this not sequentially compact.}
\newpage
\section{2/8 - Recitation}
Let $n \in \NN$, $x,y\in \RR$. \begin{enumerate}
    \item Prove that $x^n - y^n = (x-y)(x^{n-1} + x^{n-2}y + \cdots + xy^{n-2} + y^{n-1})$.
    \begin{proof}
        Base case: $n=1$ is trivial.

        Now assume that for any $n \in \NN$, $x^n - y^n = (x-y)(x^{n-1} + x^{n-2}y + \cdots + xy^{n-2} + y^{n-1})$. We want to show that this is true for $n+1$. We have that $x^{n+1} - y^{n+1} = x(x^n - y^n) + y^n(x - y) = x(x-y)(x^{n-1} + x^{n-2}y + \cdots + xy^{n-2} + y^{n-1}) + y^n(x-y)$. Then we get $(x-y)(x^{n} + x^{n-1}y + \cdots + xy^{n-1} + y^{n}) = (x-y)(x^{n} + x^{n-1}y + \cdots + xy^{n-1} + y^{n})$.
    \end{proof}
    \item Prove that when $|x-y| \leq 1$, then $|x^n - y^n| \leq n(1 + |x|)^{n-1}|x-y|$.
    \begin{proof}
       Let $|x-y| \leq 1$. Then we have that $|x^n - y^n| = |(x-y)(x^{n-1} + x^{n-2}y + \cdots + xy^{n-2} + y^{n-1})| \leq |x-y|(|x^{n-1}| + |x^{n-2}y| + \cdots + |xy^{n-2}| + |y^{n-1}|) \leq |x-y|(|x^{n-1}| + |x^{n-2}||y| + \cdots + |x||y^{n-2}| + |y^{n-1}|) \leq |x-y|(|x^{n-1}| + |x^{n-2}||y| + \cdots + |x||y^{n-2}| + |y^{n-1}|) \leq |x-y|(|x^{n-1}| + |x^{n-2}| + \cdots + |x| + 1) \leq n(1 + |x|)^{n-1}|x-y|$.
    \end{proof}
    \item Let $E = \{x \in \RR : x^n > 3\}$ for a fixed $n$. Prove that $E$ is open.
    \begin{proof}
        Let $x \in E$. We want to show that there is an $r>0$ such that $B(x, r) \subseteq E$. Take $r = \dfrac{x^n - 3}{n(1 + |x|)^{n-1}}$ and take $y \in B(x, r)$. Then $|x-y| < r \Rightarrow |x^n| - |y^n| \leq |x^n - y^n| \leq n(1 + |x|)^{n-1}|x-y| < n(1 + |x|)^{n-1}r < x^n - 3$. Then $y^n \geq x^n - n(1 + |x|)^{n-1}r > 3$. Thus, $y \in E$. Thus, $B(x, r) \subseteq E$. Thus, $E$ is open.
    \end{proof}
    \item Consider the space $l^\infty([0, 1]) = \{ f : [0, 1] \to \RR \text{ bounded}\}$. Define $\Vert f \Vert_\infty = \sup_{x \in [0, 1]} |f(x)|$. 
    
    Let $E = \{f \in l^\infty([0, 1]) : f(x) > 0 \ \forall x \in [0, 1]\}$. Prove that $E$ is not open.
    \begin{proof}
        Consider \[f(x) = \begin{cases} x & x \in [0, 1) \\ 1 & x = 1 \end{cases}\] Then let $r>0$ and consider $g(x) = f(x) \cdot \dfrac{r}{2}$. Then $g(x) \in B(f, r)$. But $g(x) \notin E$ because $g(1) = \dfrac{r}{2}$. Thus, $B(f, r) \not\subseteq E$. Thus, $E$ is not open.
    \end{proof}
\end{enumerate}

\section{Limits}
\dfn{Limits}{Let $(X, d_X)$ and $(Y, d_Y)$ be metric spaces, $E \subseteq X$, $f:E \to Y$. Let $x_0 \in \operatorname{acc}E$. 

Take $l \in Y$. $l$ is the \emph{limit} of $f$ as $x \to x_0$. We write $\lim_{x \to x_0} f(x) = l$ if for every $\epsilon > 0$, there exists $\delta > 0$ such that $0 < d_X(x, x_0) < \delta \Rightarrow d_Y(f(x), l) < \epsilon$. We can also write it as $f(x) \to l$ as $x \to x_0$.}
\nt{Even if $x_0 \in E$, you don't take in the definition for the limit.}

\thm{}{Let $(X, d_X)$ and $(Y, d_Y)$ be metric spaces, $E \subseteq X$, $f:E \to Y$, and $x_0 \in \operatorname{acc}E$. If $\lim_{x \to x_0} f(x)$ exists, then it is unique.}
\begin{proof}
    Assume that $\lim_{x \to x_0} f(x) = l$ and $\lim_{x \to x_0} f(x) = m$. Take $\epsilon = \frac{d_Y(l, m)}{2} > 0$. Then there exists $\delta_1 > 0$ such that $0 < d_X(x, x_0) < \delta_1 \Rightarrow d_Y(f(x), l) < \epsilon$. There also exists $\delta_2 > 0$ such that $0 < d_X(x, x_0) < \delta_2 \Rightarrow d_Y(f(x), m) < \epsilon$. Take $\delta = \min(\delta_1, \delta_2)$. Then $0 < d_X(x, x_0) < \delta \Rightarrow d_Y(f(x), l) < \epsilon$ and $d_Y(f(x), m) < \epsilon$. Then $d_Y(l, m) \leq d_Y(l, f(x)) + d_Y(f(x), m) < 2\epsilon = d_Y(l, m)$. This is a contradiction, so $l = m$.
\end{proof}
\ex{$\RR^2$}{Take $(x_0, y_0) \in \RR^2$ and $y_0 \neq 0$. Compute \[\lim_{(x, y) \to (x_0, y_0)} \dfrac{x}{y}\]
We want to show that this is $\dfrac{x_0}{y_0}$. We have the set $E = \{(x, y) \in \RR^2 : y \neq 0 \}$. We also know that $(x_0, y_0) \in \operatorname{acc}E$. What we know if that $(x, y) \to (x_0, y_0)$: $|x-x_0|$ and $|y-y_0|$ are going to be small. Then 
\begin{align*}
    \left| f(x, y) - \frac{x_0}{y_0}\right| &= \left| \frac{x}{y} - \frac{x_0}{y_0} \right| \\
    &= \left| \frac{xy_0 - x_0y_0}{yy_0} \right| \\
    &= \left| \frac{xy_0 - x_0y_0 + x_0y_0 - x_0y}{yy_0} \right| \\
    &= \left| \frac{y_0(x-x_0) + x_0(y_0 - y)}{yy_0} \right| \\
    &\leq \frac{|y_0| |x-x_0| + |x_0| |y_0 - y|}{|y| |y_0|} \\
    &= \frac{|x-x_0|}{|y|} + \frac{|x_0| |y_0 - y|}{|y| |y_0|}
\end{align*} 
Then we have $\delta < \frac{|y_0|}{2}$. If $|y-y_0| < \delta < \frac{y_0}{2}$, then we get $|y| \geq \frac{|y_0|}{2} \Rightarrow \frac{1}{|y|} \leq \frac{2}{|y_0|}$. 
\begin{align*}
    \frac{|x-x_0|}{|y|} + \frac{|x_0| |y_0 - y|}{|y| |y_0|} &\leq \frac{2|x-x_0|}{|y_0|} + \frac{2|x_0| |y_0 - y|}{|y_0|^2}
\end{align*}
Take $\delta = \min\left\{\epsilon, \frac{|y_0|}{2} \right\} > 0$. Then $0 <  \Vert(x, y) - (x_0, y_0) \Vert < \delta$. 
\begin{align*}
    |x-x_0| &= \sqrt{(x-x_0)^2} \leq \sqrt{(x-x_0)^2 + (y-y_0)^2} \\
    |y-y_0| &\leq \delta
\end{align*}
So, 
\begin{align*}
    \left|f(x, y) - \frac{x_0}{y_0} \right| < \epsilon\left(\frac{2}{|y_0} + \frac{2}{|y_0|^2} \right)
\end{align*}
}
Say you can prove that for every $\epsilon > 0$, $\exists \delta > 0 $ such that \begin{align*}
    d(f(x), l ) < \epsilon |\log(\epsilon)| \text{ for all } x \in E \text{ such that } 0 < d(x, x_0) < \delta
\end{align*}
For every $\eta > 0$ (``my epsilon''), since $\lim_{\epsilon \to 0 ^+} \epsilon | \log(\epsilon)| = 0$, $\exists \delta_1 > 0 $ such that $\epsilon | \log(\epsilon) | < \eta $ for all $0 < \epsilon < \delta_1$.

So given $\eta > 0$, take $ 0 < \epsilon < \delta_1$. Find $\eta$ from $d(f(x), l) < \epsilon | \log(\epsilon) | < \eta$ for all $x \in E$ such that $0 < d(x, x_0) < \delta$. This means that
\begin{align*}
    d_Y(f(x), l) < \epsilon | \log(\epsilon) | < \eta
\end{align*}
for all $x \in E$, $0 < d(x, x_0) < \delta$. Thus, $\lim_{x \to x_0} f(x) = l$.
\newpage
\section{Limits Continued}
\dfn{Restriction}{Assume that $\lim_{x\to x_0}f(x) = l$ exists. Let $F \subseteq E$ such that $x_0 \in  \operatorname{acc}F$. The function $f: F \to Y$ is called the \emph{restriction} of $f$ to $F$. It is denoted as $f|_F$.}
\nt{If $\lim_{x\to x_0}f(x) = l$, then $\lim_{x\to x_0}f|_F(x) = l$.}
\noindent So to prove that the limit does not exist, you can conjure up two restrictions of the function and show that the limits are different.
\ex{Limits that don't exist}{
    Consider $\lim_{x\to 0} \sin\left(\frac{1}{x}\right)$. We can find two restrictions of the function and show that the limits are different.
    \begin{itemize}
        \item $\frac{1}{x} = 2\pi n + \frac{\pi}{2}$
        \item $x_n = \frac{1}{2\pi n + \frac{\pi}{2}}$
    \end{itemize}
    So we have:
    \begin{itemize}
        \item $\sin x_n = \sin\left(\frac{\pi}{2} + 2\pi n\right) = 1$
        \item $\sin x_n = \sin\left(\frac{1}{2\pi n}\right) =0$
    \end{itemize}
    Thus, the limit does not exist.
}
\exer{TODO in Recitation}{\begin{itemize}
    \item $\lim_{(x, y) \to (0, 0)} \frac{xy}{x^2 + y^2}$ (no)
    \item $\lim_{(x, y) \to (0, 0)} \frac{x^2y}{x^2 + y^2}$ (yes, 0)
    \item $\lim_{(x, y) \to (0, 0)} \frac{x^{10000000000}y}{y - \sin(x)}$ (no)
\end{itemize}}
\noindent Now we talk about the composition of limits. 
\ex{}{Consider \[g(y) = \begin{cases} 1 & y \neq 0 \\ 2 & y = 0 \end{cases}.\] The limit of $g(y)$ as $y \to 0$ is $1$. Now consider $f(x) = 0$. The limit of $f(x)$ as $x \to x_0$ is $0$. Now consider $g(f(x))$. The limit of $g(f(x))$ as $x \to x_0$ is $2$.}
\thm{Composition of Limits}{Let $(X, d_X)$, $(Y, d_Y)$, and $(Z, d_Z)$ be metric spaces, $E \subseteq X$, $F \subseteq Y$, $f:E \to F$, $g:F \to Z$, and $x_0 \in \operatorname{acc}E$. Assume there exists $\lim_{x \to x_0} f(x) = l \in Y$. Assume $l \in \operatorname{acc}F$ and that there is $\lim_{y \to l} g(y) = L \in Z$. Assume that either $f(x) \neq l$ for all $x \in E$ or $ l\in F$ and $g(l) = L$. Then there is $\lim_{x \to x_0}g(f(x)) = L$.}
\begin{proof}
    Since $\lim_{y \to l} g(y) = L$, there for every $\epsilon >0$, there exists $\delta > 0$ such that $d_Z(g(y), L) < \epsilon$ for all $y \in F$ with $0 < d_Y(y, l) < \delta$. We would like to take $y = f(x)$. Use $\delta$ as ``my epsilon'' for the definition of the limit of $f(x)$. Then to find $\eta > 0$ such that $d_X(f(x), l) < \delta$ for all $x \in E$ with $0 < d_X(x, x_0) < \eta$. Now we split into cases:
    \begin{itemize}
        \item Assume $f(x) \neq l$ for all $x \in E$. Then $0 < d_Y(f(x), l)$ so we can take $y = f(x)$ to get $d_z(g(f(x)), L) < \epsilon$  for all $x \in E$ with $0 < d_X(x, x_0) < \eta$. This means that there exists $\lim_{x \to x_0} g(f(x)) = L$.
        \item Assume $l \in F$ and $g(l) = L$. If $f(x) = l$, then $d_Z(g(f(x)), L) = d_Z(g(l), L) = 0$ for all $x \in E$ with $0 < d_X(x, x_0) < \eta$. If $f(x) \neq l$, then take $y = f(x)$ to get $0 < d_Y(f(x), l)$ so we can take $y = f(x)$ to get $d_z(g(f(x)), L) < \epsilon$  for all $x \in E$ with $0 < d_X(x, x_0) < \eta$. This means that there exists $\lim_{x \to x_0} g(f(x)) = L$.
        This means that there exists $\lim_{x \to x_0} g(f(x)) = L$.
    \end{itemize}
\end{proof}
\cor{Limits of the Sum/Products/Quotients}{Let $(X, d)$ be a metric space and $E \in X$. Then take $f: E \to \RR$ and $g: E \to \RR$. Let $x_0 \in X$ and $x_0 \in \operatorname{acc}E$. Assume $\lim_{x \to x_0} f(x) = l$ and $\lim_{x \to x_0} g(x) = m$. Then we have the following results:
\begin{itemize}
    \item $\lim_{x \to x_0} (f+g)(x) = l + m$.
    \item $\lim_{x \to x_0} (f\cdot g)(x) = l \cdot m$.
    \item $\lim_{x \to x_0} \frac{f}{g}(x) = \frac{l}{m}$.
\end{itemize}}
\begin{proof}
    We can use the composition of limits to prove this. We'll just proceed with the quotient case. Consider $x \to (f(x), g(x))$. Then consider the function that takes $(s, t) \to \frac{s}{t}$ and call it $h$. THen we have $\frac{f(x)}{g(x)} = h(f(x), g(x))$. We then have $\lim_{x \to x_0} (f(x), g(x)) = (l, m)$ and $\lim_{(s, t) \to (l, m)} h(s, t) = \frac{l}{m} = h(l, m)$. So now we can use the composition of limits to get $\lim_{x \to x_0} \frac{f}{g}(x) = \frac{l}{m}$.

    The other two cases are similar. For products, you need to show that the limit as $(x,y) \to (x_0, y_0)$ of $xy$ is $x_0y_0$ and similarly for sum.
\end{proof}

\section{Squeeze Theorem}
\thm{Squeeze Theorem}{Let $(X, d_X)$ be a metric space, $E \subseteq X$, $f:E \to \RR$, $g:E \to \RR$, and $h:E \to \RR$. Let $x_0 \in \operatorname{acc}E$ and have $f \leq g \leq h$. Assume that $\lim_{x \to x_0}f(x) = l = \lim_{x \to x_0} h(x)$. Then $\lim_{x \to x_0} g(x) = l$.}
\begin{proof}
    Assume $\lim_{x \to x_0}f(x) = l = \lim_{x \to x_0} h(x)$. Then for every $\epsilon > 0$, there exists $\delta_1 > 0$ such that $0 < d_X(x, x_0) < \delta_1 \Rightarrow |f(x) - l| < \epsilon$ and $0 < d_X(x, x_0) < \delta_2 \Rightarrow |h(x) - l| < \epsilon$. Take $\delta = \min (\delta_1, \delta_2)$ and $x \in E$ with $ 0 < d(x, x_0) < \delta$. Then $l - \epsilon < f(x) < g(x) < h(x) < l + \epsilon$. Then $|g(x) - l| < \epsilon$ for all $x \in E$ with $0 < d_X(x, x_0) < \delta$. Thus, $\lim_{x \to x_0} g(x) = l$.
\end{proof}
\ex{}{\[\lim_{x \to 0} |x|^a \sin \frac{1}{x} = 0\] for all $a > 0$.

Let $Q> 0$. Then $O \leq | |x|^Q \sin\frac{1}{x} | \leq |x|^Q$. Since both sides tend to $0$ as $x \to 0$, then the middle does as well.}

\dfn{Increasing}{$f: E \to \RR$ is \emph{increasing} if $f(x) \leq f(y)$ for all $x \leq y$. It is strictly increasing if $f(x) < f(y)$ for all $x < y$.}
\dfn{Decreasing}{$f: E \to \RR$ is \emph{decreasing} if $f(x) \geq f(y)$ for all $x \leq y$. It is strictly decreasing if $f(x) > f(y)$ for all $x < y$.}
\dfn{Divergent}{Let $(X, d_x)$ be a metric space with $E \subseteq X$, $x_0 \in \operatorname{acc}E$, and $f: E \to \RR$. We say that $f$ diverges to $+\infty$ as $x \to x_0$ if for every $M > 0\in \RR$, there exists $\delta > 0$ such that $f(x) > M$ for all $x \in E$ with $0 < d_X(x, x_0) < \delta$. We say that $f$ diverges to $-\infty$ as $x \to x_0$ if for every $M < 0 \in \RR$, there exists $\delta > 0$ such that $f(x) < M$ for all $x \in E$ with $0 < d_X(x, x_0) < \delta$.}
\thm{}{Let $E \subseteq \RR$ and $f : E \to \RR$ be increasing. Let $x_0 \in \RR$. Assume $x_0$ is an accumulation point of $E \cap (-\infty, x_0)$. Then there is \[\lim_{{x\to x_0}^-} f(x) = \sup_{E \cap (-\infty, x_0)} f(x) \]
Now if $x_0$ is an accumulation point of $E \cap (x_0, \infty)$, then there is \[\lim_{{x\to x_0}^+} f(x) = \inf_{E \cap (x_0, \infty)} f(x) \]}
\begin{proof} $\\$
    \begin{itemize}
        \item Case 1: Assume $f$ is bounded form above on $E \cap (-\infty, x_0)$. Let $l = \sup_{E \cap (-\infty, x_0)} f(x)$. Then for every $\epsilon > 0$, there exists $x_1 \in E \cap (-\infty, x_0)$ such that $l - \epsilon < f(x_1) \leq l$. Then for every $\epsilon > 0$, there exists $\delta > 0$ such that $l - \epsilon < f(x_1) \leq l$. Take $\delta = x_0 - x_1 > 0$. Let $x \in E$ with $x_0 - \delta < x < x_0$. Since $f$ is increasing, we have $l - \epsilon < f(x_1) \leq f(x) \leq l < l + \epsilon$. Thus, $\lim_{{x\to x_0}^-} f(x) = l$.
        \item Case 2: If $f$ is not bounded from above, then for every $M > 0$, there exists $x_1 \in E \cap (-\infty, x_0)$ such that $f(x_1) > M$. Let $\delta = x_0 - x_1 > 0$. Then for every $x \in E$ with $x_1 = x_0 - \delta < x < x_0$, we have $f(x) \geq f(x_1) > M$. Thus, $\lim_{{x\to x_0}^-} f(x) = +\infty$.
    \end{itemize}
    The other case is similar.
\end{proof}

\dfn{Infinite Sum}{Let $X$ be a set and take $f: X \to [0, \infty]$. The \emph{infinite sum} is defined as: \[ \sum_{x \in X} f(x) = \sup \left\{ \sum_{x \in F} f(x) : F \subseteq X \text{ finite} \right\}.\]}
\mlenma{}{Let $X$ be nonempty with $f : X \to [0, \infty]$. Assume that $\sum_{x \in X} f(x) < \infty$. Then $\{x \in X : f(x) > 0\}$ is countable.}
\begin{proof}
    Take $ n\in \NN$ and define $X_n = \{x \in X : f(x) \geq \frac{1}{n}\}$. Let $E \subseteq X_n$ be finite. Then $\frac{1}{n} |E| < \sum_{x \in E} f(x) \leq M$. Then $|E| < nM$. Thus, $X_n$ is countable. Then $\bigcup_{n \in \NN} X_n  = \{x \in X : f(x) > 0\}$ is countable.
\end{proof}
\section{2/15 -  LIMIT  !!!!!!!!!}

\ex{}{\begin{itemize}
    \item Let $f(x) = \frac{xy}{x^2y^2}$. Find $\lim_{(x, y) \to (0, 0)} f(x, y)$. If we take the restriction $y = mx$, we see that the limit depends on $m$ which is a contradiction.
    \item Let $f(x, y) = \frac{x^2y}{x^2 + y^2}$. Find $\lim_{(x, y) \to (0, 0)} f(x, y)$. We can use polar coordinates to show that this is $0$. 
\end{itemize}}

\section{This Theorem}
\thm{}{Take $I \subseteq \RR$ to be an interval with $f: I \to \RR$ increasing. Then for all but countably many $x_0 \in I$, there is $\lim_{{x\to x_0}^-} f(x) =\lim_{{x\to x_0}^+} f(x) = f(x_0)$. }
\begin{proof}
    Let $I = [a,b]$. For every $x \in (a, b)$, there exists \[\lim_{y \to x^+} =: f_+(x), \quad \lim_{y \to x^-} =: f_-(x).\]
    Let $S(x) = f_+(x) - f_-(x) \geq 0$, which is the jump of $f$ at $x$. Then we have that $\lim_{y \to x} f(y) = f(x) \iff S(x) = 0$. Let $J \in [a, b]$ be any finite subset, and write \[J = \{x_1, \ldots, x_k\}, \quad \text{where} x_1 < 
    \cdots < x_k.\]
    Since $f$ is increasing, we have that 
    \begin{align*}
        f(a) \leq f_-(x_1) \leq f_+(x_1) \leq f_-(x_2) \leq f_+(x_2) \leq \cdots \leq f_-(x_k) \leq f_+(x_k) \leq f(b).
    \end{align*}
    So, 
    \begin{align*}
        \sum_{x \in J} S(x) = \sum_{x \in J} f_+(x) - f_-(x) \leq f(b) - f(a),
    \end{align*}
    which implies that $\sum_{x \in (a, b)} S(x) \leq f(b) - f(a)$. It follows that the amount of discontinuities is countable.
\end{proof}
\chapter{}
\dfn{Series}{Given a normed space $X$ and a sequence $\{x_n\}_n$, of vectors in $X$, we call the $n$th-partial sum the vector $s_n = \sum_{k=1}^n x_k$. The sequence $\{s_n\}_n$ of partial sums is called infinite series or \emph{series} and is denoted $\sum_{n=1}^\infty x_n$.

If there exists $\lim_{n \to \infty} s_n = s \in X$, then we say that the series $\sum_{n=1}^\infty x_n$ \emph{converges} to $s$ and $s$ is called the \emph{sum} of the series. If the limit does not converge, we say the series \emph{oscillates}.}

\section{More Series}
\thm{}{Let $X$ be a normed space. Consdier the series $\sum_{n=1}^\infty x_n$. If the series converges, then $\lim_{n \to \infty} x_n = 0$.}
\begin{proof}
    We know by the hypothesis that $s_n = x_1 + \cdots  + x_n$. Also $s_n \to s$ as $n \to \infty$. As such, we can write $x_n = s_n - s_{n-1}$ where both values on the RHS tend to $s$, meaning that $x_n$ tends to 0 as $n\to \infty$. 
\end{proof}
\nt{The above theorem is often useful to negate. In the exercises, we can also use teh fact that i $\lim_{n \to \infty}$ does not exists or does not equal 0, then $\sum_{n=1}^\infty x_n$ cannot converge. 

Also important to note that this series is very much one directional. For example, consider the following sums:
\begin{align*}
    \sum_{n=1}^\infty \frac{1}{n} &\text{ diverges} \\
    \sum_{n=1}^\infty \frac{1}{n^2} &\text{ converges}
\end{align*}
However, both values here tend to $0$ as $n \to \infty$.}

\ex{Geometric Series}{Consider $\sum_{n=1}^\infty x^n$. We know that $\lim_{n\to \infty} x^n = 0$ iff $|x| < 1$. So if $|x| \geq 1$, then the series does not converge. The theorem above does not help us for the $|x| < 1$ case. So let's compute the partial sum:
\begin{align*}
    s_n &= \sum_{k=1}^n x^k \\
    &= \frac{x^{n+1} - x}{x-1}
\end{align*}
So we have that $\lim_{n \to \infty} s_n = \frac{x}{1-x}$ for $|x| < 1$.}
\ex{}{Consider $X = \ell^\infty(E) = \{f : E \to \RR \text{ bounded}\}$ for $E \subseteq \RR$. The norm here is the supremum norm. Consider the series $\sum_{n=1}^\infty f_n(x)$ of random functions in $X$. We need to check that \[ \sup_{x \in E} |f_n(x)| \to 0 \text{ as } n \to \infty\] for the series to converge. If the limit $\neq 0$, then the series does not converge.}
\ex{Combining the above}{Let our space be $\ell^\infty((-1, 1))$ and consider the series $\sum_{n=1}^\infty f_n(x)$ where $f_n(x) = x^n$. We know that $\sup_{x \in (-1, 1)} |x^n| = 1$ for all $n$. Thus, the series does not converge.}

\thm{}{Consider a series of nonnegative terms $\sum_{n=1}^\infty x_n$ in $\RR$. Either the series converges or diverges to $+\infty$. }
\begin{proof}
    We know that $s_{m+1} \geq s_m$ for all $m$ and that these values are increasing, so $\lim_{m \to \infty} s_n = \sup_{n}s_n \in [0, \infty]$. 
\end{proof}
\thm{Comparison Test}{Let $\sum_{n=1}^\infty x_n$ and $\sum_{n=1}^\infty y_n$ be series of nonnegative terms in $\RR$. Assume that $0 \leq x_n \leq y_n$ for all $n \geq N$ for some $N$. If $\sum_{n=1}^\infty y_n$ converges, then $\sum_{n=1}^\infty x_n$ converges. If $\sum_{n=1}^\infty x_n$ diverges, then $\sum_{n=1}^\infty y_n$ diverges.}
\begin{proof}
    Consider the first case. So let $s_n = \sum x_n$ and $t_n = \sum y_n$. Since $y_n$ converges, $\lim t_n = T$ exists, so $t_n$ is bounded by $T$. Then for all $n \geq N$:
    \begin{align*}
        s_n &:= x_1 + \cdots + x_{N-1} + x_N + \cdots + x_n \\
        &\leq x_1 + \cdots x_{N-1} + y_N + \cdots + y_n \\
        &\leq x_1 + \cdots + x_{N-1} + T
    \end{align*}
    Hence, $\{s_n\}$ is bounded and increasing, so it converges.

    For the second case, we have that $s_n \to \infty$. So since \begin{align*}
        s_n \leq (x_1  +\cdots + x_{N-1}) + t_n
    \end{align*}
    we have that $t_n \to \infty$ as $n \to \infty$. 
\end{proof}

\ex{Examples}{
    \begin{enumerate}
        \item $\sum_{n=1}^\infty \left(\frac{1 + \cos n}{3}\right)^n$. We know that $\lim_{n \to \infty} \left(\frac{1 + \cos n}{3}\right)^n = 0$ so the series isn't divergent.
        
        We will compare it to $0 \leq \left(\frac{1 + \cos n}{3}\right)^n \leq \left(\frac{2}{3}\right)^n$. We know that $\sum_{n=1}^\infty \left(\frac{2}{3}\right)^n$ converges, so the series $\sum_{n=1}^\infty \left(\frac{1 + \cos n}{3}\right)^n$ converges by the comparison test.
        \item $\sum_{n=1}^\infty 1 - \cos \frac{1}{3^n}$. We know that $\lim_{n \to \infty} 1 - \cos \frac{1}{3^n} = 0$ so the series isn't divergent.
        
        We have that $\lim_{t \to 0} \frac{1-\cos t}{t} = 0$. Take $\epsilon = 1$ and find $\delta > 0$ such that $\left| \frac{1 - \cos t}{t} - 0 \right| < 1$ for all $0 <  |t| < \delta$. We know that $-1 < \frac{1 - \cos t}{t} < 1$. Now take $1 - \cos \frac{1}{3^n}  < \frac{1}{3^n}$ for all $n$ such that $ \frac{1}{3^n} < \delta $. 

        So, $1 - \cos \frac{1}{3^n} < \frac{1}{3^n}$ for all $ n > N$. THe RHS converges so by comparison test, the LHS converges.
        \item $\sum_{n=1}^\infty \frac{\sin \frac{1}{n^3}}{\log \left( 1 + \frac{1}{n} \right)} \left( e^{1/m} - 1 \right)$. We know that $\sin \frac{1}{n^3} \sim \frac{1}{n^3}$, $\log\left( 1 + \frac{1}{n} \right) \sim \frac{1}{n}$ and $e^{1/n} - 1 \sim \frac{1}{n}$. So we have that $\frac{\sin \frac{1}{n^3}}{\log \left( 1 + \frac{1}{n} \right)} \left( e^{1/m} - 1 \right) \sim \frac{1}{n^3} \cdot \frac{1}{n} \cdot \frac{1}{n} = \frac{1}{n^3}$. 
        \item Prove by induction that $n! > 2^n$ when $n \geq 4$. This implies that $\frac{1}{n!} \leq \frac{1}{2^n}$ for $n \geq 4$.  Since $\sum_{n=1}^\infty \frac{1}{2^n} < \infty$, comparison test tells us that $\sum_{n=0}^\infty \frac{1}{n!} < \infty$. The sum of the series is called \[e := \sum_{n=0}^\infty \frac{1}{n!}.\]
    \end{enumerate}
}

\thm{Root Test}{Let $x_n \geq 0$. 
\begin{enumerate}
    \item If $\limsup_{n \to \infty} \sqrt[n]{x_n} < 1$, then $\sum_{n=1}^\infty x_n < \infty$. 
    \item If $\limsup_{n\to \infty } \sqrt[n]{x_n} > 1$, then $\sum_{n=1}^\infty x_n = \infty$.
    \item If $\limsup_{n\to \infty } \sqrt[n]{x_n} = 1$, then the test is inconclusive.
\end{enumerate}}
\begin{proof}
    \begin{enumerate}
        \item Let $\ell = \limsup_{n\to \infty} \sqrt[n]{x^n}$. Assume $\ell < 1$. Find $\epsilon > 0$ such that $\ell + \epsilon < 1$. Then there exists $N$ such that $\sqrt[n]{x_n} < \ell + \epsilon$ for all $n \geq N$. Then $\sqrt[n]{x_n} > \ell - \epsilon$ for infinitely many $n$. Taking the first inequality, we have that $x_n < (\ell + \epsilon)^n$ for all $n \geq N$. By the comparison test, we have that $\sum_{n=1}^\infty (\ell + \epsilon)^n$ converges, so $\sum_{n=1}^\infty x_n$ converges.
        \item Assume $\ell > 1$. For $\epsilon > 0$ small, $(l - \epsilon) > 1$. So $x_n \geq (l - \epsilon)^n$ for infinitely many $n$. Since the RHS goes to infinity, a subsequence also goes to $\infty$, so $\lim_{n\to \infty} x_n \neq 0$ so the series cannot converge.
    \end{enumerate}
\end{proof}
\ex{Inconclusive Root Test}{
    Consider the series \[\sum_{n=1}^\infty \frac{1}{n} .\] Then, we have:
    \begin{align*}
        \sqrt[n]{\frac{1}{n}} = \left( \frac{1}{n}\right)^\frac{1}{n} = e^{\log\left(\frac{1}{n}\right)^\frac{1}{n}} = e^{\frac{\log\left(\frac{1}{n}\right)}{n}}
    \end{align*}
    The exponent goes to $0$, so the limit is 1.

    Now consider the series \[\sum_{n=1}^\infty \frac{1}{n^2} .\] Then, we have:
    \begin{align*}
        \sqrt[n]{\frac{1}{n^2}} = \left( \frac{1}{n^2}\right)^\frac{1}{n} = e^{\log\left(\frac{1}{n^2}\right)^\frac{1}{n}} = e^{\frac{\log\left(\frac{1}{n^2}\right)}{n}}
    \end{align*}
    The exponent goes to $0$, so the limit is 1.

    We see that the first series diverges and the second series converges, so the root test is inconclusive when the $\limsup$ is 1. 
}
\ex{More Root Test}{Consider the series \[\sum_{n=1}^\infty \frac{n^2+1}{2^n}.\] We have that $\lim_{n \to \infty} \frac{n^2+ 1}{2^n} = 0$, so the series isn't divergent. Consider the root test now. We have that \[\sqrt[n]{\frac{n^2+1}{2^n}} =  \frac{\sqrt[n]{n^2+1}}{2} =\frac{1}{2}e^{\frac{1}{n}\log(n^2+1)} \to \frac{1}{2}e^0 = \frac{1}{2} < 1.\] So the series converges.}

\exer{}{Let $x_n \geq 0$. Prove that \[\liminf_{n\to \infty} \frac{x_{n+1}}{x_n} \leq \liminf_{n\to \infty} \sqrt[n]{x_n} \leq \limsup_{n\to \infty} \sqrt[n]{x_n} \leq \limsup_{n\to \infty } \frac{x_{n+1}}{x_n}.\]
Find an example where the last inequality is strict:
\[\limsup_{n\to \infty} \sqrt[n]{x_n} < \limsup_{n\to \infty } \frac{x_{n+1}}{x_n}.\]
An example is \[x_n = \begin{cases} 1 & n \text{ odd} \\ 2 & n \text{ even} \end{cases}.\]}
\dfn{Ratio Test}{Let $x_n > 0$. 
\begin{enumerate}
    \item If $\limsup_{n\to \infty} \frac{x_{n+1}}{x_n} < 1$, then the series converges.
    \item If $\liminf_{n \to \infty} \frac{x_{n+1}}{x_n} > 1$, then the series diverges.
\end{enumerate}}
\begin{proof}
\begin{enumerate}
    \item This is a one-line proof. If the limit is less than 1, then the series converges by the root test by the exercise above. That is $, \limsup_{n\to \infty} \frac{x_{n+1}}{x_n} < 1 \implies \limsup_{n\to \infty} \sqrt[n]{x_n} < 1$.
    \item This is also a one-line proof. If the limit is greater than 1, then the series diverges by the root test by the exercise above. That is, $\liminf_{n \to \infty} \frac{x_{n+1}}{x_n} > 1 \implies \limsup_{n\to \infty} \sqrt[n]{x_n} > 1$.
\end{enumerate}    
\end{proof}
\ex{}{Consider the sequence:
\[x_n = \begin{cases} \frac{1}{2^n} & n \text{ odd} \\ \frac{1}{3^n} & n \text{ even} \end{cases}.\]
We have that $\limsup_{n\to \infty} \frac{x_{n+1}}{x_n} = \infty$ and that $\liminf_{n \to \infty} \frac{x_{n+1}}{x_n} = 0$. As such, we cannot apply the ratio test in this case. We try the root test instead.

We have that $\limsup_{n\to \infty} \sqrt[n]{x_n} = \frac{1}{2}$ and that $\liminf_{n\to \infty} \sqrt[n]{x_n} = \frac{1}{3}$. Because of the first one, we know that the series converges.}

\dfn{Integral Test}{Let $f: [1, \infty) \to [0, \infty)$ be a decreasing function in $[N, \infty)$. Then the series $\sum_{n=1}^\infty f(n)$ converges if and only if $\lim_{L \to \infty}\int_1^L f(x) \, dx$ converges.}
\begin{proof}
    We start with the forward direction. Consider $\int_{N}^\ell f(x)\, dx$ for integer $\ell$. We have:
    \begin{align*}
        \int_{N}^\ell f(x)\, dx &= \sum_{n=N}^{\ell-1} \int_n^{n+1} f(x)\, dx \\
    \end{align*}
    If $n \leq x \leq n+1$ and $f$ decreasing, we have that $f(n+1) \leq f(x) \leq f(n)$. For each $n$, we have:
    \begin{align*}
        \sum_{n=N}^{\ell-1} \int_n^{n+1} f(x)\, dx \leq \sum_{n=N}^{\ell-1}f(n)
    \end{align*}
    If $\lim_{\ell \to \infty} \int_{N}^\ell f(x)\, dx$ diverges, then $\sum_{n=N}^\infty f(n)$ diverges. So, 
    \begin{align*}
        \int_N^\ell f(x)\, dx &= \sum_{n=N}^{\ell-1} \int_n^{n+1} f(x)\, dx \\
        &\geq \sum_{n=N}^{\ell-1} f(n+1)
    \end{align*}
    So if $\lim_{\ell \to \infty} \int_{N}^\ell f(x)\, dx$ converges, then $\sum_{n=N}^\infty f(n)$ converges since it is less than or equal to.
\end{proof}

% \section{2/22 - Uniform Convergence}
% \dfn{Uniform Convergence}{Let $E$ be a set and take $\ell^\infty(E) = \{f : E \to \RR \text{ bounded}\}$. $\Vert f \Vert_\infty = \sup_{x \in E} |f(x)| < \infty$. Let $\{f_n\}_n \subset \ell^\infty(E)$, $f \in \ell^\infty(E)$. We say that $f_n \to f$ \emph{uniformly} when $\lim_{n \to \infty} \Vert f_n - f \Vert_\infty  = \lim_{n \to \infty} d(f_n, f)= 0$.}
% \nt{If $x \in E$, then $|f_n(x) - f(x)| \leq \Vert f_n - f \Vert_\infty$. This means that uniform convergence implies pointwise convergence. However, it is important to note that the converse is not true. That is, pointwise convergence does not imply uniform convergence. Here is an example. Let:
% \[f_n(x) = x^n \quad f(x) = \begin{cases} 0 & x \in [0, 1) \\ 1 & x = 1 \end{cases}.\]
% }
% \ex{}{Consider the following sequences:
% \begin{itemize}
%     \item $f_n(x) = n^2(x-1)e^{-n(x-1)}$. Find the largest set $E_\circ \subseteq \RR$ where $f_n$ converges pointwise.
%     \begin{itemize}
%         \item For $x=1$, $f_n(1) = 0\, \forall \, n \in \NN$, so $\lim_{n\to \infty} f_n(1) = 0$.
%         \item For $x > 1$, $f_n(x) = \frac{n^2(x-1)}{e^{n(x-1)}} \to 0$ as $n \to \infty$.
%         \item For $x < 1$, $f_n(x) = \frac{-n^2(x-1)}{e^{n(x-1)}} \to -\infty$ as $n \to \infty$.
%     \end{itemize}
%     So, $E_\circ = [1, \infty)$.
%     \item Sketch the graph of $f_n - f$ in $E_\circ$.
% \end{itemize}}

\section{More Series}
\ex{Integral Test}{Consider:
    \begin{itemize}
        \item $\sum_{n=1}^\infty \frac{1}{n^a}$ for $a > 0$ First we check that $\lim_{n \to infty} \frac{1}{n^a} = 0$. This is indeed true. Now we define $f(x) = \frac{1}{x^a}$ for $x > 0$. This function is decreasing, so we can use the integral test. We have:
        \begin{align*}
            \int_1^\infty \frac{1}{x^a}\, dx &= \lim_{L \to \infty} \int_1^L \frac{1}{x^a}\, dx \\
            &= \lim_{L \to \infty} \left[ \frac{x^{1-a}}{1-a} \right]_1^L \\
            &= \lim_{L \to \infty} \left[ \frac{L^{1-a}}{1-a} - \frac{1}{1-a} \right] \\
            &= \frac{-1}{a-1} \quad \text{if } a > 1
            &= \infty \quad \text{if } a < 1
        \end{align*}
        If $a = 1$, then we have:
        \begin{align*}
            \int_1^\infty \frac{1}{x}\, dx &= \lim_{L \to \infty} \int_1^L \frac{1}{x}\, dx \\
            &= \lim_{L \to \infty} \left[ \log(x) \right]_1^L \\
            &= \lim_{L \to \infty} \left[ \log(L) - \log(1) \right] \\
            &= \infty
        \end{align*}
        So, the series converges if $a > 1$ and diverges if $a \leq 1$.
        \item $\sum_{n=2}^\infty \frac{1}{n^a \log n}$. We have that $\lim_{n \to \infty} \frac{1}{n^a \log n} = 0$. We define $f(x) = \frac{1}{x^a \log x}$ for $x > 2$. This function is decreasing, so we can use the integral test. We have:
        \begin{align*}
            \int_2^\infty \frac{1}{x^a \log x}\, dx &= \lim_{L \to \infty} \int_2^L \frac{1}{x^a \log x}\, dx \\
            &= \lim_{L \to \infty} \left[ \frac{\log(\log(x))}{1-a} \right]_2^L \\
            &= \lim_{L \to \infty} \left[ \frac{\log(\log(L))}{1-a} - \frac{\log(\log(2))}{1-a} \right] \\
            &= \infty \quad \text{if } a > 1
        \end{align*}
    \end{itemize}
}
\dfn{Alternating Series}{Let $\{a_n\}_n$ be a sequence of positive numbers. The series $\sum_{n=1}^\infty (-1)^{n+1}a_n$ is called \emph{Alternating}}
\thm{Leibniz Test}{Consider $\sum_{n=1}^\infty (-1)^n a_n$ where $a_n \geq 0$.
    If $\{a_n\}_n$ is decreasing and $\lim_{n \to \infty} a_n = 0$, then the series converges and $|S - s_n| \leq a_{n+1}$ for all $n$.}
\begin{proof}
    Write
    \begin{align*}
        s_{2n + 1} &= -a_1 + (a_2 - a_3) + (a_4 - a_5) + \cdots + (a_{2n} - a_{2n + 1}) \\
        &= -(a_1 - a_2) - (a_3 - a_4) - \cdots - (a_{2n-1} - a_{2n}) - a_{2n+1} 
    \end{align*}
    Since $a_n$ is decreasing, we have that $a_i - a_{i-1} \geq 0$. And from the first equality, we get that $s_{2n+1} \leq s_{2n+3}$, meaning that $s_{2n + 1}$ is an increasing sequence. But from the second equality, we get that $s_{2n+1} \leq -a_{2n+1} \leq 0$. So, there exists:
    \begin{align*}
        \lim_{n \to \infty} s_{2n+1} = \sup_n s_{2n+1} = S \in (-\infty, 0]
    \end{align*}
    Since $s_{2n+1} = s_{2n} + a_{2n+1}$ and $\lim_{n\to\infty} a_n = 0$, we have that $\lim_{n\to\infty} s_{2n} = S$. So, the series converges.

    Moreover, we have that: 
    \begin{align*}
        s_{2n} = -(a_1 - a_2) - (a_3 - a_4) - \cdots - (a_{2n-1} - a_{2n}),
        \end{align*}
        which implies that $s_{2n} \geq s_{2n+2}$, meaning that $s_{2n}$ is a decreasing sequence. So $\inf_n s_{2n} = S \in (-\infty, 0]$. Therefore, $s_{2n+1} \leq S \leq s_{2n}$. It follows that
        \begin{align*}
            |S - s_{2n}| &= s_{2n} - S \leq s_{2n} - s_{2n+1} = a_{2n+1} \\
            |S - s_{2n+1}| &= s_{2n+1} - S \leq s_{2n+2} - s_{2n+1} = a_{2n+1}
        \end{align*}
        as desired.
\end{proof}
\cor{}{Also if an alternating series converges, then the remainder $R_n = |S - S_n|$ satisfies $0 \leq R_n \leq a_{2n+1}$.}
\begin{proof}
    We have that $S_{2n + 1} \leq S$ and that $S_{2n}$ is decreasing. So $S = \inf_{n \in \NN} S_{2n}$, so $S \leq S_{2n}$. This yields $|S - S_{2n} | = S_{2n} - S \leq S_{2n} - S_{2n + 1} = a_{2n + 1}$. For the other case, we have $|S - S_{2n+1}| + S - S_{2n+1} \leq S_{2n+2} - S_{2n + 1} \leq a_{2n+2}$.
\end{proof}
\ex{}{Consider the sequence \[ \sum_{n=1}^\infty (-1)^n \frac{n \log n}{1 + n^2}. \] We consider $\lim_{n \to \infty} \frac{n \log n}{1 + n^2}$. This is similar to the limit of $\frac{\log n}{n}$, which diverges. So by comparison test, our limit diverges. So we have:
\begin{align*}
    f(x) &= \frac{x \log x}{1 + x^2} \\
    f'(x) &= \frac{\log x + 1}{x^2 + 1} - \frac{2x^2 \log x}{(x^2 + 1)^2}
\end{align*}
We can somehow show that $f'(x) < 0$ for all $x \geq N$ for some $N$}

\newpage 
\thm{}{Let $E, \ell^\infty(E) = \{f : E \to \RR \text{ bounded}\}$. Let $\{f_n\}_n \subset \ell^\infty(E)$ and $f \in \ell^\infty(E)$. 
\begin{enumerate}
    \item If $\sum_{n=1}^\infty \sup_{x \in E}|f_n(x)| < \infty$, then $\sum_{n=1}^\infty f_n(x)$ converges uniformly in $E$.
    \item If $\sum_{n=1}^\infty f_n(x)$ converges uniformly to $f$, then $\lim_{n \to \infty} \sup_{x \in E}|f_n(x)| = 0$. 
\end{enumerate}}
\ex{}{
    \begin{enumerate}
        \item Consider the series $\sum_{n=1}^\infty \frac{e^{nx}}{n}$, for $x \in \RR$.
        
        $\frac{e^{nx}}{n} > 0 \, \forall x \in \RR$.
        \begin{itemize}
            \item For $x = 0$, we have $\sum_{n=1}^\infty \frac{1}{n} = \infty$.
            \item For $x > 0$, we have $\lim_{n\to \infty} \frac{e^{nx}}{n} = \infty$.
            \item For $x  < 0$, $\left(\frac{e^{nx}}{n}^{1/n}\right) = \frac{e^x}{n^{1/n}} \to e^x$ as $ n \to infty$. 
        \end{itemize}
        This shows that there is pointwise convergence when $x  < 0$. So if we want to determine a subset were there is uniform convergence, then we have to consider only $x \in (-\infty, 0)$.

        So consider $E = (-\infty ,-\epsilon)$ for some $\epsilon > 0$. Consider the sequence of functions defined as \[f_n(x) = \frac{e^{nx}}{n}\] for $x \in E$. Then we have:
        \[f_n'(x) = e^{nx} > 0 \implies \sup_{x \in E}|f_n(x)| = \frac{e^{-n\epsilon}}{n}\]
        So, by our theorem, we have that $\sum_{n=1}^\infty \frac{e^{nx}}{n}$ converges uniformly in $E$.
        \item Consider $\sum_{n=1}^\infty \frac{x^{2n}}{\sqrt[3]{n}} \log\left( 1 + \frac{x^2}{\sqrt[3]{n}}\right)$, for $x \in \RR$. 
        
        \begin{itemize}
            \item for $x = 0$, it converges to 0.
            \item for $|x| > 1$, we have that \begin{align*}
                \lim_{n\to \infty} \frac{x^{2n}}{\sqrt[3]{n}} \log\left( 1 + \frac{x^2}{\sqrt[3]{n}}\right) = \lim_{n \to \infty} \frac{x^{2n+2}}{n^{2/3}} \frac{\log\left( 1 + \frac{x^2}{\sqrt[3]{n}}\right)}{x^2/\sqrt[3]{n}} = \infty
            \end{align*}
            \item for $|x| < 1$, $\lim_{n \to \infty} \frac{x^{2n}}{\sqrt[3]{n}} \log\left( 1 + \frac{x^2}{\sqrt[3]{n}}\right) = 0$
        \end{itemize}
        \item Consider $\sum_{n=1}^\infty \frac{x^n}{n}$ for $x \geq 0$.
        \begin{itemize}
            \item for $x = 0$, there is pointwise convergence .
            \item for $x \geq 1$, there is no pointwise convergence.
            \item For $x \in (0, 1)$, we have that $\lim_{n \to \infty} \frac{x^n}{n} = 0$.
        \end{itemize}
    \end{enumerate}
}
\newpage
\thm{}{Take some $x_n \in \RR$ and consider the series $\sum_{n=1}^\infty$. If $\sum_{n\to \infty} |x_n|$ converges, then $\sum_{n=1}^\infty x_n$ converges.}
\nt{The converse isn't true. Consider the alternating version of the harmonic series.}
\dfn{}{Let $t \in \RR$. We define:
\begin{align*}
    t^+ &= \max(t, 0) \\
    t^- &= \max(-t, 0)
\end{align*}
From these, we derive:
\begin{align*}
    |t| = t^+ + t^-  \quad \text{and} \quad t = t^+ - t^-
\end{align*}}
\begin{proof}
    We have $0 \leq x_n^+ \leq |x_n|$. By comparison test, we have that $\sum_{n=1}^\infty x_n^+$ converges. We also have $0 \leq x_n^- \leq |x_n|$. By comparison test, we have that $\sum_{n=1}^\infty x_n^-$ converges. Remember that by the limit of the sum,
    \begin{align*}
        \sum_{n=1}^\infty x_n^+ &= \lim_{\ell \to \infty}\sum_{n=1}^\ell x_n^+ \\
        \sum_{n=1}^\infty x_n^- &= \lim_{\ell \to \infty}\sum_{n=1}^\ell x_n^-
    \end{align*}
    So, we have that $$\sum_{n=1}^\infty x_n = 
    \lim_{\ell \to \infty}\sum_{n=1}^\ell x_n^+ -  x_n^- = \lim_{\ell \to \infty} \sum_{n=1}^\ell x_n.$$
    This implies that $x_n$ converges as desired.
\end{proof}
\thm{}{Let $E$ be a set and $\ell^\infty(E) = \{f : E \to \RR \text{ bounded}\}$. Let $\{f_n\}_n \subset \ell^\infty(E)$ and $f \in \ell^\infty(E)$. Then,
\begin{enumerate}
    \item If $\sum_{n = 1}^\infty \sup_{x \in E}|f_n(x)| < \infty$, then $\sum_{n=1}^\infty f_n(x)$ converges uniformly in $E$.
    \item If $\sum_{n=1}^\infty f_n(x)$ converges uniformly to $f$, then $\lim_{n \to \infty} \sup_{x \in E}|f_n(x)| = 0$.
\end{enumerate}}
\begin{proof}
    Let $a_n = \sup_{x \in E}|f_n(x)|$. We know that the sum of the $a_n$ converges in $\RR$. Fix an $x \in E$. We have that $0 \leq |f_n(x)| \leq a_n$. By the comparison test, we have that $\sum_{n=1}^\infty |f_n(x)|$ converges pointwise. So by the previous theorem, $\sum_{n=1}^\infty f_n(x)$ converges pointwise in $\RR$. This isn't good enough; we want uniform convergence. That is, we want:
    \begin{align*}
        \Vert f - \sum_{n = 1}^\infty f_n \Vert_\infty \to 0
    \end{align*}
    FINSIH THIS LATER
\end{proof}
\dfn{Continuity}{Let $(X, d_x)$ and $(Y, d_Y)$ be metric spaces. Let $E \subseteq X$ and $f: E \to Y$. Let $x_0 \in E$ and assume $x_0 \in \operatorname{acc}E$. We say that $f$ is continuous at $x_0$ if there is $\lim_{x \to x_0} f(x) = f(x_0)$. We say that $f$ is continuous on $E$ if $f$ is continuous at all $x_0 \in E$.

We denote $C(E)$ as the continuous functions on $E$.}
\ex{}{\begin{enumerate}
    \item Consider sequences. That is, $f \NN \to \RR$. This is continous because $\NN \cap \operatorname{acc}\NN = \emptyset$.
    \item If we have $f: [0, 1] \cup \{3\} \to \RR$, we only check continuity at $x_0 \in [0, 1]$. $f$ is continuous at 3.
    \item The sum, product, quotient (denominator nonzero), and composition of two continuous functions is continuous.
\end{enumerate}}
\exer{Continuity}{
    \begin{itemize}
        \item $x^n$ continuous
        \item $\sin(x)$ continuous
        \item $\cos(x)$ continuous
    \end{itemize}
}
\dfn{Relatively Open}{Let $(X, d_X)$ be a metric space and $E \subseteq X$. We say that $F \subseteq E$ is \emph{relatively open} in $E$ if $F = E \cap U$ with $U$ open.}
\thm{}{Let $(X, d_X)$ and $(Y, d_Y)$ be metric spaces. Let $E \subseteq X$ and $f: E \to Y$. Then $f$ is continuous on $E$ if and only if for all open sets $V \subseteq Y$, $f^{-1}(V)$ is relatively open in $E$.}

\ex{}{Let $F = \{(x, y) \in \RR^2 : x + \sin y > 4 \}$.Then $f(x,y) = x + \sin y$ is continuous because $F = f^{-1}((4, \infty))$.}

\begin{proof}
    We start with the forward direction. Assume that $f$ is continuous on $E$. Let $V \subseteq Y$ be open. Consider $f^{-1}(V)$. If $V \neq \emptyset$, let $x_0 \in f^{-1}(V)$. Then $f(x_0) \in V$. Find $B_Y(f(x_0), \epsilon) \subseteq V$. If $x_0 \in \operatorname{acc}E$, then we can find $\delta > 0$ such that if $x \in E$ and $d_X(x, x_0) < \delta$, then $d_Y(f(x), f(x_0)) < \epsilon$. So if $x \in B(x_0, \delta) \cap E$, then $f(x) \in B_Y(f(x_0), \epsilon) \subseteq V$. So $B(x_0, \delta) \cap E \subseteq f^{-1}(V)$.

    If $x_0$ isn't an accumulation point, then there exists $\delta > 0$ such that $B(x_0, \delta) \cap E = \{x_0\}$. So $f^{-1}(V) = \{x_0\}$. 
    
    So we just have $f^{-1}(V) = E \cap \bigcup_{x \in E} B(x, \delta_x)$. So $f^{-1}(V)$ is relatively open in $E$.

    For the backward direction, assume that $f^{-1}(V)$ is relatively open for all $V \subseteq Y$ that are open. We want to show that $f$ is continuous. Let $x_0 \in E \cap \operatorname{acc}E$. We want to show that the limit as $x \to x_0$ of $f(x)$ is $f(x_0)$. Take $V = B_Y(f(x_0), \epsilon)$. Since $f^{-1}(V)$ is relatively open, $f^{-1}(B(f(x_0), \epsilon)) = E \cap U$ for some open $U$. So $x_0 \in U$, so there exists $\delta > 0$ such that $B(x_0, \delta) \cap E \subseteq U$. So if $x \in B(x_0, \delta) \cap E$, then $f(x) \in B_Y(f(x_0), \epsilon)$. So $\lim_{x \to x_0} f(x) = f(x_0)$ because $d(f(x), f(x_0)) < \epsilon$.
\end{proof}
\newcommand{\acc}{\operatorname{acc}}

\thm{}{Let $(X, d_X)$ and $(Y, d_Y)$ be metric spaces. Take $ K \subseteq X$ that is sequentially compact and $f: K \to Y$ that is continuous. Then $f(K)$ is sequentially compact.}
\begin{proof}
    Let $y_n \in f(K)$. Then there exists $x_n \in K$ such that $f(x_n) = y_n$. Since $K$ is sequentially compact, there exists a subsequence $\{x_{n_k}\}_k$ that converges to $x \in K$. Since $f$ is continuous, we have that $f(x_{n_k}) \to f(x)$ if $x \in \operatorname{acc}E$. If $x \notin \acc E$, then it is a constant sequence and we are done. So $f(x) \in f(K)$. 
\end{proof}
\thm{Weierstrass Theorem}{Let $(X, d_X)$ be a metric space and $K \subseteq X$ that is sequentially compact. Let $f: K \to \RR$ be continuous. Then $f$ is bounded and attains its bounds.}
\begin{proof}
    We know that $f(K)$ closed and bounded by the previous theorem. As such, there exists $\sup f(K) = \ell \in \RR$. We want to show that this is the maximum. Consider $\ell - \frac{1}{n}$. This is not an upper bound for $f(K)$, so there exists $x_n \in K$ such that $f(x_n) > \ell - \frac{1}{n}$. Since $K$ is sequentially compact, there exists a subsequence $\{x_{n_k}\}_k$ that converges to $x \in K$. So, 
    \begin{align*}
        \ell - \frac{1}{n_k} < f(x_{n_k}) \leq \ell
    \end{align*}
    As $k \to \infty$, we have that $f(x_{n_k}) \to \ell$. So $f(x) = \ell$. So $f$ attains its maximum.
\end{proof}

\thm{Let $I \subseteq \RR$ be an interval and $f:I \to \RR$ continuous and assume there exist $x_1$ and $x_2$ such that $f(x_1) < 0 < f(x_2)$. Then there exists $x_0 \in I$ such that $f(x_0) = 0$.}

\begin{proof}
    Assume $x_1 < x_2$. So $
    lim_{x \to x_1} = f(x_1) < 0$. Let $\epsilon = -f(x_1)/2$ to find $\delta_1 > 0$ such that $f(x) < 0$ in $[x_1, x_1 = \delta_1]$. 

    We also have that $\lim_{x \to x_2} f(x) = f(x_2) > 0$. Let $\epsilon = f(x_2)/2 > 0$. So we can find $\delta_2 > 0$ such that $f(x) > 0$ in $[x_2 - \delta_2, x_2]$.

    Consider the set $E = \{ x \in [x_1, x_2] : f(x) < 0\}$, which is bounded above by $x_2 - \delta_2$. So $\sup E = \ell \in [x_1 + \delta_2, x_2 - \delta-2]$ exists.
    
    We claim that $f(\ell) = 0$. If $f(\ell) < 0$, then by dfn of continuity, $f(\ell) = \lim_{x \to \ell} f(x)$. Take $\epsilon = -\ell / 2$ and find $\delta_3 > 0$ such that $f(x) < 0$ in $[\ell - \delta_3, \ell + \delta_3]$. So $\ell + \delta_3 \in E$, which is a contradiction because $\ell$ is a maximum. If $f(\ell) > 0$, then take $\epsilon = \ell / 2$ and find $\delta_4 > 0$ such that $f(x) > 0$ in $[\ell - \delta_4, \ell + \delta_4]$. So $\ell - \delta_4 \in E$, which is a contradiction because it is then a better lower bound than $\ell$. So $f(\ell) = 0$ by trichotomy.
\end{proof}
\cor{}{A polynomial of odd degree has at least one zero.}
\begin{proof}
    Consider $p(x)$ that has odd degree. WLOG assume the first coefficient is positive. So we have:
    \begin{align*}
        \lim_{x\to \infty} p(x) = \infty \quad \lim_{x \to -\infty} p(x) = -\infty
    \end{align*}
    By the previous theorem, we have that there exists $x_0 \in \RR$ such that $p(x_0) = 0$.
\end{proof}

\cor{}{Consider the interval $I \subseteq \RR$ with $f: I \to \RR$ continuous. Then $f(I)$ is an interval with endpoints $\inf f(I)$ and $\sup f(I)$.}
\begin{proof}
    Let $y_1, y_2 \in f(I)$ with $y_1 < y_2$ and let $y_1 < t < y_2$. Then we wnant to show that $t \in f(I)$. Consider $g(x) = f(x) - t$. There exists $x_1$ and $x_2$ such that $f(x_1) = y_1$ and $f(x_2) = y_2$. So $g(x_1) < 0 < g(x_2)$. So by the previous theorem, there exists $x_0 \in I$ such that $g(x_0) = 0$. So $f(x_0) = t$.
\end{proof}

\cor{}{Let $I \subseteq \RR$ be an interval and $f : I \to \RR$ that is continuous and injective. Then $f^{-1} : f(I) \to \RR$ is continuous.}
\ex{bad}{Consider $E = [0, 1] \cup (2, 3]$. Then let:
\begin{align*}
    f(x) = \begin{cases} x & x \in [0, 1] \\ x - 1 & x \in (2, 3] \end{cases}
\end{align*}
This does not have a continuous inverse.}
\begin{proof}
    First we show that $f$ is monotone. Assume FSOC that there are $a<b$ such that $f(a) < f(b)$. Then $f$ is strictly increasing. 
\end{proof}
\thm{}{Let $(X, d_X)$ and $(Y, d_Y)$ be metric spaces. Let $K \subseteq X$ be sequentially compact. Let $f: K \to Y$ be continuous and injective. Then $f^{-1} : f(K) \to X$ is continuous.}
\begin{proof}
    Let $y_0 \in f(K)$ and let $y_0 \in \acc f(K)$. We claim that $\lim_{y \to y_0} f^{-1}(y) = f^{-1}(y_0)$. So BWOC, there exists $\epsilon > 0$ such that for every $\delta$, we can find $y \in B_Y(y_0, \delta)$ such that $d_X(f^{-1}(y), f^{-1}(y_0)) \geq \epsilon$. Let $\delta = 1/n$. Then we can find $y_n \in B_Y(y_0, 1/n)$ such that $d_X(f^{-1}(y_n), f^{-1}(y_0)) \geq \epsilon$. $y_n \in f(K)$, so there is $x_n \in K$ such that $f(x_n) = y_n$. Since $K$ is sequentially compact, there exists a subsequence $\{x_{n_k}\}_k$ that converges to $x_0 \in K$. So since $f$ is continuous, we have that $x_{n_k} \to x_0 \implies f(x_{n_k})= y_{n_k}\to f(x_0) = y_0$. $d_X(f^{-1}(y_{n_k}), f^{-1}(y_0)) \geq \epsilon > 0$. But we have that $d_X(x_{n_k}, x_0) \to 0$, contradiction. As such, we have that $\lim_{y \to y_0} f^{-1}(y) = f^{-1}(y_0)$ and so $f^{-1}$ is continuous.
\end{proof}

\dfn{Directional Derivatives}{Let $(X, \Vert \cdot \Vert_X)$ and $(Y, \Vert \cdot \Vert_Y)$ be normed spaces. Let $f: E \to Y$, $E \subseteq X$, $x_0 \in E$, $v \in X$, $\Vert v \Vert_X = 1$. $L = \{ x \in E : x = x_0 + tv \text{ for some } t \in \RR\}$. Assume $x_0 \in \acc L$. 

The directional derivative of $f$ at $x_0$ in the direction of $v$ is:
\[\lim_{t \to 0} \frac{f(x_0 + tv) - f(x_0)}{t},\] whenever the limit exists. It is denoted as $\frac{\partial f}{\partial v}(x_0)$. }

\nt{If $X = \RR^N$ and $\ell_i$ is the $i$th vector in the canonical basis, then $\frac{\partial f}{\partial \ell_i}(x_0)$ is the $i$th partial derivative of $f$ at $x_0$.

Also, if we have $f(x, y, z)$, then $\frac{\partial f}{\partial x}(x_0, y_0, z_0)$ is the same as $\lim_{t \to 0} \frac{f(x_0, y_0 + t, z_0) - f(x_0, y_0, z_0)}{t}$.}

\dfn{One-dimensional Derivative}{Let $X = \RR$ and $v = 1$. Then:
\begin{align*}
    \frac{\partial f}{\partial v}(x_0) = \lim_{t \to 0} \frac{f(x_0 + t) - f(x_0)}{t} = f'(x_0)
\end{align*}
This is the one-dimensional derivative of $f$ at $x_0$.}

If $f'(x_0)$ exists, then $f$ is continuous at $x_0$. This is not true for $N \geq 2$. 

\dfn{Differentiability}{Let $(X, \Vert \cdot \Vert_X)$ and $(Y, \Vert \cdot \Vert_Y)$ be normed spaces. Let $E \subseteq X$ and $f: E \to Y$. We say that $f$ is differentiable at $x_0 \in E \cap \acc E$ if there exists a linear function $L: X \to Y$ and continuous such that $\lim_{x \to x_0} \frac{f(x) - f(x_0) - L(x - x_0)}{\Vert x - x_0 \Vert_X} = 0$. We denote $L$ as $df(x_0)$. $L$ is called the differential of $f$ at $x_0$. }

\thm{(Useful to negate)}{Let $(X, \Vert \cdot \Vert_X)$ and $(Y, \Vert \cdot \Vert_Y)$ be normed spaces. Let $E \subseteq X$ and $f: E \to Y$. Let $x_0 \in E \cap \acc E$. If $f$ is differentiable at $x_0$, then $f$ is continuous at $x_0$.}
\begin{proof}
    Let $L = df(x_0)$. Then we have:
    \begin{align*}
        f(x) - f(x_0) &= f(x) - f(x_0) - L(x-x_0) + L(x-x_0) \\
        &= \frac{f(x) - f(x_0) - L(x-x_0)}{\Vert x - x_0 \Vert_X} \Vert x - x_0 \Vert_X + L(x-x_0) \\
        &= 0 \cdot 0 + L(0) = 0
    \end{align*}
    So $f$ is continuous at $x_0$.
\end{proof}
\thm{(Also useful to negate)}{Let $(X, \Vert \cdot \Vert_X)$ and $(Y, \Vert \cdot \Vert_Y)$ be normed spaces. Let $E \subseteq X$ and $f: E \to Y$. Let $x_0 \in E \cap \acc E$. Assume $f$ is differentiable at $x_0$. Let $v \in X$ with $\Vert v \Vert_X = 1$. Assume $x_0 \in \acc L$, $L = \{ x \in E : x = x_0 + tv \text{ for some } t \in \RR\}$. Then $\frac{\partial f}{\partial v}(x_0) = T(v)$ where $T$ is the differential of $f$ at $x_0$.}
\begin{proof}
    Want to show:
    \begin{align*}
        \lim_{t \to 0} \frac{f(x_0 + tv) - f(x_0)}{t} = T(v).
    \end{align*}
    By the definition of differentiability, we know that:
    \begin{align*}
        \lim_{x \to x_0} \frac{f(x) - f(x_0) - T(x-x_0)}{\Vert x - x_0 \Vert_X} = 0
    \end{align*}
    where $T: X \to Y$ is linear and continuous. Take $x = x_0 + tv$ (restriction). Then we have:
    \begin{align*}
        \frac{f(x) - f(x_0) - T(x-x_0)}{\Vert x - x_0 \Vert_X} &= \frac{f(x_0 + tv) - f(x_0) - T(tv)}{\Vert tv \Vert_X} \\
        &= \frac{f(x_0 + tv) - f(x_0) - tT(v)}{|t| \Vert v \Vert_X} \\
        &= \frac{f(x_0 + tv) - f(x_0) - tT(v)}{|t|}
    \end{align*}
    If we take the limit from the right, we get:
    \begin{align*}
        \lim_{t \to 0^+} \frac{f(x) - f(x_0) - T(x-x_0)}{\Vert x - x_0 \Vert_X} &= 0 \\
        &= \lim_{t \to 0^+} \frac{f(x_0 + tv) - f(x_0) - tT(v)}{|t|} \\
        &= \lim_{t \to 0^+} \frac{f(x_0 + tv) - f(x_0)}{t} - T(v) \\
        &\implies \lim_{t \to 0^+} \frac{f(x_0 + tv) - f(x_0)}{t} = T(v)
    \end{align*}
    If we take the limit from the left, we get:
    \begin{align*}
        \lim_{t \to 0^-} \frac{f(x) - f(x_0) - T(x-x_0)}{\Vert x - x_0 \Vert_X} &= 0 \\
        &= \lim_{t \to 0^-} \frac{f(x_0 + tv) - f(x_0) - tT(v)}{|t|} \\
        &= \lim_{t \to 0^-} \frac{f(x_0 + tv) - f(x_0)}{t} - T(v) \\
        &\implies \lim_{t \to 0^-} \frac{f(x_0 + tv) - f(x_0)}{t} = T(v)
    \end{align*}
\end{proof}

\end{document}